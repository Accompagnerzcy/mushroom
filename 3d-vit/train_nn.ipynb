{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import numpy as np\n",
    "import setting\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ComFunction as cf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.backends.cudnn as cudnn\n",
    "# from scipy.io import loadmat\n",
    "# from scipy.io import savemat\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from vit_pytorch import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type=='cuda':\n",
    "#     dtype = torch.float32\n",
    "#     torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    dtype = torch.float64\n",
    "    torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "else:\n",
    "#     dtype = torch.float32\n",
    "#     torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    dtype = torch.float64\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperData(Dataset):\n",
    "    def __init__(self, data, labels, transfor):\n",
    "        self.data = data\n",
    "        self.transformer = transfor\n",
    "        self.labels = labels\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index,:,:]\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __labels__(self):\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_neighborhood_band(x_train, band, band_patch, patch=5):\n",
    "    nn = band_patch // 2\n",
    "    pp = (patch*patch) // 2\n",
    "    x_train_reshape = x_train.reshape(x_train.shape[0], patch*patch, band)\n",
    "    x_train_band = np.zeros((x_train.shape[0], patch*patch*band_patch, band),dtype=float)\n",
    "    # 中心区域\n",
    "    x_train_band[:,nn*patch*patch:(nn+1)*patch*patch,:] = x_train_reshape\n",
    "    #左边镜像\n",
    "    for i in range(nn):\n",
    "        if pp > 0:\n",
    "            x_train_band[:,i*patch*patch:(i+1)*patch*patch,:i+1] = x_train_reshape[:,:,band-i-1:]\n",
    "            x_train_band[:,i*patch*patch:(i+1)*patch*patch,i+1:] = x_train_reshape[:,:,:band-i-1]\n",
    "        else:\n",
    "            x_train_band[:,i:(i+1),:(nn-i)] = x_train_reshape[:,0:1,(band-nn+i):]\n",
    "            x_train_band[:,i:(i+1),(nn-i):] = x_train_reshape[:,0:1,:(band-nn+i)]\n",
    "    #右边镜像\n",
    "    for i in range(nn):\n",
    "        if pp > 0:\n",
    "            x_train_band[:,(nn+i+1)*patch*patch:(nn+i+2)*patch*patch,:band-i-1] = x_train_reshape[:,:,i+1:]\n",
    "            x_train_band[:,(nn+i+1)*patch*patch:(nn+i+2)*patch*patch,band-i-1:] = x_train_reshape[:,:,:i+1]\n",
    "        else:\n",
    "            x_train_band[:,(nn+1+i):(nn+2+i),(band-i-1):] = x_train_reshape[:,0:1,:(i+1)]\n",
    "            x_train_band[:,(nn+1+i):(nn+2+i),:(band-i-1)] = x_train_reshape[:,0:1,(i+1):]\n",
    "    return x_train_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH=setting.EPOCH\n",
    "BATCH_SIZE=setting.BATCH_SIZE\n",
    "LR=setting.LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (2000, 5, 5, 204)\n",
      "train label name: [0 1]\n",
      "0.9285714030265808\n",
      "0.04900568351149559\n",
      "train max: 0.9285714030265808\n"
     ]
    }
   ],
   "source": [
    "train_filename =setting.train_data_name +'_'+ str(setting.PATCH_SIZE) + '_'+ str(setting.DTYPE) +'.h5'\n",
    "with h5py.File(train_filename,'r') as readfile:\n",
    "    train=readfile['train_patch'][:]\n",
    "    train_labels=readfile['train_labels'][:]\n",
    "print('train size:', train.shape)\n",
    "print('train label name:',np.unique(train_labels))\n",
    "print(np.max(train))\n",
    "print(np.min(train))\n",
    "\n",
    "train_max=np.max(np.abs(train))\n",
    "print('train max:',train_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 25, 204)\n",
      "(2000, 204, 25)\n"
     ]
    }
   ],
   "source": [
    "train_band = gain_neighborhood_band(train, setting.band, setting.band_patches, setting.PATCH_SIZE)\n",
    "print(train_band.shape)\n",
    "train_band =train_band.transpose(0,2,1)\n",
    "print(train_band .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val size: (5000, 5, 5, 204)\n",
      "val label name: [0 1]\n",
      "0.9399999976158142\n",
      "0.05027933046221733\n",
      "val max: 0.9399999976158142\n",
      "(5000, 25, 204)\n",
      "(5000, 204, 25)\n"
     ]
    }
   ],
   "source": [
    "val_filename = setting.val_data_name +'_'+ str(setting.PATCH_SIZE) +'_'+ str(setting.DTYPE) +'.h5'\n",
    "with h5py.File(val_filename,'r') as readfile:\n",
    "    val=readfile['val_patch'][:]\n",
    "    val_labels=readfile['val_labels'][:]\n",
    "print('val size:', val.shape)\n",
    "print('val label name:',np.unique(val_labels))\n",
    "print(np.max(val))\n",
    "print(np.min(val))\n",
    "\n",
    "val_max=np.max(np.abs(val))\n",
    "print('val max:',val_max)\n",
    "\n",
    "\n",
    "val_band = gain_neighborhood_band(val, setting.band, setting.band_patches, setting.PATCH_SIZE)\n",
    "print(val_band.shape)\n",
    "val_band =val_band.transpose(0,2,1)\n",
    "print(val_band.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu\n",
    "# train_set=HyperData(train_band,train_labels, None)\n",
    "# trainloader= Data.DataLoader(dataset=train_set,batch_size=BATCH_SIZE,shuffle=True, num_workers=0)\n",
    "\n",
    "# val_set=HyperData(val_band, val_labels, None)\n",
    "# valloader= Data.DataLoader(dataset=val_set,batch_size=BATCH_SIZE,shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gpu\n",
    "train_set=HyperData(train_band,train_labels, None)\n",
    "trainloader= Data.DataLoader(dataset=train_set,batch_size=BATCH_SIZE,shuffle=True, num_workers=0,generator=torch.Generator(device='cuda'))\n",
    "\n",
    "val_set=HyperData(val_band, val_labels, None)\n",
    "valloader= Data.DataLoader(dataset=val_set,batch_size=BATCH_SIZE,shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/nn_2022_10_31_17_16\n"
     ]
    }
   ],
   "source": [
    "time_str=time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "\n",
    "writer_name=os.path.join(setting.writer_name,'nn_'+time_str)\n",
    "print(writer_name)\n",
    "writer = SummaryWriter(writer_name)\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=ViT(\n",
    "    image_size = setting.PATCH_SIZE,\n",
    "    near_band = setting.band_patches,\n",
    "    num_patches = setting.band,\n",
    "    num_classes = setting.num_class,\n",
    "    dim = 64,\n",
    "    depth = 5,\n",
    "    heads = 4,\n",
    "    mlp_dim = 8,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    mode = setting.mode\n",
    ").to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=setting.LR, weight_decay=setting.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=setting.EPOCH//10, gamma=setting.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc=0\n",
    "best_epoch=0\n",
    "\n",
    "global_steps=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ViT_train_result/traintime_2022_10_31_17_16.txt\n",
      "epoch: 0\n",
      "train oa: 51.0 train loss 0.7098739804003865\n",
      " val oa: 0.5 val_loss: 0.6939834429799513\n",
      "epoch: 0 best val accuracy: 0.5\n",
      "epoch: 1\n",
      "train oa: 50.9 train loss 0.6993595417193874\n",
      " val oa: 0.5486 val_loss: 0.6912001627438752\n",
      "epoch: 1 best val accuracy: 0.5486\n",
      "epoch: 2\n",
      "train oa: 51.85 train loss 0.6976823457440992\n",
      " val oa: 0.5 val_loss: 0.7050806393917176\n",
      "epoch: 3\n",
      "train oa: 51.45 train loss 0.6978322761420901\n",
      " val oa: 0.5882 val_loss: 0.6893843802628564\n",
      "epoch: 3 best val accuracy: 0.5882\n",
      "epoch: 4\n",
      "train oa: 51.6 train loss 0.6957683074257136\n",
      " val oa: 0.5176 val_loss: 0.6864967351942561\n",
      "epoch: 5\n",
      "train oa: 53.9 train loss 0.6854799063327356\n",
      " val oa: 0.5754 val_loss: 0.6736702481452409\n",
      "epoch: 6\n",
      "global steps   200 running loss: 0.027\n",
      "train oa: 57.25 train loss 0.6636881410686728\n",
      " val oa: 0.6328 val_loss: 0.6683310978339421\n",
      "epoch: 6 best val accuracy: 0.6328\n",
      "epoch: 7\n",
      "train oa: 59.95 train loss 0.6501521406862032\n",
      " val oa: 0.584 val_loss: 0.6492990836199006\n",
      "epoch: 8\n",
      "train oa: 62.45 train loss 0.6225974546123304\n",
      " val oa: 0.6192 val_loss: 0.651453798816041\n",
      "epoch: 9\n",
      "train oa: 65.25 train loss 0.6128925681747385\n",
      " val oa: 0.7154 val_loss: 0.5758161015623302\n",
      "epoch: 9 best val accuracy: 0.7154\n",
      "epoch: 10\n",
      "train oa: 68.0 train loss 0.5823015091168723\n",
      " val oa: 0.7368 val_loss: 0.526258011397767\n",
      "epoch: 10 best val accuracy: 0.7368\n",
      "epoch: 11\n",
      "train oa: 69.55 train loss 0.5540565858642008\n",
      " val oa: 0.7498 val_loss: 0.5414947404693566\n",
      "epoch: 11 best val accuracy: 0.7498\n",
      "epoch: 12\n",
      "global steps   400 running loss: 0.045\n",
      "train oa: 71.4 train loss 0.5411677966962843\n",
      " val oa: 0.709 val_loss: 0.5278863409609507\n",
      "epoch: 13\n",
      "train oa: 70.7 train loss 0.532106708997715\n",
      " val oa: 0.7512 val_loss: 0.527101005081694\n",
      "epoch: 13 best val accuracy: 0.7512\n",
      "epoch: 14\n",
      "train oa: 73.0 train loss 0.513283833850548\n",
      " val oa: 0.7618 val_loss: 0.4866013703439405\n",
      "epoch: 14 best val accuracy: 0.7618\n",
      "epoch: 15\n",
      "train oa: 73.1 train loss 0.5141992144569495\n",
      " val oa: 0.769 val_loss: 0.4890600255269195\n",
      "epoch: 15 best val accuracy: 0.769\n",
      "epoch: 16\n",
      "train oa: 72.55 train loss 0.5095106440546537\n",
      " val oa: 0.743 val_loss: 0.49218057219368466\n",
      "epoch: 17\n",
      "train oa: 73.8 train loss 0.5041661093969835\n",
      " val oa: 0.7256 val_loss: 0.4864406582354967\n",
      "epoch: 18\n",
      "global steps   600 running loss: 0.058\n",
      "train oa: 74.4 train loss 0.4948014006934676\n",
      " val oa: 0.7626 val_loss: 0.49594345595733696\n",
      "epoch: 19\n",
      "train oa: 73.15 train loss 0.49979852836051014\n",
      " val oa: 0.7652 val_loss: 0.4710513803995794\n",
      "epoch: 20\n",
      "train oa: 73.65 train loss 0.48842692208918836\n",
      " val oa: 0.7628 val_loss: 0.46473176366088503\n",
      "epoch: 21\n",
      "train oa: 75.45 train loss 0.4783861569880162\n",
      " val oa: 0.7438 val_loss: 0.5252981903358371\n",
      "epoch: 22\n",
      "train oa: 75.2 train loss 0.4940109605900628\n",
      " val oa: 0.7746 val_loss: 0.46279660324277055\n",
      "epoch: 22 best val accuracy: 0.7746\n",
      "epoch: 23\n",
      "train oa: 75.95 train loss 0.4620653160174594\n",
      " val oa: 0.7696 val_loss: 0.46211575623923595\n",
      "epoch: 24\n",
      "global steps   800 running loss: 0.078\n",
      "train oa: 74.45 train loss 0.4849974904115669\n",
      " val oa: 0.8002 val_loss: 0.46135782641399925\n",
      "epoch: 24 best val accuracy: 0.8002\n",
      "epoch: 25\n",
      "train oa: 75.45 train loss 0.4709174315194199\n",
      " val oa: 0.7916 val_loss: 0.45154629285139325\n",
      "epoch: 26\n",
      "train oa: 75.7 train loss 0.4675365566170687\n",
      " val oa: 0.794 val_loss: 0.4448840362243211\n",
      "epoch: 27\n",
      "train oa: 75.75 train loss 0.4638995811790255\n",
      " val oa: 0.7828 val_loss: 0.4772638708300788\n",
      "epoch: 28\n",
      "train oa: 77.4 train loss 0.44906574621834977\n",
      " val oa: 0.797 val_loss: 0.4457032721243818\n",
      "epoch: 29\n",
      "train oa: 75.8 train loss 0.4681800415264598\n",
      " val oa: 0.7692 val_loss: 0.47079481182187227\n",
      "epoch: 30\n",
      "train oa: 77.25 train loss 0.4531734050124578\n",
      " val oa: 0.7738 val_loss: 0.47769889071147287\n",
      "epoch: 31\n",
      "global steps  1000 running loss: 0.019\n",
      "train oa: 76.4 train loss 0.459903645625563\n",
      " val oa: 0.7558 val_loss: 0.46027299303301716\n",
      "epoch: 32\n",
      "train oa: 75.7 train loss 0.46332125346029857\n",
      " val oa: 0.7932 val_loss: 0.4571257399626211\n",
      "epoch: 33\n",
      "train oa: 75.65 train loss 0.459034490013567\n",
      " val oa: 0.8026 val_loss: 0.44761938729707385\n",
      "epoch: 33 best val accuracy: 0.8026\n",
      "epoch: 34\n",
      "train oa: 78.05 train loss 0.4332343196840695\n",
      " val oa: 0.8008 val_loss: 0.44771708427682994\n",
      "epoch: 35\n",
      "train oa: 77.65 train loss 0.447448965618905\n",
      " val oa: 0.7792 val_loss: 0.4300256526905801\n",
      "epoch: 36\n",
      "train oa: 78.4 train loss 0.44514274533886633\n",
      " val oa: 0.7748 val_loss: 0.434758146594598\n",
      "epoch: 37\n",
      "global steps  1200 running loss: 0.035\n",
      "train oa: 77.15 train loss 0.43960583717954765\n",
      " val oa: 0.7924 val_loss: 0.42038635412305786\n",
      "epoch: 38\n",
      "train oa: 77.55 train loss 0.4353082711520798\n",
      " val oa: 0.7764 val_loss: 0.43763999888565136\n",
      "epoch: 39\n",
      "train oa: 77.65 train loss 0.44252964814299217\n",
      " val oa: 0.7734 val_loss: 0.42243635366777754\n",
      "epoch: 40\n",
      "train oa: 75.65 train loss 0.43311009356015917\n",
      " val oa: 0.8064 val_loss: 0.42530210328355633\n",
      "epoch: 40 best val accuracy: 0.8064\n",
      "epoch: 41\n",
      "train oa: 77.35 train loss 0.4278056709663397\n",
      " val oa: 0.7746 val_loss: 0.4770438131783608\n",
      "epoch: 42\n",
      "train oa: 78.5 train loss 0.4211458793534575\n",
      " val oa: 0.7898 val_loss: 0.4335357713557157\n",
      "epoch: 43\n",
      "global steps  1400 running loss: 0.051\n",
      "train oa: 77.65 train loss 0.4335599345565981\n",
      " val oa: 0.796 val_loss: 0.4198078033853656\n",
      "epoch: 44\n",
      "train oa: 77.65 train loss 0.4280347422020921\n",
      " val oa: 0.7966 val_loss: 0.42575195805152904\n",
      "epoch: 45\n",
      "train oa: 77.25 train loss 0.43471026585150024\n",
      " val oa: 0.7564 val_loss: 0.47732097783331495\n",
      "epoch: 46\n",
      "train oa: 76.35 train loss 0.4495319789646094\n",
      " val oa: 0.7912 val_loss: 0.4260644234959955\n",
      "epoch: 47\n",
      "train oa: 77.2 train loss 0.4394039500109801\n",
      " val oa: 0.7926 val_loss: 0.42534422825828333\n",
      "epoch: 48\n",
      "train oa: 77.15 train loss 0.4416737988625131\n",
      " val oa: 0.7808 val_loss: 0.4537328743104106\n",
      "epoch: 49\n",
      "global steps  1600 running loss: 0.068\n",
      "train oa: 77.6 train loss 0.4222764784397398\n",
      " val oa: 0.7866 val_loss: 0.4330988496182914\n",
      "epoch: 50\n",
      "train oa: 79.1 train loss 0.4187153227142543\n",
      " val oa: 0.7754 val_loss: 0.42383965790478456\n",
      "epoch: 51\n",
      "train oa: 78.25 train loss 0.41705593018230824\n",
      " val oa: 0.7538 val_loss: 0.4920831315346518\n",
      "epoch: 52\n",
      "train oa: 77.95 train loss 0.4309932169626933\n",
      " val oa: 0.7938 val_loss: 0.413964291108472\n",
      "epoch: 53\n",
      "train oa: 79.35 train loss 0.42736822502142024\n",
      " val oa: 0.794 val_loss: 0.44136610867651416\n",
      "epoch: 54\n",
      "train oa: 79.35 train loss 0.4141676546639599\n",
      " val oa: 0.801 val_loss: 0.41265352333986477\n",
      "epoch: 55\n",
      "train oa: 78.75 train loss 0.4090965733580446\n",
      " val oa: 0.8008 val_loss: 0.39762899483769704\n",
      "epoch: 56\n",
      "global steps  1800 running loss: 0.018\n",
      "train oa: 77.15 train loss 0.429863900004925\n",
      " val oa: 0.7982 val_loss: 0.43927935039540145\n",
      "epoch: 57\n",
      "train oa: 78.15 train loss 0.4148752900891021\n",
      " val oa: 0.7812 val_loss: 0.45731409115943294\n",
      "epoch: 58\n",
      "train oa: 77.7 train loss 0.44016252878307793\n",
      " val oa: 0.7834 val_loss: 0.4496282884012893\n",
      "epoch: 59\n",
      "train oa: 78.75 train loss 0.41414496462315303\n",
      " val oa: 0.7924 val_loss: 0.42810715187340653\n",
      "epoch: 60\n",
      "train oa: 78.9 train loss 0.4106776642314344\n",
      " val oa: 0.799 val_loss: 0.4249085006003854\n",
      "epoch: 61\n",
      "train oa: 79.1 train loss 0.4052535356884485\n",
      " val oa: 0.7958 val_loss: 0.4221905976381406\n",
      "epoch: 62\n",
      "global steps  2000 running loss: 0.034\n",
      "train oa: 79.1 train loss 0.41243163596841714\n",
      " val oa: 0.8048 val_loss: 0.43353860139172307\n",
      "epoch: 63\n",
      "train oa: 79.3 train loss 0.4088631356703714\n",
      " val oa: 0.7886 val_loss: 0.4600759700945928\n",
      "epoch: 64\n",
      "train oa: 79.4 train loss 0.415414159552529\n",
      " val oa: 0.782 val_loss: 0.4701846751091843\n",
      "epoch: 65\n",
      "train oa: 79.35 train loss 0.4076613455373805\n",
      " val oa: 0.7984 val_loss: 0.4128685969065613\n",
      "epoch: 66\n",
      "train oa: 80.2 train loss 0.40755464919116413\n",
      " val oa: 0.7898 val_loss: 0.41716484866418624\n",
      "epoch: 67\n",
      "train oa: 78.95 train loss 0.40767583316473316\n",
      " val oa: 0.7774 val_loss: 0.449839212031633\n",
      "epoch: 68\n",
      "global steps  2200 running loss: 0.048\n",
      "train oa: 79.75 train loss 0.40580715092534925\n",
      " val oa: 0.804 val_loss: 0.40669766003346136\n",
      "epoch: 69\n",
      "train oa: 78.1 train loss 0.4193854358310219\n",
      " val oa: 0.7966 val_loss: 0.4119449849507921\n",
      "epoch: 70\n",
      "train oa: 77.7 train loss 0.41426167872343844\n",
      " val oa: 0.7372 val_loss: 0.5070982804125329\n",
      "epoch: 71\n",
      "train oa: 78.75 train loss 0.40924177580379867\n",
      " val oa: 0.7818 val_loss: 0.4382952855146962\n",
      "epoch: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 78.25 train loss 0.4102133837076668\n",
      " val oa: 0.7552 val_loss: 0.4716881117823161\n",
      "epoch: 73\n",
      "train oa: 78.8 train loss 0.4117710307553159\n",
      " val oa: 0.79 val_loss: 0.4148137883066518\n",
      "epoch: 74\n",
      "global steps  2400 running loss: 0.067\n",
      "train oa: 77.65 train loss 0.4207701882939345\n",
      " val oa: 0.7926 val_loss: 0.4145647618354425\n",
      "epoch: 75\n",
      "train oa: 79.55 train loss 0.4045874962736571\n",
      " val oa: 0.7818 val_loss: 0.4350871394767743\n",
      "epoch: 76\n",
      "train oa: 78.15 train loss 0.39919630715355314\n",
      " val oa: 0.7798 val_loss: 0.458083001474762\n",
      "epoch: 77\n",
      "train oa: 79.1 train loss 0.4089002511678486\n",
      " val oa: 0.7788 val_loss: 0.4483194359915226\n",
      "epoch: 78\n",
      "train oa: 78.0 train loss 0.3978191201632818\n",
      " val oa: 0.786 val_loss: 0.45438562437099134\n",
      "epoch: 79\n",
      "train oa: 78.3 train loss 0.40563520738037934\n",
      " val oa: 0.7924 val_loss: 0.4161284021723073\n",
      "epoch: 80\n",
      "train oa: 77.65 train loss 0.41706666192751274\n",
      " val oa: 0.8002 val_loss: 0.40943170507566756\n",
      "epoch: 81\n",
      "global steps  2600 running loss: 0.015\n",
      "train oa: 79.5 train loss 0.3990641422533307\n",
      " val oa: 0.78 val_loss: 0.41624648445685464\n",
      "epoch: 82\n",
      "train oa: 77.8 train loss 0.4084755637406926\n",
      " val oa: 0.8024 val_loss: 0.41720809025332634\n",
      "epoch: 83\n",
      "train oa: 78.8 train loss 0.40523255881540293\n",
      " val oa: 0.796 val_loss: 0.4078096783102154\n",
      "epoch: 84\n",
      "train oa: 79.65 train loss 0.39807711376659827\n",
      " val oa: 0.7978 val_loss: 0.4178364839246026\n",
      "epoch: 85\n",
      "train oa: 80.0 train loss 0.407243583743316\n",
      " val oa: 0.793 val_loss: 0.4227976299189859\n",
      "epoch: 86\n",
      "train oa: 80.05 train loss 0.400236728061106\n",
      " val oa: 0.7938 val_loss: 0.4134461190401337\n",
      "epoch: 87\n",
      "global steps  2800 running loss: 0.032\n",
      "train oa: 79.55 train loss 0.39616721238180236\n",
      " val oa: 0.7536 val_loss: 0.48937882800527294\n",
      "epoch: 88\n",
      "train oa: 79.25 train loss 0.4041864860737673\n",
      " val oa: 0.7878 val_loss: 0.42738003702101346\n",
      "epoch: 89\n",
      "train oa: 79.55 train loss 0.3935005795794154\n",
      " val oa: 0.7714 val_loss: 0.4430398371961077\n",
      "epoch: 90\n",
      "train oa: 78.6 train loss 0.4071396764049742\n",
      " val oa: 0.7962 val_loss: 0.42050229292717456\n",
      "epoch: 91\n",
      "train oa: 79.4 train loss 0.40042863193859435\n",
      " val oa: 0.7624 val_loss: 0.4647867330374436\n",
      "epoch: 92\n",
      "train oa: 78.2 train loss 0.40444828132931565\n",
      " val oa: 0.796 val_loss: 0.40813496193545606\n",
      "epoch: 93\n",
      "global steps  3000 running loss: 0.048\n",
      "train oa: 79.75 train loss 0.3975913494032605\n",
      " val oa: 0.7896 val_loss: 0.4053388218086392\n",
      "epoch: 94\n",
      "train oa: 80.25 train loss 0.3916396217088464\n",
      " val oa: 0.8038 val_loss: 0.4047271170643663\n",
      "epoch: 95\n",
      "train oa: 79.4 train loss 0.3889585773796424\n",
      " val oa: 0.7592 val_loss: 0.47838169353326143\n",
      "epoch: 96\n",
      "train oa: 79.5 train loss 0.4012181491376553\n",
      " val oa: 0.7876 val_loss: 0.4240475434155046\n",
      "epoch: 97\n",
      "train oa: 79.65 train loss 0.4076737857541632\n",
      " val oa: 0.7762 val_loss: 0.42702892324089614\n",
      "epoch: 98\n",
      "train oa: 79.0 train loss 0.40078282325980913\n",
      " val oa: 0.7448 val_loss: 0.4826061780626779\n",
      "epoch: 99\n",
      "global steps  3200 running loss: 0.062\n",
      "train oa: 79.45 train loss 0.38504096776187324\n",
      " val oa: 0.7994 val_loss: 0.41480730070229804\n",
      "epoch: 100\n",
      "train oa: 80.05 train loss 0.39906242260753155\n",
      " val oa: 0.8056 val_loss: 0.39131440024876085\n",
      "epoch: 101\n",
      "train oa: 78.4 train loss 0.41151009220362533\n",
      " val oa: 0.791 val_loss: 0.42715621902199613\n",
      "epoch: 102\n",
      "train oa: 80.65 train loss 0.3938879449749151\n",
      " val oa: 0.7996 val_loss: 0.40360817286682604\n",
      "epoch: 103\n",
      "train oa: 80.3 train loss 0.37971114737697337\n",
      " val oa: 0.7966 val_loss: 0.40260317282128344\n",
      "epoch: 104\n",
      "train oa: 79.95 train loss 0.3765452731147521\n",
      " val oa: 0.8084 val_loss: 0.3860940198479701\n",
      "epoch: 104 best val accuracy: 0.8084\n",
      "epoch: 105\n",
      "train oa: 79.65 train loss 0.3962844693170929\n",
      " val oa: 0.7854 val_loss: 0.4156349799526137\n",
      "epoch: 106\n",
      "global steps  3400 running loss: 0.016\n",
      "train oa: 79.25 train loss 0.4119730946107352\n",
      " val oa: 0.7926 val_loss: 0.418832557882365\n",
      "epoch: 107\n",
      "train oa: 79.65 train loss 0.39285019225084516\n",
      " val oa: 0.7778 val_loss: 0.43977998552652137\n",
      "epoch: 108\n",
      "train oa: 80.05 train loss 0.3855086090300138\n",
      " val oa: 0.7768 val_loss: 0.43064166578621127\n",
      "epoch: 109\n",
      "train oa: 79.5 train loss 0.3926559081220021\n",
      " val oa: 0.7924 val_loss: 0.41703394079026646\n",
      "epoch: 110\n",
      "train oa: 80.05 train loss 0.38493093083472957\n",
      " val oa: 0.8024 val_loss: 0.3992808545929916\n",
      "epoch: 111\n",
      "train oa: 80.9 train loss 0.3760015722619521\n",
      " val oa: 0.7778 val_loss: 0.44264043285441296\n",
      "epoch: 112\n",
      "global steps  3600 running loss: 0.030\n",
      "train oa: 80.15 train loss 0.3849008512621727\n",
      " val oa: 0.8016 val_loss: 0.40807380641210983\n",
      "epoch: 113\n",
      "train oa: 80.55 train loss 0.3761912993746715\n",
      " val oa: 0.8138 val_loss: 0.3986279433419415\n",
      "epoch: 113 best val accuracy: 0.8138\n",
      "epoch: 114\n",
      "train oa: 80.2 train loss 0.39455881263698206\n",
      " val oa: 0.7796 val_loss: 0.44654015518032764\n",
      "epoch: 115\n",
      "train oa: 79.45 train loss 0.3925238783949294\n",
      " val oa: 0.8122 val_loss: 0.3979438264033859\n",
      "epoch: 116\n",
      "train oa: 80.35 train loss 0.3798168687767282\n",
      " val oa: 0.804 val_loss: 0.399801571282789\n",
      "epoch: 117\n",
      "train oa: 80.4 train loss 0.3774001817099993\n",
      " val oa: 0.8008 val_loss: 0.4247725668715791\n",
      "epoch: 118\n",
      "global steps  3800 running loss: 0.046\n",
      "train oa: 80.25 train loss 0.38521809432796555\n",
      " val oa: 0.8112 val_loss: 0.4023455287999354\n",
      "epoch: 119\n",
      "train oa: 80.25 train loss 0.3801816100458768\n",
      " val oa: 0.7902 val_loss: 0.4172036424857969\n",
      "epoch: 120\n",
      "train oa: 79.75 train loss 0.38933504326583224\n",
      " val oa: 0.8092 val_loss: 0.40993618973012536\n",
      "epoch: 121\n",
      "train oa: 79.8 train loss 0.40031705961213354\n",
      " val oa: 0.7706 val_loss: 0.4702001607103058\n",
      "epoch: 122\n",
      "train oa: 80.0 train loss 0.3826853233545747\n",
      " val oa: 0.737 val_loss: 0.5220134933946826\n",
      "epoch: 123\n",
      "train oa: 79.45 train loss 0.4007129994098183\n",
      " val oa: 0.7848 val_loss: 0.44769263454090735\n",
      "epoch: 124\n",
      "global steps  4000 running loss: 0.062\n",
      "train oa: 80.45 train loss 0.38779359282548453\n",
      " val oa: 0.764 val_loss: 0.44732931594906816\n",
      "epoch: 125\n",
      "train oa: 80.35 train loss 0.3905368471372698\n",
      " val oa: 0.7496 val_loss: 0.47523704506747305\n",
      "epoch: 126\n",
      "train oa: 79.65 train loss 0.3944391666750267\n",
      " val oa: 0.8134 val_loss: 0.4016327060214565\n",
      "epoch: 127\n",
      "train oa: 80.65 train loss 0.3842178903436405\n",
      " val oa: 0.815 val_loss: 0.4146533089378517\n",
      "epoch: 127 best val accuracy: 0.815\n",
      "epoch: 128\n",
      "train oa: 79.3 train loss 0.3955037942978089\n",
      " val oa: 0.7732 val_loss: 0.43906661835415656\n",
      "epoch: 129\n",
      "train oa: 78.55 train loss 0.40948022550606145\n",
      " val oa: 0.8004 val_loss: 0.3977706921142412\n",
      "epoch: 130\n",
      "train oa: 80.6 train loss 0.38448303040843235\n",
      " val oa: 0.8144 val_loss: 0.3893047238501769\n",
      "epoch: 131\n",
      "global steps  4200 running loss: 0.016\n",
      "train oa: 80.85 train loss 0.3723840976019289\n",
      " val oa: 0.8052 val_loss: 0.39196831716619956\n",
      "epoch: 132\n",
      "train oa: 80.5 train loss 0.38842594177322565\n",
      " val oa: 0.7956 val_loss: 0.4118565216145234\n",
      "epoch: 133\n",
      "train oa: 80.95 train loss 0.3794756904257437\n",
      " val oa: 0.7814 val_loss: 0.4284759234763587\n",
      "epoch: 134\n",
      "train oa: 81.4 train loss 0.3789967920911151\n",
      " val oa: 0.807 val_loss: 0.39392706702552027\n",
      "epoch: 135\n",
      "train oa: 80.1 train loss 0.391999611102714\n",
      " val oa: 0.8072 val_loss: 0.39389391830070036\n",
      "epoch: 136\n",
      "train oa: 80.75 train loss 0.3748813406576045\n",
      " val oa: 0.81 val_loss: 0.3979323394937722\n",
      "epoch: 137\n",
      "global steps  4400 running loss: 0.031\n",
      "train oa: 79.25 train loss 0.3949171815830562\n",
      " val oa: 0.8042 val_loss: 0.39896192049531864\n",
      "epoch: 138\n",
      "train oa: 79.8 train loss 0.38985021921342783\n",
      " val oa: 0.7812 val_loss: 0.4396937518581141\n",
      "epoch: 139\n",
      "train oa: 80.0 train loss 0.3821375363854635\n",
      " val oa: 0.8014 val_loss: 0.409930401814074\n",
      "epoch: 140\n",
      "train oa: 80.15 train loss 0.3914929499342675\n",
      " val oa: 0.798 val_loss: 0.3987531306711714\n",
      "epoch: 141\n",
      "train oa: 80.3 train loss 0.38385049621417405\n",
      " val oa: 0.8024 val_loss: 0.40463523741071455\n",
      "epoch: 142\n",
      "train oa: 81.05 train loss 0.3815832080919866\n",
      " val oa: 0.7878 val_loss: 0.4286836679567966\n",
      "epoch: 143\n",
      "global steps  4600 running loss: 0.046\n",
      "train oa: 80.1 train loss 0.38234687363656766\n",
      " val oa: 0.7804 val_loss: 0.44928465431126496\n",
      "epoch: 144\n",
      "train oa: 80.75 train loss 0.38602128932189633\n",
      " val oa: 0.8022 val_loss: 0.40375395421918214\n",
      "epoch: 145\n",
      "train oa: 79.4 train loss 0.38274246101853143\n",
      " val oa: 0.7988 val_loss: 0.409919862110702\n",
      "epoch: 146\n",
      "train oa: 79.75 train loss 0.3787268640899741\n",
      " val oa: 0.7912 val_loss: 0.418921939514679\n",
      "epoch: 147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 81.7 train loss 0.37627457320277496\n",
      " val oa: 0.7784 val_loss: 0.4612044317020128\n",
      "epoch: 148\n",
      "train oa: 80.45 train loss 0.3826075242231011\n",
      " val oa: 0.764 val_loss: 0.4532559242018713\n",
      "epoch: 149\n",
      "global steps  4800 running loss: 0.060\n",
      "train oa: 80.6 train loss 0.37242668769235515\n",
      " val oa: 0.803 val_loss: 0.3866770816541727\n",
      "epoch: 150\n",
      "train oa: 79.7 train loss 0.38930641091696505\n",
      " val oa: 0.808 val_loss: 0.3976387050363821\n",
      "epoch: 151\n",
      "train oa: 80.7 train loss 0.375073469871989\n",
      " val oa: 0.806 val_loss: 0.3944007724409364\n",
      "epoch: 152\n",
      "train oa: 80.55 train loss 0.3870648049168159\n",
      " val oa: 0.7908 val_loss: 0.4253283124943161\n",
      "epoch: 153\n",
      "train oa: 79.75 train loss 0.3847733648588281\n",
      " val oa: 0.8094 val_loss: 0.39322442061963353\n",
      "epoch: 154\n",
      "train oa: 80.15 train loss 0.4023309454918677\n",
      " val oa: 0.7824 val_loss: 0.43609138788772434\n",
      "epoch: 155\n",
      "train oa: 80.75 train loss 0.3737745948328105\n",
      " val oa: 0.7996 val_loss: 0.39990087744058267\n",
      "epoch: 156\n",
      "global steps  5000 running loss: 0.015\n",
      "train oa: 80.8 train loss 0.38920869442347455\n",
      " val oa: 0.8084 val_loss: 0.39479112376552816\n",
      "epoch: 157\n",
      "train oa: 82.0 train loss 0.37902050848279883\n",
      " val oa: 0.7928 val_loss: 0.4189026064625617\n",
      "epoch: 158\n",
      "train oa: 80.6 train loss 0.38254068897505267\n",
      " val oa: 0.8174 val_loss: 0.393192982715495\n",
      "epoch: 158 best val accuracy: 0.8174\n",
      "epoch: 159\n",
      "train oa: 80.4 train loss 0.3838356129477349\n",
      " val oa: 0.806 val_loss: 0.3863316975710155\n",
      "epoch: 160\n",
      "train oa: 79.75 train loss 0.38185224119768957\n",
      " val oa: 0.7864 val_loss: 0.41411812494965405\n",
      "epoch: 161\n",
      "train oa: 81.2 train loss 0.3722861401994888\n",
      " val oa: 0.7866 val_loss: 0.41870882687491945\n",
      "epoch: 162\n",
      "global steps  5200 running loss: 0.028\n",
      "train oa: 81.0 train loss 0.36913052502991617\n",
      " val oa: 0.7914 val_loss: 0.4098535817054053\n",
      "epoch: 163\n",
      "train oa: 80.15 train loss 0.3821403036591202\n",
      " val oa: 0.8002 val_loss: 0.40658616183048285\n",
      "epoch: 164\n",
      "train oa: 80.65 train loss 0.39020779474477213\n",
      " val oa: 0.819 val_loss: 0.3803212840069323\n",
      "epoch: 164 best val accuracy: 0.819\n",
      "epoch: 165\n",
      "train oa: 80.15 train loss 0.3788845027814951\n",
      " val oa: 0.8126 val_loss: 0.3873813337678035\n",
      "epoch: 166\n",
      "train oa: 80.6 train loss 0.37888802560974255\n",
      " val oa: 0.8098 val_loss: 0.3906395061049812\n",
      "epoch: 167\n",
      "train oa: 80.55 train loss 0.37799527435390606\n",
      " val oa: 0.807 val_loss: 0.3876646304328793\n",
      "epoch: 168\n",
      "global steps  5400 running loss: 0.045\n",
      "train oa: 81.15 train loss 0.37204398055546056\n",
      " val oa: 0.825 val_loss: 0.3769407062330423\n",
      "epoch: 168 best val accuracy: 0.825\n",
      "epoch: 169\n",
      "train oa: 80.75 train loss 0.37744182258959236\n",
      " val oa: 0.808 val_loss: 0.401116173133481\n",
      "epoch: 170\n",
      "train oa: 80.35 train loss 0.38307600124573454\n",
      " val oa: 0.8146 val_loss: 0.3858758672165344\n",
      "epoch: 171\n",
      "train oa: 81.05 train loss 0.38075344714203485\n",
      " val oa: 0.7954 val_loss: 0.3964856994584837\n",
      "epoch: 172\n",
      "train oa: 80.6 train loss 0.3758916617323028\n",
      " val oa: 0.8086 val_loss: 0.38992203947182674\n",
      "epoch: 173\n",
      "train oa: 80.3 train loss 0.3789020812888747\n",
      " val oa: 0.802 val_loss: 0.40014053144840145\n",
      "epoch: 174\n",
      "global steps  5600 running loss: 0.060\n",
      "train oa: 80.05 train loss 0.3750374909805599\n",
      " val oa: 0.8186 val_loss: 0.3890462216900802\n",
      "epoch: 175\n",
      "train oa: 80.45 train loss 0.37311223241246627\n",
      " val oa: 0.82 val_loss: 0.3842737640280062\n",
      "epoch: 176\n",
      "train oa: 82.05 train loss 0.3690293231448436\n",
      " val oa: 0.8222 val_loss: 0.3763562110360862\n",
      "epoch: 177\n",
      "train oa: 80.7 train loss 0.37902655098437027\n",
      " val oa: 0.819 val_loss: 0.38921733948143167\n",
      "epoch: 178\n",
      "train oa: 81.5 train loss 0.37386025155064373\n",
      " val oa: 0.8116 val_loss: 0.3967209760766597\n",
      "epoch: 179\n",
      "train oa: 81.05 train loss 0.3765543603629297\n",
      " val oa: 0.8212 val_loss: 0.37453459666902483\n",
      "epoch: 180\n",
      "train oa: 81.1 train loss 0.38932007766376997\n",
      " val oa: 0.8182 val_loss: 0.38097310196237216\n",
      "epoch: 181\n",
      "global steps  5800 running loss: 0.016\n",
      "train oa: 78.85 train loss 0.3867799046886567\n",
      " val oa: 0.8104 val_loss: 0.4009160425654612\n",
      "epoch: 182\n",
      "train oa: 81.45 train loss 0.3646721218238986\n",
      " val oa: 0.8068 val_loss: 0.39292552343808756\n",
      "epoch: 183\n",
      "train oa: 80.65 train loss 0.36072332224720466\n",
      " val oa: 0.7762 val_loss: 0.42736849249329006\n",
      "epoch: 184\n",
      "train oa: 80.95 train loss 0.37076431786728065\n",
      " val oa: 0.8042 val_loss: 0.3966183431315939\n",
      "epoch: 185\n",
      "train oa: 82.25 train loss 0.3715954191775099\n",
      " val oa: 0.8002 val_loss: 0.3966023361920864\n",
      "epoch: 186\n",
      "train oa: 80.75 train loss 0.3667456838428993\n",
      " val oa: 0.786 val_loss: 0.41390647073187536\n",
      "epoch: 187\n",
      "global steps  6000 running loss: 0.028\n",
      "train oa: 82.05 train loss 0.35542017483590066\n",
      " val oa: 0.8122 val_loss: 0.40647743354337873\n",
      "epoch: 188\n",
      "train oa: 82.1 train loss 0.370268624038679\n",
      " val oa: 0.8198 val_loss: 0.3874311070747663\n",
      "epoch: 189\n",
      "train oa: 81.8 train loss 0.3613815712840107\n",
      " val oa: 0.7756 val_loss: 0.4396727399155518\n",
      "epoch: 190\n",
      "train oa: 81.75 train loss 0.3553395548066265\n",
      " val oa: 0.76 val_loss: 0.4723521404783114\n",
      "epoch: 191\n",
      "train oa: 81.1 train loss 0.37176351887951603\n",
      " val oa: 0.823 val_loss: 0.388257640654973\n",
      "epoch: 192\n",
      "train oa: 81.85 train loss 0.36800223044045655\n",
      " val oa: 0.8094 val_loss: 0.3964172067942283\n",
      "epoch: 193\n",
      "global steps  6200 running loss: 0.047\n",
      "train oa: 80.8 train loss 0.3911564950152243\n",
      " val oa: 0.7902 val_loss: 0.4252407286569574\n",
      "epoch: 194\n",
      "train oa: 79.5 train loss 0.3837943671216986\n",
      " val oa: 0.8138 val_loss: 0.39322983393681327\n",
      "epoch: 195\n",
      "train oa: 81.15 train loss 0.36150640133732276\n",
      " val oa: 0.8234 val_loss: 0.38214389230720297\n",
      "epoch: 196\n",
      "train oa: 81.7 train loss 0.360130047555784\n",
      " val oa: 0.8178 val_loss: 0.38921131015745186\n",
      "epoch: 197\n",
      "train oa: 81.8 train loss 0.3599240922059541\n",
      " val oa: 0.818 val_loss: 0.3816789222384722\n",
      "epoch: 198\n",
      "train oa: 81.85 train loss 0.35315645966606213\n",
      " val oa: 0.8334 val_loss: 0.36940453704985377\n",
      "epoch: 198 best val accuracy: 0.8334\n",
      "epoch: 199\n",
      "global steps  6400 running loss: 0.060\n",
      "train oa: 80.2 train loss 0.3755554269214541\n",
      " val oa: 0.796 val_loss: 0.3991282952740162\n",
      "epoch: 200\n",
      "train oa: 79.5 train loss 0.3910272123235232\n",
      " val oa: 0.8176 val_loss: 0.3947363675817178\n",
      "epoch: 201\n",
      "train oa: 81.8 train loss 0.3490835602810462\n",
      " val oa: 0.8068 val_loss: 0.3987390858014849\n",
      "epoch: 202\n",
      "train oa: 81.75 train loss 0.3642294877203233\n",
      " val oa: 0.8314 val_loss: 0.36435685758015596\n",
      "epoch: 203\n",
      "train oa: 80.3 train loss 0.3790963297284716\n",
      " val oa: 0.8198 val_loss: 0.37764839424655056\n",
      "epoch: 204\n",
      "train oa: 82.25 train loss 0.3559965369450556\n",
      " val oa: 0.8126 val_loss: 0.3926375799500744\n",
      "epoch: 205\n",
      "train oa: 82.0 train loss 0.364571915144118\n",
      " val oa: 0.8348 val_loss: 0.36335869027744233\n",
      "epoch: 205 best val accuracy: 0.8348\n",
      "epoch: 206\n",
      "global steps  6600 running loss: 0.015\n",
      "train oa: 81.7 train loss 0.3636360572652376\n",
      " val oa: 0.8212 val_loss: 0.3827393299332762\n",
      "epoch: 207\n",
      "train oa: 81.55 train loss 0.35881887131444673\n",
      " val oa: 0.8046 val_loss: 0.39483600273149133\n",
      "epoch: 208\n",
      "train oa: 81.8 train loss 0.3734956525491146\n",
      " val oa: 0.7972 val_loss: 0.4140454948648553\n",
      "epoch: 209\n",
      "train oa: 82.75 train loss 0.349393237934183\n",
      " val oa: 0.8232 val_loss: 0.37868455846120647\n",
      "epoch: 210\n",
      "train oa: 82.5 train loss 0.3674078199192941\n",
      " val oa: 0.829 val_loss: 0.36800736241303217\n",
      "epoch: 211\n",
      "train oa: 80.5 train loss 0.37513752325829847\n",
      " val oa: 0.828 val_loss: 0.3754993946932464\n",
      "epoch: 212\n",
      "global steps  6800 running loss: 0.029\n",
      "train oa: 82.1 train loss 0.36056469450621204\n",
      " val oa: 0.8102 val_loss: 0.39751573001137763\n",
      "epoch: 213\n",
      "train oa: 81.55 train loss 0.3736328068136877\n",
      " val oa: 0.8396 val_loss: 0.37701302302790074\n",
      "epoch: 213 best val accuracy: 0.8396\n",
      "epoch: 214\n",
      "train oa: 81.4 train loss 0.3682332560169387\n",
      " val oa: 0.8182 val_loss: 0.38507124761129796\n",
      "epoch: 215\n",
      "train oa: 81.7 train loss 0.3693325971162179\n",
      " val oa: 0.8302 val_loss: 0.36470284874219633\n",
      "epoch: 216\n",
      "train oa: 82.0 train loss 0.3648631520257349\n",
      " val oa: 0.802 val_loss: 0.3944420770166193\n",
      "epoch: 217\n",
      "train oa: 81.0 train loss 0.3692816386495342\n",
      " val oa: 0.8324 val_loss: 0.37479588740936776\n",
      "epoch: 218\n",
      "global steps  7000 running loss: 0.043\n",
      "train oa: 82.1 train loss 0.3588356027734302\n",
      " val oa: 0.8314 val_loss: 0.37605976919808426\n",
      "epoch: 219\n",
      "train oa: 82.15 train loss 0.3613628542069525\n",
      " val oa: 0.8034 val_loss: 0.4060548435363342\n",
      "epoch: 220\n",
      "train oa: 80.75 train loss 0.3717004921866905\n",
      " val oa: 0.7992 val_loss: 0.4273831244771323\n",
      "epoch: 221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 81.75 train loss 0.3689412273126322\n",
      " val oa: 0.8204 val_loss: 0.37134321606210735\n",
      "epoch: 222\n",
      "train oa: 82.1 train loss 0.36783264557479384\n",
      " val oa: 0.8266 val_loss: 0.39132373663285036\n",
      "epoch: 223\n",
      "train oa: 81.15 train loss 0.3651731204446346\n",
      " val oa: 0.8314 val_loss: 0.3781694330944945\n",
      "epoch: 224\n",
      "global steps  7200 running loss: 0.057\n",
      "train oa: 82.45 train loss 0.3547081831224353\n",
      " val oa: 0.8124 val_loss: 0.3805874103241806\n",
      "epoch: 225\n",
      "train oa: 81.0 train loss 0.37371207903742604\n",
      " val oa: 0.8406 val_loss: 0.3671940024735219\n",
      "epoch: 225 best val accuracy: 0.8406\n",
      "epoch: 226\n",
      "train oa: 82.3 train loss 0.35904706318783336\n",
      " val oa: 0.8242 val_loss: 0.3765428149084402\n",
      "epoch: 227\n",
      "train oa: 82.35 train loss 0.36254643932676983\n",
      " val oa: 0.8216 val_loss: 0.3876547334185007\n",
      "epoch: 228\n",
      "train oa: 82.25 train loss 0.34625639634842936\n",
      " val oa: 0.8216 val_loss: 0.3940154420097124\n",
      "epoch: 229\n",
      "train oa: 83.4 train loss 0.35246913190724066\n",
      " val oa: 0.8494 val_loss: 0.35404604025321906\n",
      "epoch: 229 best val accuracy: 0.8494\n",
      "epoch: 230\n",
      "train oa: 82.65 train loss 0.3544558688386827\n",
      " val oa: 0.807 val_loss: 0.4014777264403905\n",
      "epoch: 231\n",
      "global steps  7400 running loss: 0.014\n",
      "train oa: 81.6 train loss 0.3753623714132768\n",
      " val oa: 0.837 val_loss: 0.3747957483152924\n",
      "epoch: 232\n",
      "train oa: 81.15 train loss 0.36869455406813806\n",
      " val oa: 0.831 val_loss: 0.3737922597518513\n",
      "epoch: 233\n",
      "train oa: 81.5 train loss 0.3916366951641496\n",
      " val oa: 0.8312 val_loss: 0.3766651882182319\n",
      "epoch: 234\n",
      "train oa: 81.15 train loss 0.3695942092127242\n",
      " val oa: 0.8036 val_loss: 0.3865773343599591\n",
      "epoch: 235\n",
      "train oa: 82.25 train loss 0.35908140195257177\n",
      " val oa: 0.823 val_loss: 0.38054452217475593\n",
      "epoch: 236\n",
      "train oa: 81.8 train loss 0.34969804286765654\n",
      " val oa: 0.8278 val_loss: 0.3880387622653455\n",
      "epoch: 237\n",
      "global steps  7600 running loss: 0.029\n",
      "train oa: 81.5 train loss 0.36763003826673724\n",
      " val oa: 0.845 val_loss: 0.35886761213365587\n",
      "epoch: 238\n",
      "train oa: 82.25 train loss 0.35613391155950136\n",
      " val oa: 0.8232 val_loss: 0.3943654576330054\n",
      "epoch: 239\n",
      "train oa: 81.7 train loss 0.3549446802338107\n",
      " val oa: 0.8314 val_loss: 0.36758613703243787\n",
      "epoch: 240\n",
      "train oa: 82.3 train loss 0.35906209139972\n",
      " val oa: 0.8356 val_loss: 0.37873020815416975\n",
      "epoch: 241\n",
      "train oa: 82.8 train loss 0.3529007277686062\n",
      " val oa: 0.7758 val_loss: 0.4487798085185748\n",
      "epoch: 242\n",
      "train oa: 81.8 train loss 0.3689625058121881\n",
      " val oa: 0.8204 val_loss: 0.38716859225689365\n",
      "epoch: 243\n",
      "global steps  7800 running loss: 0.042\n",
      "train oa: 82.3 train loss 0.35711416298885823\n",
      " val oa: 0.8072 val_loss: 0.4146826541716098\n",
      "epoch: 244\n",
      "train oa: 80.2 train loss 0.3759864344911452\n",
      " val oa: 0.7964 val_loss: 0.4090617683592504\n",
      "epoch: 245\n",
      "train oa: 82.4 train loss 0.3474443426674533\n",
      " val oa: 0.831 val_loss: 0.3869630497638604\n",
      "epoch: 246\n",
      "train oa: 82.2 train loss 0.3501970450964029\n",
      " val oa: 0.8334 val_loss: 0.3780390387642823\n",
      "epoch: 247\n",
      "train oa: 82.5 train loss 0.3587847918526858\n",
      " val oa: 0.8158 val_loss: 0.3966718120612582\n",
      "epoch: 248\n",
      "train oa: 81.9 train loss 0.36394212762686284\n",
      " val oa: 0.794 val_loss: 0.41035643913695874\n",
      "epoch: 249\n",
      "global steps  8000 running loss: 0.058\n",
      "train oa: 82.75 train loss 0.3615764051029939\n",
      " val oa: 0.7882 val_loss: 0.41830369767762327\n",
      "epoch: 250\n",
      "train oa: 83.5 train loss 0.35012934970157605\n",
      " val oa: 0.8236 val_loss: 0.38684793330318484\n",
      "epoch: 251\n",
      "train oa: 83.55 train loss 0.3450423326916506\n",
      " val oa: 0.834 val_loss: 0.3682067931668208\n",
      "epoch: 252\n",
      "train oa: 81.35 train loss 0.3653881427250956\n",
      " val oa: 0.8156 val_loss: 0.38580770507556483\n",
      "epoch: 253\n",
      "train oa: 83.95 train loss 0.3395548849095811\n",
      " val oa: 0.835 val_loss: 0.3566501622915014\n",
      "epoch: 254\n",
      "train oa: 82.4 train loss 0.35885746063299484\n",
      " val oa: 0.8312 val_loss: 0.3687971375899472\n",
      "epoch: 255\n",
      "train oa: 82.1 train loss 0.3535143507154332\n",
      " val oa: 0.83 val_loss: 0.36673349423796375\n",
      "epoch: 256\n",
      "global steps  8200 running loss: 0.014\n",
      "train oa: 83.25 train loss 0.3485939698073601\n",
      " val oa: 0.8348 val_loss: 0.3710256890996165\n",
      "epoch: 257\n",
      "train oa: 82.2 train loss 0.3492119358801681\n",
      " val oa: 0.814 val_loss: 0.3874701393320195\n",
      "epoch: 258\n",
      "train oa: 82.5 train loss 0.3589200757337548\n",
      " val oa: 0.8368 val_loss: 0.3695762128833192\n",
      "epoch: 259\n",
      "train oa: 82.0 train loss 0.34898494639976885\n",
      " val oa: 0.8376 val_loss: 0.36686519241109833\n",
      "epoch: 260\n",
      "train oa: 82.45 train loss 0.35210892650093095\n",
      " val oa: 0.844 val_loss: 0.3484254697195895\n",
      "epoch: 261\n",
      "train oa: 82.05 train loss 0.3589112647008347\n",
      " val oa: 0.8414 val_loss: 0.3597124674040516\n",
      "epoch: 262\n",
      "global steps  8400 running loss: 0.027\n",
      "train oa: 82.35 train loss 0.34858695045123433\n",
      " val oa: 0.8304 val_loss: 0.3771866908375786\n",
      "epoch: 263\n",
      "train oa: 81.7 train loss 0.3599155673570947\n",
      " val oa: 0.8364 val_loss: 0.3651393679836193\n",
      "epoch: 264\n",
      "train oa: 82.55 train loss 0.3495427425904577\n",
      " val oa: 0.8416 val_loss: 0.35716120234278687\n",
      "epoch: 265\n",
      "train oa: 81.85 train loss 0.35536198144539377\n",
      " val oa: 0.8262 val_loss: 0.38608444021740823\n",
      "epoch: 266\n",
      "train oa: 81.2 train loss 0.3511079575608223\n",
      " val oa: 0.8426 val_loss: 0.3640574436957267\n",
      "epoch: 267\n",
      "train oa: 82.9 train loss 0.34735463318973686\n",
      " val oa: 0.833 val_loss: 0.36664582656064854\n",
      "epoch: 268\n",
      "global steps  8600 running loss: 0.041\n",
      "train oa: 82.9 train loss 0.34876010935661245\n",
      " val oa: 0.8364 val_loss: 0.3888957233769371\n",
      "epoch: 269\n",
      "train oa: 82.0 train loss 0.3574333576711287\n",
      " val oa: 0.7838 val_loss: 0.4358305764814677\n",
      "epoch: 270\n",
      "train oa: 82.6 train loss 0.36040521568773926\n",
      " val oa: 0.8338 val_loss: 0.36139981801279214\n",
      "epoch: 271\n",
      "train oa: 81.8 train loss 0.36042792844938776\n",
      " val oa: 0.8272 val_loss: 0.36855267422590665\n",
      "epoch: 272\n",
      "train oa: 81.5 train loss 0.3577825840496003\n",
      " val oa: 0.8414 val_loss: 0.36362456184740566\n",
      "epoch: 273\n",
      "train oa: 82.95 train loss 0.35109928601877793\n",
      " val oa: 0.843 val_loss: 0.3519182572151793\n",
      "epoch: 274\n",
      "global steps  8800 running loss: 0.054\n",
      "train oa: 81.65 train loss 0.3381426206484838\n",
      " val oa: 0.8282 val_loss: 0.37710030050316357\n",
      "epoch: 275\n",
      "train oa: 82.4 train loss 0.3591740429908956\n",
      " val oa: 0.8464 val_loss: 0.34537088952094036\n",
      "epoch: 276\n",
      "train oa: 83.25 train loss 0.3454931258840809\n",
      " val oa: 0.8206 val_loss: 0.3912242637408852\n",
      "epoch: 277\n",
      "train oa: 82.65 train loss 0.34286343175906103\n",
      " val oa: 0.8446 val_loss: 0.36068576771776695\n",
      "epoch: 278\n",
      "train oa: 82.9 train loss 0.3503265391762202\n",
      " val oa: 0.8418 val_loss: 0.35498784423430146\n",
      "epoch: 279\n",
      "train oa: 82.1 train loss 0.3359944999926948\n",
      " val oa: 0.8378 val_loss: 0.36692610353405575\n",
      "epoch: 280\n",
      "train oa: 83.1 train loss 0.3439111948625814\n",
      " val oa: 0.8284 val_loss: 0.37245635157886914\n",
      "epoch: 281\n",
      "global steps  9000 running loss: 0.014\n",
      "train oa: 82.6 train loss 0.34919189689664104\n",
      " val oa: 0.8372 val_loss: 0.35231165786395957\n",
      "epoch: 282\n",
      "train oa: 82.85 train loss 0.3492172776390313\n",
      " val oa: 0.8492 val_loss: 0.3549451808366014\n",
      "epoch: 283\n",
      "train oa: 83.5 train loss 0.34841380839906155\n",
      " val oa: 0.8502 val_loss: 0.3481927631920206\n",
      "epoch: 283 best val accuracy: 0.8502\n",
      "epoch: 284\n",
      "train oa: 82.35 train loss 0.3426841101421553\n",
      " val oa: 0.8524 val_loss: 0.34977505420155297\n",
      "epoch: 284 best val accuracy: 0.8524\n",
      "epoch: 285\n",
      "train oa: 83.05 train loss 0.3429284939716306\n",
      " val oa: 0.8454 val_loss: 0.3556452935129899\n",
      "epoch: 286\n",
      "train oa: 83.05 train loss 0.339020116209869\n",
      " val oa: 0.8248 val_loss: 0.3803107412695234\n",
      "epoch: 287\n",
      "global steps  9200 running loss: 0.028\n",
      "train oa: 83.85 train loss 0.343330335798192\n",
      " val oa: 0.8318 val_loss: 0.3653255242477588\n",
      "epoch: 288\n",
      "train oa: 82.05 train loss 0.35628460295105313\n",
      " val oa: 0.8444 val_loss: 0.35026274910963706\n",
      "epoch: 289\n",
      "train oa: 82.05 train loss 0.34883998461221244\n",
      " val oa: 0.8006 val_loss: 0.39622378668917724\n",
      "epoch: 290\n",
      "train oa: 82.25 train loss 0.3714418604161747\n",
      " val oa: 0.8472 val_loss: 0.36582086379659545\n",
      "epoch: 291\n",
      "train oa: 83.5 train loss 0.3524169237546009\n",
      " val oa: 0.8442 val_loss: 0.35907177496836407\n",
      "epoch: 292\n",
      "train oa: 82.0 train loss 0.3514535290205164\n",
      " val oa: 0.8482 val_loss: 0.36502169089951086\n",
      "epoch: 293\n",
      "global steps  9400 running loss: 0.041\n",
      "train oa: 83.0 train loss 0.3388372698657479\n",
      " val oa: 0.8404 val_loss: 0.36612422573349174\n",
      "epoch: 294\n",
      "train oa: 82.2 train loss 0.34718971847388025\n",
      " val oa: 0.8458 val_loss: 0.3578791597600562\n",
      "epoch: 295\n",
      "train oa: 83.0 train loss 0.33967881543196643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.8486 val_loss: 0.351564415397216\n",
      "epoch: 296\n",
      "train oa: 82.85 train loss 0.3375293984930665\n",
      " val oa: 0.8416 val_loss: 0.34942459300109974\n",
      "epoch: 297\n",
      "train oa: 82.15 train loss 0.3419950289647881\n",
      " val oa: 0.8448 val_loss: 0.3473751340564938\n",
      "epoch: 298\n",
      "train oa: 83.05 train loss 0.3426377850130757\n",
      " val oa: 0.8498 val_loss: 0.3424345761338352\n",
      "epoch: 299\n",
      "global steps  9600 running loss: 0.053\n",
      "train oa: 83.6 train loss 0.3337204475565075\n",
      " val oa: 0.8252 val_loss: 0.3637712604963822\n",
      "epoch: 300\n",
      "train oa: 82.95 train loss 0.33464414715439617\n",
      " val oa: 0.8474 val_loss: 0.3622726489098591\n",
      "epoch: 301\n",
      "train oa: 82.9 train loss 0.34088342163125374\n",
      " val oa: 0.8298 val_loss: 0.3831547284941999\n",
      "epoch: 302\n",
      "train oa: 81.7 train loss 0.34574571609926363\n",
      " val oa: 0.839 val_loss: 0.3636380366181213\n",
      "epoch: 303\n",
      "train oa: 83.45 train loss 0.34182506167925547\n",
      " val oa: 0.8262 val_loss: 0.38876831247871746\n",
      "epoch: 304\n",
      "train oa: 83.65 train loss 0.3305635521373022\n",
      " val oa: 0.8532 val_loss: 0.3441199422568333\n",
      "epoch: 304 best val accuracy: 0.8532\n",
      "epoch: 305\n",
      "train oa: 82.8 train loss 0.34122771326285845\n",
      " val oa: 0.8332 val_loss: 0.3868593591979973\n",
      "epoch: 306\n",
      "global steps  9800 running loss: 0.014\n",
      "train oa: 82.85 train loss 0.35114501812317606\n",
      " val oa: 0.852 val_loss: 0.34361296139196257\n",
      "epoch: 307\n",
      "train oa: 82.6 train loss 0.34641942063284037\n",
      " val oa: 0.8536 val_loss: 0.34715352195303567\n",
      "epoch: 307 best val accuracy: 0.8536\n",
      "epoch: 308\n",
      "train oa: 82.0 train loss 0.36956150007180016\n",
      " val oa: 0.843 val_loss: 0.3505626633696507\n",
      "epoch: 309\n",
      "train oa: 83.45 train loss 0.337688501269684\n",
      " val oa: 0.8052 val_loss: 0.3874197439984149\n",
      "epoch: 310\n",
      "train oa: 83.1 train loss 0.33224692134211353\n",
      " val oa: 0.8354 val_loss: 0.3665661306182779\n",
      "epoch: 311\n",
      "train oa: 83.1 train loss 0.3466113919756017\n",
      " val oa: 0.8418 val_loss: 0.3606360553695614\n",
      "epoch: 312\n",
      "global steps 10000 running loss: 0.029\n",
      "train oa: 83.05 train loss 0.34202114475424245\n",
      " val oa: 0.8492 val_loss: 0.3490110290890952\n",
      "epoch: 313\n",
      "train oa: 83.1 train loss 0.3376684248268371\n",
      " val oa: 0.8212 val_loss: 0.3795733998530636\n",
      "epoch: 314\n",
      "train oa: 83.65 train loss 0.33836504131123524\n",
      " val oa: 0.827 val_loss: 0.3699757309578863\n",
      "epoch: 315\n",
      "train oa: 81.95 train loss 0.3493433176415915\n",
      " val oa: 0.8184 val_loss: 0.36702194568963215\n",
      "epoch: 316\n",
      "train oa: 83.4 train loss 0.34383510079376517\n",
      " val oa: 0.8428 val_loss: 0.3521397701161411\n",
      "epoch: 317\n",
      "train oa: 83.05 train loss 0.3416665873809265\n",
      " val oa: 0.805 val_loss: 0.3932586245797246\n",
      "epoch: 318\n",
      "global steps 10200 running loss: 0.041\n",
      "train oa: 83.75 train loss 0.3342159468627658\n",
      " val oa: 0.853 val_loss: 0.3460868472558822\n",
      "epoch: 319\n",
      "train oa: 83.85 train loss 0.33905304043130957\n",
      " val oa: 0.8024 val_loss: 0.3866935802033609\n",
      "epoch: 320\n",
      "train oa: 83.35 train loss 0.33926599935836005\n",
      " val oa: 0.8158 val_loss: 0.4065669345108257\n",
      "epoch: 321\n",
      "train oa: 83.0 train loss 0.34447864154321256\n",
      " val oa: 0.8534 val_loss: 0.339573863933685\n",
      "epoch: 322\n",
      "train oa: 82.5 train loss 0.3407550195949615\n",
      " val oa: 0.8434 val_loss: 0.35751853543465145\n",
      "epoch: 323\n",
      "train oa: 82.7 train loss 0.3456164231878043\n",
      " val oa: 0.8498 val_loss: 0.36269377198733505\n",
      "epoch: 324\n",
      "global steps 10400 running loss: 0.054\n",
      "train oa: 84.35 train loss 0.3400648629672185\n",
      " val oa: 0.8314 val_loss: 0.36157788545680075\n",
      "epoch: 325\n",
      "train oa: 83.9 train loss 0.3384063211353237\n",
      " val oa: 0.8416 val_loss: 0.36244973928371255\n",
      "epoch: 326\n",
      "train oa: 83.65 train loss 0.330968369566498\n",
      " val oa: 0.8464 val_loss: 0.3482122992899296\n",
      "epoch: 327\n",
      "train oa: 83.85 train loss 0.34234105956886896\n",
      " val oa: 0.8532 val_loss: 0.34822439743074735\n",
      "epoch: 328\n",
      "train oa: 83.6 train loss 0.3387060454616432\n",
      " val oa: 0.8554 val_loss: 0.33715092368669997\n",
      "epoch: 328 best val accuracy: 0.8554\n",
      "epoch: 329\n",
      "train oa: 83.5 train loss 0.3315625035352519\n",
      " val oa: 0.8528 val_loss: 0.340829320104615\n",
      "epoch: 330\n",
      "train oa: 83.35 train loss 0.33507561750853476\n",
      " val oa: 0.8344 val_loss: 0.36108140284593254\n",
      "epoch: 331\n",
      "global steps 10600 running loss: 0.012\n",
      "train oa: 83.9 train loss 0.3319474832871917\n",
      " val oa: 0.8596 val_loss: 0.3377307367202667\n",
      "epoch: 331 best val accuracy: 0.8596\n",
      "epoch: 332\n",
      "train oa: 84.05 train loss 0.3384609734536306\n",
      " val oa: 0.8428 val_loss: 0.36607823029444164\n",
      "epoch: 333\n",
      "train oa: 84.9 train loss 0.32393526052551586\n",
      " val oa: 0.836 val_loss: 0.36426988132155835\n",
      "epoch: 334\n",
      "train oa: 83.8 train loss 0.33052590429510464\n",
      " val oa: 0.85 val_loss: 0.3517732269091516\n",
      "epoch: 335\n",
      "train oa: 83.2 train loss 0.3327784843647914\n",
      " val oa: 0.845 val_loss: 0.36008385675970184\n",
      "epoch: 336\n",
      "train oa: 83.9 train loss 0.3238379485975417\n",
      " val oa: 0.822 val_loss: 0.3986764901070178\n",
      "epoch: 337\n",
      "global steps 10800 running loss: 0.028\n",
      "train oa: 82.55 train loss 0.34876183582140957\n",
      " val oa: 0.8588 val_loss: 0.328113426332842\n",
      "epoch: 338\n",
      "train oa: 84.1 train loss 0.32664924426729763\n",
      " val oa: 0.858 val_loss: 0.3379867808316691\n",
      "epoch: 339\n",
      "train oa: 83.15 train loss 0.33296715345208455\n",
      " val oa: 0.8572 val_loss: 0.3412318632296871\n",
      "epoch: 340\n",
      "train oa: 84.45 train loss 0.3145065421343126\n",
      " val oa: 0.8576 val_loss: 0.3371116701910996\n",
      "epoch: 341\n",
      "train oa: 83.3 train loss 0.3374241353654468\n",
      " val oa: 0.8414 val_loss: 0.3632583986070883\n",
      "epoch: 342\n",
      "train oa: 83.05 train loss 0.3504600910366817\n",
      " val oa: 0.848 val_loss: 0.3469769233623693\n",
      "epoch: 343\n",
      "global steps 11000 running loss: 0.041\n",
      "train oa: 84.05 train loss 0.3377325004987203\n",
      " val oa: 0.8406 val_loss: 0.35834426341452863\n",
      "epoch: 344\n",
      "train oa: 83.7 train loss 0.328462493614035\n",
      " val oa: 0.8134 val_loss: 0.4027244592750611\n",
      "epoch: 345\n",
      "train oa: 82.8 train loss 0.34664422913353204\n",
      " val oa: 0.8456 val_loss: 0.352474360777024\n",
      "epoch: 346\n",
      "train oa: 84.05 train loss 0.3305395955507203\n",
      " val oa: 0.854 val_loss: 0.3367437910067983\n",
      "epoch: 347\n",
      "train oa: 83.1 train loss 0.3396073724761471\n",
      " val oa: 0.8226 val_loss: 0.39176545807808877\n",
      "epoch: 348\n",
      "train oa: 83.95 train loss 0.3275548592555397\n",
      " val oa: 0.834 val_loss: 0.37607433858990785\n",
      "epoch: 349\n",
      "global steps 11200 running loss: 0.051\n",
      "train oa: 84.25 train loss 0.32056923505168583\n",
      " val oa: 0.8294 val_loss: 0.377296244911342\n",
      "epoch: 350\n",
      "train oa: 83.45 train loss 0.33305185772235163\n",
      " val oa: 0.8404 val_loss: 0.3631781556322472\n",
      "epoch: 351\n",
      "train oa: 83.85 train loss 0.3391227784596937\n",
      " val oa: 0.8596 val_loss: 0.33372389139669173\n",
      "epoch: 352\n",
      "train oa: 83.55 train loss 0.331793174347015\n",
      " val oa: 0.8366 val_loss: 0.3718754894013079\n",
      "epoch: 353\n",
      "train oa: 83.75 train loss 0.34457573861795543\n",
      " val oa: 0.8362 val_loss: 0.3666464599592599\n",
      "epoch: 354\n",
      "train oa: 84.2 train loss 0.3260030562545625\n",
      " val oa: 0.8332 val_loss: 0.37535736783270934\n",
      "epoch: 355\n",
      "train oa: 84.55 train loss 0.3283974391668547\n",
      " val oa: 0.8456 val_loss: 0.3585547221826117\n",
      "epoch: 356\n",
      "global steps 11400 running loss: 0.013\n",
      "train oa: 83.7 train loss 0.34030494421743307\n",
      " val oa: 0.8416 val_loss: 0.35565465426800147\n",
      "epoch: 357\n",
      "train oa: 82.65 train loss 0.34441203466069326\n",
      " val oa: 0.8548 val_loss: 0.3398539527999821\n",
      "epoch: 358\n",
      "train oa: 84.7 train loss 0.32185186732300697\n",
      " val oa: 0.8484 val_loss: 0.35505233967040223\n",
      "epoch: 359\n",
      "train oa: 83.55 train loss 0.33460590695386605\n",
      " val oa: 0.8476 val_loss: 0.3482098572840894\n",
      "epoch: 360\n",
      "train oa: 83.9 train loss 0.3278605607798881\n",
      " val oa: 0.8588 val_loss: 0.33454524264299473\n",
      "epoch: 361\n",
      "train oa: 83.75 train loss 0.3360079298829809\n",
      " val oa: 0.836 val_loss: 0.3636649379596966\n",
      "epoch: 362\n",
      "global steps 11600 running loss: 0.025\n",
      "train oa: 84.5 train loss 0.3187067674943642\n",
      " val oa: 0.8384 val_loss: 0.36760187839282127\n",
      "epoch: 363\n",
      "train oa: 83.45 train loss 0.3345482781003182\n",
      " val oa: 0.8608 val_loss: 0.33095450141917715\n",
      "epoch: 363 best val accuracy: 0.8608\n",
      "epoch: 364\n",
      "train oa: 83.8 train loss 0.3280947861944899\n",
      " val oa: 0.8502 val_loss: 0.35878719367319384\n",
      "epoch: 365\n",
      "train oa: 84.15 train loss 0.32838873974003696\n",
      " val oa: 0.8666 val_loss: 0.3273559026219374\n",
      "epoch: 365 best val accuracy: 0.8666\n",
      "epoch: 366\n",
      "train oa: 84.05 train loss 0.32563946397262916\n",
      " val oa: 0.8556 val_loss: 0.34708510873503684\n",
      "epoch: 367\n",
      "train oa: 84.45 train loss 0.3231040274351197\n",
      " val oa: 0.8484 val_loss: 0.3542261795263447\n",
      "epoch: 368\n",
      "global steps 11800 running loss: 0.040\n",
      "train oa: 84.8 train loss 0.32698036653693735\n",
      " val oa: 0.842 val_loss: 0.3695616783512271\n",
      "epoch: 369\n",
      "train oa: 83.15 train loss 0.33178123056811687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.8478 val_loss: 0.3469361460939629\n",
      "epoch: 370\n",
      "train oa: 85.45 train loss 0.3167010616597378\n",
      " val oa: 0.8114 val_loss: 0.4055176216120453\n",
      "epoch: 371\n",
      "train oa: 83.95 train loss 0.33708727923459886\n",
      " val oa: 0.8338 val_loss: 0.37193649552080915\n",
      "epoch: 372\n",
      "train oa: 85.1 train loss 0.3173055843291738\n",
      " val oa: 0.8326 val_loss: 0.37144068610982556\n",
      "epoch: 373\n",
      "train oa: 83.85 train loss 0.322655566040339\n",
      " val oa: 0.8554 val_loss: 0.34358855766712765\n",
      "epoch: 374\n",
      "global steps 12000 running loss: 0.051\n",
      "train oa: 83.35 train loss 0.318083341916844\n",
      " val oa: 0.8612 val_loss: 0.3386681940387811\n",
      "epoch: 375\n",
      "train oa: 84.55 train loss 0.3176499613222641\n",
      " val oa: 0.8508 val_loss: 0.3539444311214954\n",
      "epoch: 376\n",
      "train oa: 85.5 train loss 0.30958018420753064\n",
      " val oa: 0.8632 val_loss: 0.34290073117184716\n",
      "epoch: 377\n",
      "train oa: 84.4 train loss 0.32880600918284775\n",
      " val oa: 0.8364 val_loss: 0.3697453171159058\n",
      "epoch: 378\n",
      "train oa: 85.0 train loss 0.32004151935164077\n",
      " val oa: 0.8552 val_loss: 0.34601469907977306\n",
      "epoch: 379\n",
      "train oa: 85.0 train loss 0.3113725636124692\n",
      " val oa: 0.856 val_loss: 0.3347431306352164\n",
      "epoch: 380\n",
      "train oa: 83.9 train loss 0.32678832457334894\n",
      " val oa: 0.8628 val_loss: 0.3364113064824242\n",
      "epoch: 381\n",
      "global steps 12200 running loss: 0.013\n",
      "train oa: 84.75 train loss 0.3309622805345655\n",
      " val oa: 0.8498 val_loss: 0.36350832902494407\n",
      "epoch: 382\n",
      "train oa: 84.15 train loss 0.3240944337241972\n",
      " val oa: 0.8488 val_loss: 0.35077896303229145\n",
      "epoch: 383\n",
      "train oa: 85.05 train loss 0.31820728194130055\n",
      " val oa: 0.8318 val_loss: 0.3776437654857562\n",
      "epoch: 384\n",
      "train oa: 84.3 train loss 0.3255244564644802\n",
      " val oa: 0.8408 val_loss: 0.3520353311785547\n",
      "epoch: 385\n",
      "train oa: 84.75 train loss 0.3170018504797545\n",
      " val oa: 0.8604 val_loss: 0.34582277977265613\n",
      "epoch: 386\n",
      "train oa: 83.45 train loss 0.32817017216522926\n",
      " val oa: 0.8668 val_loss: 0.33501215314017707\n",
      "epoch: 386 best val accuracy: 0.8668\n",
      "epoch: 387\n",
      "global steps 12400 running loss: 0.025\n",
      "train oa: 85.05 train loss 0.31132075154309596\n",
      " val oa: 0.8532 val_loss: 0.341288260600069\n",
      "epoch: 388\n",
      "train oa: 84.2 train loss 0.3227199126383241\n",
      " val oa: 0.8482 val_loss: 0.3447153945983981\n",
      "epoch: 389\n",
      "train oa: 84.4 train loss 0.31614841212768663\n",
      " val oa: 0.8552 val_loss: 0.33769340482438837\n",
      "epoch: 390\n",
      "train oa: 84.5 train loss 0.3047399150435518\n",
      " val oa: 0.8516 val_loss: 0.3514551318527462\n",
      "epoch: 391\n",
      "train oa: 84.1 train loss 0.32566607943879033\n",
      " val oa: 0.8424 val_loss: 0.3638969057951716\n",
      "epoch: 392\n",
      "train oa: 84.15 train loss 0.317132766197011\n",
      " val oa: 0.8652 val_loss: 0.3336799833850217\n",
      "epoch: 393\n",
      "global steps 12600 running loss: 0.038\n",
      "train oa: 86.0 train loss 0.31624143078165\n",
      " val oa: 0.8534 val_loss: 0.3391291036427126\n",
      "epoch: 394\n",
      "train oa: 85.95 train loss 0.3046783165496273\n",
      " val oa: 0.8562 val_loss: 0.34334450539898037\n",
      "epoch: 395\n",
      "train oa: 84.8 train loss 0.3275569852504634\n",
      " val oa: 0.8506 val_loss: 0.3521789080785641\n",
      "epoch: 396\n",
      "train oa: 84.15 train loss 0.3240401790661455\n",
      " val oa: 0.8456 val_loss: 0.363560101931716\n",
      "epoch: 397\n",
      "train oa: 84.45 train loss 0.31863378644511287\n",
      " val oa: 0.8582 val_loss: 0.3570241740946881\n",
      "epoch: 398\n",
      "train oa: 81.95 train loss 0.3407232344975429\n",
      " val oa: 0.8352 val_loss: 0.36483577801739814\n",
      "epoch: 399\n",
      "global steps 12800 running loss: 0.055\n",
      "train oa: 83.25 train loss 0.3423545666493275\n",
      " val oa: 0.8602 val_loss: 0.330216110843543\n",
      "epoch: 400\n",
      "train oa: 84.85 train loss 0.3274998977944044\n",
      " val oa: 0.8494 val_loss: 0.34917965583763133\n",
      "epoch: 401\n",
      "train oa: 85.15 train loss 0.31622963537244014\n",
      " val oa: 0.8444 val_loss: 0.3443325194014425\n",
      "epoch: 402\n",
      "train oa: 85.55 train loss 0.3195608381155234\n",
      " val oa: 0.8308 val_loss: 0.37098508184040674\n",
      "epoch: 403\n",
      "train oa: 83.6 train loss 0.3372734361744336\n",
      " val oa: 0.8448 val_loss: 0.3587843538792686\n",
      "epoch: 404\n",
      "train oa: 84.3 train loss 0.32705549432907316\n",
      " val oa: 0.859 val_loss: 0.3403379314858085\n",
      "epoch: 405\n",
      "train oa: 84.6 train loss 0.3230588583636872\n",
      " val oa: 0.848 val_loss: 0.3573088963158528\n",
      "epoch: 406\n",
      "global steps 13000 running loss: 0.015\n",
      "train oa: 83.5 train loss 0.333663674991177\n",
      " val oa: 0.8562 val_loss: 0.3478400727789991\n",
      "epoch: 407\n",
      "train oa: 84.4 train loss 0.31928930559978563\n",
      " val oa: 0.8542 val_loss: 0.34971065049992056\n",
      "epoch: 408\n",
      "train oa: 84.5 train loss 0.3180920857168529\n",
      " val oa: 0.8488 val_loss: 0.35568301232923516\n",
      "epoch: 409\n",
      "train oa: 84.65 train loss 0.3202274757213091\n",
      " val oa: 0.8588 val_loss: 0.35258935693413973\n",
      "epoch: 410\n",
      "train oa: 84.65 train loss 0.3217002591479034\n",
      " val oa: 0.8566 val_loss: 0.34271712631728835\n",
      "epoch: 411\n",
      "train oa: 84.8 train loss 0.3257585180711408\n",
      " val oa: 0.8452 val_loss: 0.36174582315888004\n",
      "epoch: 412\n",
      "global steps 13200 running loss: 0.028\n",
      "train oa: 82.8 train loss 0.33939641475369414\n",
      " val oa: 0.8352 val_loss: 0.3658511909673783\n",
      "epoch: 413\n",
      "train oa: 85.55 train loss 0.31828146173218813\n",
      " val oa: 0.8356 val_loss: 0.36124088009807315\n",
      "epoch: 414\n",
      "train oa: 84.5 train loss 0.325809844019426\n",
      " val oa: 0.8612 val_loss: 0.3310953727653046\n",
      "epoch: 415\n",
      "train oa: 84.35 train loss 0.323638998228441\n",
      " val oa: 0.8706 val_loss: 0.3241300135978291\n",
      "epoch: 415 best val accuracy: 0.8706\n",
      "epoch: 416\n",
      "train oa: 84.1 train loss 0.324016172535254\n",
      " val oa: 0.8598 val_loss: 0.3543845118638223\n",
      "epoch: 417\n",
      "train oa: 85.75 train loss 0.3091478307545538\n",
      " val oa: 0.855 val_loss: 0.3576851347732005\n",
      "epoch: 418\n",
      "global steps 13400 running loss: 0.040\n",
      "train oa: 84.45 train loss 0.3256397093671047\n",
      " val oa: 0.8526 val_loss: 0.3523441842010492\n",
      "epoch: 419\n",
      "train oa: 85.8 train loss 0.315662868625486\n",
      " val oa: 0.8418 val_loss: 0.3588951946857827\n",
      "epoch: 420\n",
      "train oa: 83.0 train loss 0.31626174923852607\n",
      " val oa: 0.8424 val_loss: 0.37472287574162944\n",
      "epoch: 421\n",
      "train oa: 84.2 train loss 0.3342526739733874\n",
      " val oa: 0.866 val_loss: 0.33162918324437324\n",
      "epoch: 422\n",
      "train oa: 85.05 train loss 0.3127359207199958\n",
      " val oa: 0.8398 val_loss: 0.3633954299682523\n",
      "epoch: 423\n",
      "train oa: 84.4 train loss 0.32872398448648926\n",
      " val oa: 0.8492 val_loss: 0.3537612742104243\n",
      "epoch: 424\n",
      "global steps 13600 running loss: 0.049\n",
      "train oa: 85.15 train loss 0.30566248755285225\n",
      " val oa: 0.852 val_loss: 0.35127902296898633\n",
      "epoch: 425\n",
      "train oa: 85.65 train loss 0.2979974393977957\n",
      " val oa: 0.8552 val_loss: 0.3469767286910157\n",
      "epoch: 426\n",
      "train oa: 84.3 train loss 0.3262135160437298\n",
      " val oa: 0.8442 val_loss: 0.3634497643724964\n",
      "epoch: 427\n",
      "train oa: 84.15 train loss 0.3116051758094051\n",
      " val oa: 0.8414 val_loss: 0.3741785923740308\n",
      "epoch: 428\n",
      "train oa: 85.05 train loss 0.3183912470962115\n",
      " val oa: 0.852 val_loss: 0.3434519946500637\n",
      "epoch: 429\n",
      "train oa: 85.8 train loss 0.303786100878017\n",
      " val oa: 0.8626 val_loss: 0.3386923774256359\n",
      "epoch: 430\n",
      "train oa: 85.45 train loss 0.3136983894014885\n",
      " val oa: 0.8636 val_loss: 0.33448061653745176\n",
      "epoch: 431\n",
      "global steps 13800 running loss: 0.013\n",
      "train oa: 84.3 train loss 0.32872994505804887\n",
      " val oa: 0.862 val_loss: 0.34415996120542547\n",
      "epoch: 432\n",
      "train oa: 86.6 train loss 0.30769114315046314\n",
      " val oa: 0.8554 val_loss: 0.3449698826027884\n",
      "epoch: 433\n",
      "train oa: 86.1 train loss 0.30345254837969854\n",
      " val oa: 0.819 val_loss: 0.38010553450587814\n",
      "epoch: 434\n",
      "train oa: 84.1 train loss 0.3209775857079125\n",
      " val oa: 0.8536 val_loss: 0.3445330060417755\n",
      "epoch: 435\n",
      "train oa: 84.5 train loss 0.3174118024835321\n",
      " val oa: 0.8618 val_loss: 0.3461512523493604\n",
      "epoch: 436\n",
      "train oa: 85.6 train loss 0.30773177254508605\n",
      " val oa: 0.8552 val_loss: 0.33245243047189005\n",
      "epoch: 437\n",
      "global steps 14000 running loss: 0.025\n",
      "train oa: 84.1 train loss 0.3236615140746392\n",
      " val oa: 0.8486 val_loss: 0.3488139483851555\n",
      "epoch: 438\n",
      "train oa: 85.2 train loss 0.32440579633039157\n",
      " val oa: 0.8674 val_loss: 0.32738252878123786\n",
      "epoch: 439\n",
      "train oa: 85.2 train loss 0.31289717100455483\n",
      " val oa: 0.8578 val_loss: 0.34286755514436573\n",
      "epoch: 440\n",
      "train oa: 84.3 train loss 0.3234399633646963\n",
      " val oa: 0.8512 val_loss: 0.34799936746530585\n",
      "epoch: 441\n",
      "train oa: 84.4 train loss 0.3251159881313686\n",
      " val oa: 0.8546 val_loss: 0.3376220614050776\n",
      "epoch: 442\n",
      "train oa: 85.85 train loss 0.2993641677696148\n",
      " val oa: 0.8534 val_loss: 0.3484923767565892\n",
      "epoch: 443\n",
      "global steps 14200 running loss: 0.039\n",
      "train oa: 84.1 train loss 0.3245308694901257\n",
      " val oa: 0.8402 val_loss: 0.36202646616751294\n",
      "epoch: 444\n",
      "train oa: 85.0 train loss 0.31227336888427504\n",
      " val oa: 0.8668 val_loss: 0.3222567374237025\n",
      "epoch: 445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 84.75 train loss 0.3098704580350497\n",
      " val oa: 0.8414 val_loss: 0.3572563238570497\n",
      "epoch: 446\n",
      "train oa: 85.1 train loss 0.31009718498550976\n",
      " val oa: 0.854 val_loss: 0.34785012370205637\n",
      "epoch: 447\n",
      "train oa: 84.65 train loss 0.3116474349361717\n",
      " val oa: 0.8592 val_loss: 0.32853727247193765\n",
      "epoch: 448\n",
      "train oa: 86.1 train loss 0.30995719010203376\n",
      " val oa: 0.811 val_loss: 0.38122829829725896\n",
      "epoch: 449\n",
      "global steps 14400 running loss: 0.051\n",
      "train oa: 84.55 train loss 0.3179947963445193\n",
      " val oa: 0.852 val_loss: 0.34244478758117863\n",
      "epoch: 450\n",
      "train oa: 85.05 train loss 0.2963054130528841\n",
      " val oa: 0.842 val_loss: 0.3532078048116874\n",
      "epoch: 451\n",
      "train oa: 85.2 train loss 0.29884977840279825\n",
      " val oa: 0.8362 val_loss: 0.35765408881845756\n",
      "epoch: 452\n",
      "train oa: 84.45 train loss 0.31363409780898066\n",
      " val oa: 0.8712 val_loss: 0.32542489934877034\n",
      "epoch: 452 best val accuracy: 0.8712\n",
      "epoch: 453\n",
      "train oa: 86.1 train loss 0.3098728934867917\n",
      " val oa: 0.8626 val_loss: 0.340892623120495\n",
      "epoch: 454\n",
      "train oa: 86.2 train loss 0.29393774764350766\n",
      " val oa: 0.8552 val_loss: 0.3327993117168346\n",
      "epoch: 455\n",
      "train oa: 86.2 train loss 0.30718843961786335\n",
      " val oa: 0.8338 val_loss: 0.38364803765298466\n",
      "epoch: 456\n",
      "global steps 14600 running loss: 0.013\n",
      "train oa: 83.85 train loss 0.32107108296177583\n",
      " val oa: 0.8512 val_loss: 0.3538101521089133\n",
      "epoch: 457\n",
      "train oa: 85.75 train loss 0.31081084886814586\n",
      " val oa: 0.8594 val_loss: 0.33222408742497694\n",
      "epoch: 458\n",
      "train oa: 85.55 train loss 0.30246840223206034\n",
      " val oa: 0.8562 val_loss: 0.3332305231172655\n",
      "epoch: 459\n",
      "train oa: 85.05 train loss 0.3084685305670916\n",
      " val oa: 0.863 val_loss: 0.3330829203756819\n",
      "epoch: 460\n",
      "train oa: 84.9 train loss 0.3118120486956133\n",
      " val oa: 0.8628 val_loss: 0.34957822544883355\n",
      "epoch: 461\n",
      "train oa: 84.25 train loss 0.3177972365055171\n",
      " val oa: 0.8536 val_loss: 0.3483560393360275\n",
      "epoch: 462\n",
      "global steps 14800 running loss: 0.024\n",
      "train oa: 85.95 train loss 0.3008994310905098\n",
      " val oa: 0.8686 val_loss: 0.33659852738503665\n",
      "epoch: 463\n",
      "train oa: 85.25 train loss 0.3127691748627297\n",
      " val oa: 0.8666 val_loss: 0.3254778569309367\n",
      "epoch: 464\n",
      "train oa: 85.6 train loss 0.3067019459708203\n",
      " val oa: 0.8408 val_loss: 0.3598230545123043\n",
      "epoch: 465\n",
      "train oa: 85.4 train loss 0.30870240574200475\n",
      " val oa: 0.8506 val_loss: 0.3473873334689184\n",
      "epoch: 466\n",
      "train oa: 85.95 train loss 0.3019985368517981\n",
      " val oa: 0.8658 val_loss: 0.3315321008322574\n",
      "epoch: 467\n",
      "train oa: 85.1 train loss 0.2975170134355595\n",
      " val oa: 0.8294 val_loss: 0.3752520290268913\n",
      "epoch: 468\n",
      "global steps 15000 running loss: 0.038\n",
      "train oa: 84.65 train loss 0.3176756865520829\n",
      " val oa: 0.8228 val_loss: 0.3717675277044626\n",
      "epoch: 469\n",
      "train oa: 85.8 train loss 0.30040780450746346\n",
      " val oa: 0.8538 val_loss: 0.3485017479402844\n",
      "epoch: 470\n",
      "train oa: 85.2 train loss 0.30300464924634146\n",
      " val oa: 0.838 val_loss: 0.371123147909871\n",
      "epoch: 471\n",
      "train oa: 84.3 train loss 0.3189779199702272\n",
      " val oa: 0.8288 val_loss: 0.37733602543461997\n",
      "epoch: 472\n",
      "train oa: 85.9 train loss 0.30350214594556574\n",
      " val oa: 0.8562 val_loss: 0.33391773666212154\n",
      "epoch: 473\n",
      "train oa: 85.75 train loss 0.2931707086317403\n",
      " val oa: 0.8644 val_loss: 0.34042064493801233\n",
      "epoch: 474\n",
      "global steps 15200 running loss: 0.050\n",
      "train oa: 85.45 train loss 0.3104488291184341\n",
      " val oa: 0.8572 val_loss: 0.34497764219528626\n",
      "epoch: 475\n",
      "train oa: 85.05 train loss 0.3064979280255614\n",
      " val oa: 0.868 val_loss: 0.3159030882014584\n",
      "epoch: 476\n",
      "train oa: 86.05 train loss 0.3014561004657614\n",
      " val oa: 0.8628 val_loss: 0.3273519476482721\n",
      "epoch: 477\n",
      "train oa: 87.1 train loss 0.2798790813198443\n",
      " val oa: 0.847 val_loss: 0.36320805094269965\n",
      "epoch: 478\n",
      "train oa: 86.55 train loss 0.28928966934401845\n",
      " val oa: 0.8584 val_loss: 0.33985418839460346\n",
      "epoch: 479\n",
      "train oa: 85.4 train loss 0.2956939226814691\n",
      " val oa: 0.8712 val_loss: 0.3260356287920565\n",
      "epoch: 480\n",
      "train oa: 86.35 train loss 0.2955552084098238\n",
      " val oa: 0.8544 val_loss: 0.33980567383551\n",
      "epoch: 481\n",
      "global steps 15400 running loss: 0.011\n",
      "train oa: 85.2 train loss 0.30002379254668554\n",
      " val oa: 0.8732 val_loss: 0.3270150785118757\n",
      "epoch: 481 best val accuracy: 0.8732\n",
      "epoch: 482\n",
      "train oa: 85.6 train loss 0.28742490374566043\n",
      " val oa: 0.8576 val_loss: 0.3310085517227816\n",
      "epoch: 483\n",
      "train oa: 86.25 train loss 0.28894212800463576\n",
      " val oa: 0.842 val_loss: 0.36109605593638433\n",
      "epoch: 484\n",
      "train oa: 85.4 train loss 0.30031933047781373\n",
      " val oa: 0.8688 val_loss: 0.3294499771913379\n",
      "epoch: 485\n",
      "train oa: 85.0 train loss 0.3028318836501586\n",
      " val oa: 0.8548 val_loss: 0.33897793431204937\n",
      "epoch: 486\n",
      "train oa: 86.2 train loss 0.28241795965436123\n",
      " val oa: 0.8552 val_loss: 0.3361553805247624\n",
      "epoch: 487\n",
      "global steps 15600 running loss: 0.023\n",
      "train oa: 86.05 train loss 0.3016623962161959\n",
      " val oa: 0.8588 val_loss: 0.34274927645451253\n",
      "epoch: 488\n",
      "train oa: 85.0 train loss 0.3018975653847126\n",
      " val oa: 0.8612 val_loss: 0.33935082186798754\n",
      "epoch: 489\n",
      "train oa: 86.0 train loss 0.2976072766026017\n",
      " val oa: 0.8722 val_loss: 0.3129104123126216\n",
      "epoch: 490\n",
      "train oa: 86.0 train loss 0.2958297867198916\n",
      " val oa: 0.869 val_loss: 0.33549572504753905\n",
      "epoch: 491\n",
      "train oa: 86.55 train loss 0.2933520758169039\n",
      " val oa: 0.8576 val_loss: 0.33828634096376803\n",
      "epoch: 492\n",
      "train oa: 85.15 train loss 0.30473656321994974\n",
      " val oa: 0.8562 val_loss: 0.3473306651217316\n",
      "epoch: 493\n",
      "global steps 15800 running loss: 0.036\n",
      "train oa: 85.4 train loss 0.2952214052338164\n",
      " val oa: 0.8564 val_loss: 0.34820650089837846\n",
      "epoch: 494\n",
      "train oa: 86.35 train loss 0.2912809341888314\n",
      " val oa: 0.8542 val_loss: 0.34942169948470797\n",
      "epoch: 495\n",
      "train oa: 84.9 train loss 0.30872048406675767\n",
      " val oa: 0.8586 val_loss: 0.3409538421698326\n",
      "epoch: 496\n",
      "train oa: 84.45 train loss 0.3101414386698671\n",
      " val oa: 0.8414 val_loss: 0.3718002268075865\n",
      "epoch: 497\n",
      "train oa: 85.15 train loss 0.3184441403043668\n",
      " val oa: 0.867 val_loss: 0.3294199126112561\n",
      "epoch: 498\n",
      "train oa: 85.5 train loss 0.3157146094765172\n",
      " val oa: 0.863 val_loss: 0.34600699175505784\n",
      "epoch: 499\n",
      "global steps 16000 running loss: 0.049\n",
      "train oa: 85.8 train loss 0.30349101339723256\n",
      " val oa: 0.841 val_loss: 0.36920246341328483\n",
      "epoch: 500\n",
      "train oa: 84.9 train loss 0.3111329645714888\n",
      " val oa: 0.846 val_loss: 0.3517118967744471\n",
      "epoch: 501\n",
      "train oa: 84.95 train loss 0.29402784579884567\n",
      " val oa: 0.8582 val_loss: 0.34025841506306886\n",
      "epoch: 502\n",
      "train oa: 86.3 train loss 0.28833323406958095\n",
      " val oa: 0.8572 val_loss: 0.34125862253079614\n",
      "epoch: 503\n",
      "train oa: 86.25 train loss 0.29991694547572856\n",
      " val oa: 0.8474 val_loss: 0.3495912216802754\n",
      "epoch: 504\n",
      "train oa: 86.15 train loss 0.29603229029705846\n",
      " val oa: 0.8566 val_loss: 0.3516897736526135\n",
      "epoch: 505\n",
      "train oa: 86.65 train loss 0.28087237945991494\n",
      " val oa: 0.8724 val_loss: 0.33301794685083014\n",
      "epoch: 506\n",
      "global steps 16200 running loss: 0.014\n",
      "train oa: 85.6 train loss 0.3317532978322453\n",
      " val oa: 0.8566 val_loss: 0.3388586805877902\n",
      "epoch: 507\n",
      "train oa: 85.5 train loss 0.28871965106929076\n",
      " val oa: 0.8592 val_loss: 0.3408436860236713\n",
      "epoch: 508\n",
      "train oa: 85.45 train loss 0.3021993975797922\n",
      " val oa: 0.8584 val_loss: 0.33239792840600374\n",
      "epoch: 509\n",
      "train oa: 86.35 train loss 0.2948424873594227\n",
      " val oa: 0.869 val_loss: 0.334769074999739\n",
      "epoch: 510\n",
      "train oa: 85.95 train loss 0.3012288738669678\n",
      " val oa: 0.862 val_loss: 0.3324842082952936\n",
      "epoch: 511\n",
      "train oa: 85.7 train loss 0.2996530262904227\n",
      " val oa: 0.8592 val_loss: 0.33293955412492343\n",
      "epoch: 512\n",
      "global steps 16400 running loss: 0.022\n",
      "train oa: 87.0 train loss 0.281348215138849\n",
      " val oa: 0.8612 val_loss: 0.33132483869443363\n",
      "epoch: 513\n",
      "train oa: 85.85 train loss 0.29620710009329365\n",
      " val oa: 0.8348 val_loss: 0.37522721292196004\n",
      "epoch: 514\n",
      "train oa: 85.7 train loss 0.3107168120658246\n",
      " val oa: 0.854 val_loss: 0.35521528507682104\n",
      "epoch: 515\n",
      "train oa: 84.9 train loss 0.30434482289621906\n",
      " val oa: 0.846 val_loss: 0.3550131689192956\n",
      "epoch: 516\n",
      "train oa: 85.4 train loss 0.30974173461374505\n",
      " val oa: 0.8692 val_loss: 0.3198868692623472\n",
      "epoch: 517\n",
      "train oa: 86.85 train loss 0.2845771749075997\n",
      " val oa: 0.8676 val_loss: 0.33099007230686356\n",
      "epoch: 518\n",
      "global steps 16600 running loss: 0.035\n",
      "train oa: 85.55 train loss 0.2883279864132815\n",
      " val oa: 0.8576 val_loss: 0.3471904920049062\n",
      "epoch: 519\n",
      "train oa: 86.75 train loss 0.28690287618847354\n",
      " val oa: 0.8724 val_loss: 0.3252682559143679\n",
      "epoch: 520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 86.3 train loss 0.28398832585937667\n",
      " val oa: 0.872 val_loss: 0.3155147302624044\n",
      "epoch: 521\n",
      "train oa: 85.65 train loss 0.2939517146866016\n",
      " val oa: 0.8648 val_loss: 0.34492793328219107\n",
      "epoch: 522\n",
      "train oa: 86.4 train loss 0.29157948810373685\n",
      " val oa: 0.8682 val_loss: 0.34032496219086716\n",
      "epoch: 523\n",
      "train oa: 86.15 train loss 0.2920199246155253\n",
      " val oa: 0.8634 val_loss: 0.334695672771635\n",
      "epoch: 524\n",
      "global steps 16800 running loss: 0.047\n",
      "train oa: 85.8 train loss 0.2963143483998534\n",
      " val oa: 0.8656 val_loss: 0.32736461289994867\n",
      "epoch: 525\n",
      "train oa: 86.85 train loss 0.2901392927115101\n",
      " val oa: 0.8502 val_loss: 0.3528165272984355\n",
      "epoch: 526\n",
      "train oa: 85.95 train loss 0.28795770969677614\n",
      " val oa: 0.8292 val_loss: 0.3932477318714203\n",
      "epoch: 527\n",
      "train oa: 85.1 train loss 0.30639341195451497\n",
      " val oa: 0.8696 val_loss: 0.3227632777746745\n",
      "epoch: 528\n",
      "train oa: 86.4 train loss 0.28700077876180186\n",
      " val oa: 0.8598 val_loss: 0.3318890735831329\n",
      "epoch: 529\n",
      "train oa: 86.75 train loss 0.29044076275205616\n",
      " val oa: 0.8692 val_loss: 0.3211972690693783\n",
      "epoch: 530\n",
      "train oa: 85.85 train loss 0.2998020224869994\n",
      " val oa: 0.871 val_loss: 0.31428661650354767\n",
      "epoch: 531\n",
      "global steps 17000 running loss: 0.012\n",
      "train oa: 87.0 train loss 0.2801997717118131\n",
      " val oa: 0.8598 val_loss: 0.3316711072350516\n",
      "epoch: 532\n",
      "train oa: 87.15 train loss 0.27673706449874813\n",
      " val oa: 0.862 val_loss: 0.3288537115787988\n",
      "epoch: 533\n",
      "train oa: 86.55 train loss 0.3004493033217967\n",
      " val oa: 0.857 val_loss: 0.3436985922753269\n",
      "epoch: 534\n",
      "train oa: 86.2 train loss 0.28741435371788426\n",
      " val oa: 0.8594 val_loss: 0.3350139382732524\n",
      "epoch: 535\n",
      "train oa: 86.5 train loss 0.28944304330289927\n",
      " val oa: 0.8586 val_loss: 0.33918156483520157\n",
      "epoch: 536\n",
      "train oa: 86.1 train loss 0.2875152004667966\n",
      " val oa: 0.854 val_loss: 0.3494944242607985\n",
      "epoch: 537\n",
      "global steps 17200 running loss: 0.021\n",
      "train oa: 86.15 train loss 0.28357544925311134\n",
      " val oa: 0.8668 val_loss: 0.32939186909987084\n",
      "epoch: 538\n",
      "train oa: 86.3 train loss 0.2943782957910757\n",
      " val oa: 0.868 val_loss: 0.328270697265651\n",
      "epoch: 539\n",
      "train oa: 86.6 train loss 0.28000879845522486\n",
      " val oa: 0.8698 val_loss: 0.32284816853538895\n",
      "epoch: 540\n",
      "train oa: 86.9 train loss 0.29700960185538\n",
      " val oa: 0.8712 val_loss: 0.31295823481051455\n",
      "epoch: 541\n",
      "train oa: 86.35 train loss 0.28400125904916484\n",
      " val oa: 0.8646 val_loss: 0.327113425943628\n",
      "epoch: 542\n",
      "train oa: 86.45 train loss 0.28910747825417\n",
      " val oa: 0.8646 val_loss: 0.32623619346331356\n",
      "epoch: 543\n",
      "global steps 17400 running loss: 0.035\n",
      "train oa: 86.25 train loss 0.28794404283928726\n",
      " val oa: 0.8434 val_loss: 0.3495650526225229\n",
      "epoch: 544\n",
      "train oa: 86.4 train loss 0.28462603537420944\n",
      " val oa: 0.8586 val_loss: 0.3287270490533186\n",
      "epoch: 545\n",
      "train oa: 85.35 train loss 0.3005864363322405\n",
      " val oa: 0.8588 val_loss: 0.33442350791760594\n",
      "epoch: 546\n",
      "train oa: 85.15 train loss 0.3006757987858389\n",
      " val oa: 0.8684 val_loss: 0.3252764989267691\n",
      "epoch: 547\n",
      "train oa: 86.55 train loss 0.2893719230469934\n",
      " val oa: 0.8706 val_loss: 0.3293325144628508\n",
      "epoch: 548\n",
      "train oa: 87.1 train loss 0.281333451800862\n",
      " val oa: 0.8692 val_loss: 0.3312259127250258\n",
      "epoch: 549\n",
      "global steps 17600 running loss: 0.044\n",
      "train oa: 87.2 train loss 0.27524276958639593\n",
      " val oa: 0.8262 val_loss: 0.42278592587105507\n",
      "epoch: 550\n",
      "train oa: 86.05 train loss 0.29698694315261176\n",
      " val oa: 0.8584 val_loss: 0.331733705914743\n",
      "epoch: 551\n",
      "train oa: 85.85 train loss 0.2930009592709957\n",
      " val oa: 0.851 val_loss: 0.336770426783283\n",
      "epoch: 552\n",
      "train oa: 86.7 train loss 0.2762422595550719\n",
      " val oa: 0.8674 val_loss: 0.31552242481691123\n",
      "epoch: 553\n",
      "train oa: 86.7 train loss 0.28358460112435363\n",
      " val oa: 0.864 val_loss: 0.3351288876311136\n",
      "epoch: 554\n",
      "train oa: 87.3 train loss 0.27602957811466905\n",
      " val oa: 0.85 val_loss: 0.3479279498429543\n",
      "epoch: 555\n",
      "train oa: 87.25 train loss 0.28434592039312456\n",
      " val oa: 0.8576 val_loss: 0.3453706968907209\n",
      "epoch: 556\n",
      "global steps 17800 running loss: 0.013\n",
      "train oa: 86.2 train loss 0.2978595787345951\n",
      " val oa: 0.8768 val_loss: 0.30818531185564585\n",
      "epoch: 556 best val accuracy: 0.8768\n",
      "epoch: 557\n",
      "train oa: 86.0 train loss 0.29670347071187464\n",
      " val oa: 0.8642 val_loss: 0.33211976801075677\n",
      "epoch: 558\n",
      "train oa: 86.4 train loss 0.28524760819676537\n",
      " val oa: 0.88 val_loss: 0.3151514349221956\n",
      "epoch: 558 best val accuracy: 0.88\n",
      "epoch: 559\n",
      "train oa: 85.95 train loss 0.28330805740974463\n",
      " val oa: 0.8738 val_loss: 0.322881716757823\n",
      "epoch: 560\n",
      "train oa: 86.8 train loss 0.2793431915120204\n",
      " val oa: 0.8748 val_loss: 0.31566782772617563\n",
      "epoch: 561\n",
      "train oa: 86.5 train loss 0.28610456772426884\n",
      " val oa: 0.8636 val_loss: 0.3368423396856054\n",
      "epoch: 562\n",
      "global steps 18000 running loss: 0.023\n",
      "train oa: 85.5 train loss 0.29669426188239584\n",
      " val oa: 0.8786 val_loss: 0.31401365957706356\n",
      "epoch: 563\n",
      "train oa: 86.65 train loss 0.2775013784976506\n",
      " val oa: 0.8812 val_loss: 0.29961419420938373\n",
      "epoch: 563 best val accuracy: 0.8812\n",
      "epoch: 564\n",
      "train oa: 86.9 train loss 0.2755920705234548\n",
      " val oa: 0.8758 val_loss: 0.3225137484042789\n",
      "epoch: 565\n",
      "train oa: 86.85 train loss 0.28495483832045715\n",
      " val oa: 0.8534 val_loss: 0.34121808007540877\n",
      "epoch: 566\n",
      "train oa: 86.4 train loss 0.2898321032632752\n",
      " val oa: 0.8594 val_loss: 0.33701082755769424\n",
      "epoch: 567\n",
      "train oa: 88.0 train loss 0.2687454363766522\n",
      " val oa: 0.8688 val_loss: 0.33365605026451706\n",
      "epoch: 568\n",
      "global steps 18200 running loss: 0.037\n",
      "train oa: 86.0 train loss 0.29735709890685397\n",
      " val oa: 0.8674 val_loss: 0.32390310662884403\n",
      "epoch: 569\n",
      "train oa: 85.2 train loss 0.29826394896938346\n",
      " val oa: 0.8588 val_loss: 0.3347103353308018\n",
      "epoch: 570\n",
      "train oa: 87.35 train loss 0.2728675642149259\n",
      " val oa: 0.8714 val_loss: 0.3157317533289317\n",
      "epoch: 571\n",
      "train oa: 86.6 train loss 0.2840303150599654\n",
      " val oa: 0.8648 val_loss: 0.33386903999185086\n",
      "epoch: 572\n",
      "train oa: 87.6 train loss 0.2727997130097869\n",
      " val oa: 0.8682 val_loss: 0.32307731817080415\n",
      "epoch: 573\n",
      "train oa: 86.5 train loss 0.28597590486503216\n",
      " val oa: 0.8684 val_loss: 0.3278031039038725\n",
      "epoch: 574\n",
      "global steps 18400 running loss: 0.044\n",
      "train oa: 87.15 train loss 0.27505708658301176\n",
      " val oa: 0.86 val_loss: 0.34458258999285657\n",
      "epoch: 575\n",
      "train oa: 86.4 train loss 0.28460397384997416\n",
      " val oa: 0.8706 val_loss: 0.3215479790846924\n",
      "epoch: 576\n",
      "train oa: 86.5 train loss 0.27408885112567505\n",
      " val oa: 0.857 val_loss: 0.3408671884945772\n",
      "epoch: 577\n",
      "train oa: 87.0 train loss 0.2818434635637414\n",
      " val oa: 0.8788 val_loss: 0.3078173591687809\n",
      "epoch: 578\n",
      "train oa: 87.8 train loss 0.2738117945420779\n",
      " val oa: 0.8574 val_loss: 0.36179114804946827\n",
      "epoch: 579\n",
      "train oa: 85.95 train loss 0.292097817306139\n",
      " val oa: 0.8746 val_loss: 0.31352634711565025\n",
      "epoch: 580\n",
      "train oa: 87.0 train loss 0.2780211730680631\n",
      " val oa: 0.8742 val_loss: 0.3195253917195073\n",
      "epoch: 581\n",
      "global steps 18600 running loss: 0.011\n",
      "train oa: 87.8 train loss 0.2731101508225446\n",
      " val oa: 0.8652 val_loss: 0.3230789485099826\n",
      "epoch: 582\n",
      "train oa: 87.35 train loss 0.27553705612541374\n",
      " val oa: 0.8808 val_loss: 0.3098515791172014\n",
      "epoch: 583\n",
      "train oa: 86.0 train loss 0.2825940298235098\n",
      " val oa: 0.8462 val_loss: 0.35336910438023555\n",
      "epoch: 584\n",
      "train oa: 86.9 train loss 0.28097681939354135\n",
      " val oa: 0.8694 val_loss: 0.31942421852935954\n",
      "epoch: 585\n",
      "train oa: 87.25 train loss 0.2746072828650892\n",
      " val oa: 0.8648 val_loss: 0.3308556733733736\n",
      "epoch: 586\n",
      "train oa: 86.35 train loss 0.2739206533251099\n",
      " val oa: 0.8674 val_loss: 0.3225750391664525\n",
      "epoch: 587\n",
      "global steps 18800 running loss: 0.023\n",
      "train oa: 86.35 train loss 0.29071204064286627\n",
      " val oa: 0.8754 val_loss: 0.30311792783635716\n",
      "epoch: 588\n",
      "train oa: 87.3 train loss 0.2744078113846479\n",
      " val oa: 0.877 val_loss: 0.30183225238832456\n",
      "epoch: 589\n",
      "train oa: 86.75 train loss 0.27785611767365487\n",
      " val oa: 0.8626 val_loss: 0.3410752894518636\n",
      "epoch: 590\n",
      "train oa: 86.75 train loss 0.26659786243665057\n",
      " val oa: 0.8762 val_loss: 0.32026890934158303\n",
      "epoch: 591\n",
      "train oa: 87.6 train loss 0.2602443039958314\n",
      " val oa: 0.8616 val_loss: 0.3320262232099649\n",
      "epoch: 592\n",
      "train oa: 87.05 train loss 0.2863963418587838\n",
      " val oa: 0.8744 val_loss: 0.31566737866903305\n",
      "epoch: 593\n",
      "global steps 19000 running loss: 0.032\n",
      "train oa: 86.7 train loss 0.27357008421670426\n",
      " val oa: 0.8706 val_loss: 0.33472130959058405\n",
      "epoch: 594\n",
      "train oa: 88.85 train loss 0.26065137130267263\n",
      " val oa: 0.861 val_loss: 0.3541801923091594\n",
      "epoch: 595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 86.95 train loss 0.2853128707920825\n",
      " val oa: 0.8766 val_loss: 0.3175226200060095\n",
      "epoch: 596\n",
      "train oa: 87.65 train loss 0.2606150918741315\n",
      " val oa: 0.8798 val_loss: 0.3011410501789512\n",
      "epoch: 597\n",
      "train oa: 87.3 train loss 0.2726576503390028\n",
      " val oa: 0.8578 val_loss: 0.34124962099125805\n",
      "epoch: 598\n",
      "train oa: 87.3 train loss 0.27762206488461744\n",
      " val oa: 0.8738 val_loss: 0.32018489859495664\n",
      "epoch: 599\n",
      "global steps 19200 running loss: 0.044\n",
      "train oa: 87.05 train loss 0.2754546780204539\n",
      " val oa: 0.869 val_loss: 0.3185222258754239\n",
      "epoch: 600\n",
      "train oa: 86.65 train loss 0.2806420496053734\n",
      " val oa: 0.8694 val_loss: 0.3249132013540218\n",
      "epoch: 601\n",
      "train oa: 86.4 train loss 0.28228664053902613\n",
      " val oa: 0.8742 val_loss: 0.3252887548293245\n",
      "epoch: 602\n",
      "train oa: 87.0 train loss 0.27408234523131775\n",
      " val oa: 0.8662 val_loss: 0.32135630718433117\n",
      "epoch: 603\n",
      "train oa: 88.15 train loss 0.26376402370831376\n",
      " val oa: 0.8638 val_loss: 0.32986209841389863\n",
      "epoch: 604\n",
      "train oa: 86.45 train loss 0.28442342775179197\n",
      " val oa: 0.8756 val_loss: 0.32130761197771335\n",
      "epoch: 605\n",
      "train oa: 87.15 train loss 0.28484911356098896\n",
      " val oa: 0.8534 val_loss: 0.34808225489183403\n",
      "epoch: 606\n",
      "global steps 19400 running loss: 0.011\n",
      "train oa: 87.5 train loss 0.27759708402462524\n",
      " val oa: 0.8658 val_loss: 0.3282849564239975\n",
      "epoch: 607\n",
      "train oa: 87.15 train loss 0.28747191159303137\n",
      " val oa: 0.8602 val_loss: 0.32314670226657916\n",
      "epoch: 608\n",
      "train oa: 87.15 train loss 0.2720102272809046\n",
      " val oa: 0.8794 val_loss: 0.3004739262514355\n",
      "epoch: 609\n",
      "train oa: 87.45 train loss 0.27566156538541464\n",
      " val oa: 0.882 val_loss: 0.310176607408612\n",
      "epoch: 609 best val accuracy: 0.882\n",
      "epoch: 610\n",
      "train oa: 87.3 train loss 0.2721864041865166\n",
      " val oa: 0.8786 val_loss: 0.3161466953043389\n",
      "epoch: 611\n",
      "train oa: 87.0 train loss 0.27534285803575315\n",
      " val oa: 0.8784 val_loss: 0.3050658524416502\n",
      "epoch: 612\n",
      "global steps 19600 running loss: 0.022\n",
      "train oa: 87.55 train loss 0.26206976278245875\n",
      " val oa: 0.8816 val_loss: 0.3120848472715268\n",
      "epoch: 613\n",
      "train oa: 87.75 train loss 0.2711815029206321\n",
      " val oa: 0.8566 val_loss: 0.34441507182321035\n",
      "epoch: 614\n",
      "train oa: 86.9 train loss 0.2834100720133154\n",
      " val oa: 0.8678 val_loss: 0.3293376493510402\n",
      "epoch: 615\n",
      "train oa: 88.0 train loss 0.2580971084178207\n",
      " val oa: 0.888 val_loss: 0.30981177406695726\n",
      "epoch: 615 best val accuracy: 0.888\n",
      "epoch: 616\n",
      "train oa: 87.65 train loss 0.2695253474453469\n",
      " val oa: 0.8628 val_loss: 0.32637142320426865\n",
      "epoch: 617\n",
      "train oa: 86.55 train loss 0.28151894581916115\n",
      " val oa: 0.8702 val_loss: 0.3339167423267747\n",
      "epoch: 618\n",
      "global steps 19800 running loss: 0.032\n",
      "train oa: 88.15 train loss 0.2661041921280174\n",
      " val oa: 0.884 val_loss: 0.30275172430139946\n",
      "epoch: 619\n",
      "train oa: 87.6 train loss 0.28263201760059803\n",
      " val oa: 0.8798 val_loss: 0.30001812890718177\n",
      "epoch: 620\n",
      "train oa: 87.35 train loss 0.2804760705172547\n",
      " val oa: 0.8774 val_loss: 0.31708087250525\n",
      "epoch: 621\n",
      "train oa: 87.9 train loss 0.26269566993393384\n",
      " val oa: 0.8652 val_loss: 0.3319600375225141\n",
      "epoch: 622\n",
      "train oa: 87.7 train loss 0.26139183742065936\n",
      " val oa: 0.8744 val_loss: 0.31623760775458337\n",
      "epoch: 623\n",
      "train oa: 87.7 train loss 0.2582378715437102\n",
      " val oa: 0.8852 val_loss: 0.3011630488203347\n",
      "epoch: 624\n",
      "global steps 20000 running loss: 0.042\n",
      "train oa: 87.95 train loss 0.2625721167227828\n",
      " val oa: 0.8856 val_loss: 0.304795575789509\n",
      "epoch: 625\n",
      "train oa: 85.35 train loss 0.29909695368105244\n",
      " val oa: 0.8488 val_loss: 0.3604576460891521\n",
      "epoch: 626\n",
      "train oa: 87.0 train loss 0.26968002388745677\n",
      " val oa: 0.8664 val_loss: 0.3222364790379107\n",
      "epoch: 627\n",
      "train oa: 87.6 train loss 0.25522708096048696\n",
      " val oa: 0.8734 val_loss: 0.31689187134110547\n",
      "epoch: 628\n",
      "train oa: 87.45 train loss 0.27981683462721657\n",
      " val oa: 0.8564 val_loss: 0.3556807065812427\n",
      "epoch: 629\n",
      "train oa: 86.7 train loss 0.27961165719660724\n",
      " val oa: 0.863 val_loss: 0.3347852704270469\n",
      "epoch: 630\n",
      "train oa: 87.25 train loss 0.27001405889702973\n",
      " val oa: 0.8744 val_loss: 0.3290104650110486\n",
      "epoch: 631\n",
      "global steps 20200 running loss: 0.011\n",
      "train oa: 88.7 train loss 0.25060213056943226\n",
      " val oa: 0.8814 val_loss: 0.30445743273321396\n",
      "epoch: 632\n",
      "train oa: 87.35 train loss 0.2795979652214478\n",
      " val oa: 0.8762 val_loss: 0.30833022958589895\n",
      "epoch: 633\n",
      "train oa: 88.5 train loss 0.25509827717416617\n",
      " val oa: 0.8778 val_loss: 0.30950436497173767\n",
      "epoch: 634\n",
      "train oa: 87.5 train loss 0.27784921716135286\n",
      " val oa: 0.8704 val_loss: 0.3223618076851031\n",
      "epoch: 635\n",
      "train oa: 88.1 train loss 0.25513639885854217\n",
      " val oa: 0.8772 val_loss: 0.31364262028248846\n",
      "epoch: 636\n",
      "train oa: 87.75 train loss 0.27048676526520066\n",
      " val oa: 0.8654 val_loss: 0.348148987571587\n",
      "epoch: 637\n",
      "global steps 20400 running loss: 0.022\n",
      "train oa: 87.9 train loss 0.27405931001892725\n",
      " val oa: 0.8634 val_loss: 0.33755257424084384\n",
      "epoch: 638\n",
      "train oa: 87.9 train loss 0.25828771873273687\n",
      " val oa: 0.865 val_loss: 0.3476159643264309\n",
      "epoch: 639\n",
      "train oa: 87.8 train loss 0.25955195049722013\n",
      " val oa: 0.8736 val_loss: 0.3219312486930093\n",
      "epoch: 640\n",
      "train oa: 87.35 train loss 0.2601288066032212\n",
      " val oa: 0.8682 val_loss: 0.3560790559378071\n",
      "epoch: 641\n",
      "train oa: 87.45 train loss 0.26960973509549985\n",
      " val oa: 0.8706 val_loss: 0.30826308750977355\n",
      "epoch: 642\n",
      "train oa: 87.55 train loss 0.26004517811025346\n",
      " val oa: 0.875 val_loss: 0.29626612994504503\n",
      "epoch: 643\n",
      "global steps 20600 running loss: 0.031\n",
      "train oa: 88.05 train loss 0.26127206788488916\n",
      " val oa: 0.8736 val_loss: 0.332250568351922\n",
      "epoch: 644\n",
      "train oa: 87.6 train loss 0.2562722733238182\n",
      " val oa: 0.8722 val_loss: 0.3239757048729632\n",
      "epoch: 645\n",
      "train oa: 88.3 train loss 0.26325404285481957\n",
      " val oa: 0.877 val_loss: 0.3086642807073432\n",
      "epoch: 646\n",
      "train oa: 86.6 train loss 0.28213923188727646\n",
      " val oa: 0.8766 val_loss: 0.3303962639729106\n",
      "epoch: 647\n",
      "train oa: 87.8 train loss 0.27179645096723826\n",
      " val oa: 0.886 val_loss: 0.30347993149346547\n",
      "epoch: 648\n",
      "train oa: 88.8 train loss 0.2478331173791618\n",
      " val oa: 0.883 val_loss: 0.3056921569245942\n",
      "epoch: 649\n",
      "global steps 20800 running loss: 0.043\n",
      "train oa: 87.6 train loss 0.26881144384318795\n",
      " val oa: 0.8702 val_loss: 0.316852923215687\n",
      "epoch: 650\n",
      "train oa: 87.6 train loss 0.2556401266797588\n",
      " val oa: 0.869 val_loss: 0.330712767850619\n",
      "epoch: 651\n",
      "train oa: 87.85 train loss 0.25659304089092333\n",
      " val oa: 0.873 val_loss: 0.31026413616339005\n",
      "epoch: 652\n",
      "train oa: 87.6 train loss 0.259475253675346\n",
      " val oa: 0.882 val_loss: 0.3000007031347161\n",
      "epoch: 653\n",
      "train oa: 88.0 train loss 0.26620819308693405\n",
      " val oa: 0.8588 val_loss: 0.35779247724480084\n",
      "epoch: 654\n",
      "train oa: 87.65 train loss 0.263750693719523\n",
      " val oa: 0.8738 val_loss: 0.312597186098012\n",
      "epoch: 655\n",
      "train oa: 87.85 train loss 0.2663946445756135\n",
      " val oa: 0.851 val_loss: 0.3505437987148264\n",
      "epoch: 656\n",
      "global steps 21000 running loss: 0.011\n",
      "train oa: 88.65 train loss 0.2571971945775497\n",
      " val oa: 0.8676 val_loss: 0.33103357368768177\n",
      "epoch: 657\n",
      "train oa: 87.5 train loss 0.26686909581669027\n",
      " val oa: 0.8712 val_loss: 0.3368190416350609\n",
      "epoch: 658\n",
      "train oa: 87.75 train loss 0.27624688706076794\n",
      " val oa: 0.876 val_loss: 0.30436115100001554\n",
      "epoch: 659\n",
      "train oa: 88.55 train loss 0.24672832631825686\n",
      " val oa: 0.8728 val_loss: 0.31503285085699534\n",
      "epoch: 660\n",
      "train oa: 88.75 train loss 0.2481251605458287\n",
      " val oa: 0.867 val_loss: 0.3440634562935416\n",
      "epoch: 661\n",
      "train oa: 87.35 train loss 0.26053044264785774\n",
      " val oa: 0.8894 val_loss: 0.29790049048589645\n",
      "epoch: 661 best val accuracy: 0.8894\n",
      "epoch: 662\n",
      "global steps 21200 running loss: 0.020\n",
      "train oa: 86.85 train loss 0.26303005556633124\n",
      " val oa: 0.8858 val_loss: 0.2971937029689912\n",
      "epoch: 663\n",
      "train oa: 88.1 train loss 0.25720738276033484\n",
      " val oa: 0.8828 val_loss: 0.30346093395843415\n",
      "epoch: 664\n",
      "train oa: 89.2 train loss 0.24855368856918422\n",
      " val oa: 0.882 val_loss: 0.3047760746855482\n",
      "epoch: 665\n",
      "train oa: 87.95 train loss 0.25879545532413634\n",
      " val oa: 0.8668 val_loss: 0.3218696413059875\n",
      "epoch: 666\n",
      "train oa: 88.45 train loss 0.256787299638091\n",
      " val oa: 0.8712 val_loss: 0.3165727233986768\n",
      "epoch: 667\n",
      "train oa: 89.15 train loss 0.24611319452890587\n",
      " val oa: 0.8844 val_loss: 0.3035787122975052\n",
      "epoch: 668\n",
      "global steps 21400 running loss: 0.031\n",
      "train oa: 87.95 train loss 0.26598107566532786\n",
      " val oa: 0.869 val_loss: 0.3200168006347721\n",
      "epoch: 669\n",
      "train oa: 88.35 train loss 0.24778895334513118\n",
      " val oa: 0.8724 val_loss: 0.3178869751472064\n",
      "epoch: 670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 88.8 train loss 0.2522906880583912\n",
      " val oa: 0.8776 val_loss: 0.3209800872419151\n",
      "epoch: 671\n",
      "train oa: 87.25 train loss 0.2541897477488941\n",
      " val oa: 0.8728 val_loss: 0.3148745768801827\n",
      "epoch: 672\n",
      "train oa: 88.25 train loss 0.2560247357004346\n",
      " val oa: 0.8858 val_loss: 0.2964130641699267\n",
      "epoch: 673\n",
      "train oa: 89.4 train loss 0.24486285253104775\n",
      " val oa: 0.8764 val_loss: 0.3159460311275343\n",
      "epoch: 674\n",
      "global steps 21600 running loss: 0.041\n",
      "train oa: 88.55 train loss 0.2553426531421615\n",
      " val oa: 0.8776 val_loss: 0.30581688489196496\n",
      "epoch: 675\n",
      "train oa: 87.7 train loss 0.2619152864913545\n",
      " val oa: 0.883 val_loss: 0.2982865953022518\n",
      "epoch: 676\n",
      "train oa: 88.65 train loss 0.24184288467414325\n",
      " val oa: 0.8682 val_loss: 0.31483046622056354\n",
      "epoch: 677\n",
      "train oa: 88.65 train loss 0.25561091924685736\n",
      " val oa: 0.867 val_loss: 0.33619171388512703\n",
      "epoch: 678\n",
      "train oa: 87.65 train loss 0.2602515180266524\n",
      " val oa: 0.8768 val_loss: 0.31294431288377184\n",
      "epoch: 679\n",
      "train oa: 88.35 train loss 0.2555618733294781\n",
      " val oa: 0.884 val_loss: 0.3048032402310851\n",
      "epoch: 680\n",
      "train oa: 87.8 train loss 0.2623479838045704\n",
      " val oa: 0.884 val_loss: 0.3115110112454674\n",
      "epoch: 681\n",
      "global steps 21800 running loss: 0.009\n",
      "train oa: 88.5 train loss 0.2633850778041347\n",
      " val oa: 0.8642 val_loss: 0.323257317011623\n",
      "epoch: 682\n",
      "train oa: 86.6 train loss 0.28695843627860845\n",
      " val oa: 0.8868 val_loss: 0.2950822941917815\n",
      "epoch: 683\n",
      "train oa: 87.8 train loss 0.25073225615953076\n",
      " val oa: 0.8816 val_loss: 0.3019465691690924\n",
      "epoch: 684\n",
      "train oa: 88.65 train loss 0.24917436556052852\n",
      " val oa: 0.8776 val_loss: 0.3160795881321393\n",
      "epoch: 685\n",
      "train oa: 87.9 train loss 0.26136469986768285\n",
      " val oa: 0.8806 val_loss: 0.29607264533122335\n",
      "epoch: 686\n",
      "train oa: 88.25 train loss 0.25679101694806467\n",
      " val oa: 0.8848 val_loss: 0.3066489461360472\n",
      "epoch: 687\n",
      "global steps 22000 running loss: 0.020\n",
      "train oa: 88.95 train loss 0.2513653164742267\n",
      " val oa: 0.8726 val_loss: 0.3214630580850619\n",
      "epoch: 688\n",
      "train oa: 88.85 train loss 0.25378065615425727\n",
      " val oa: 0.8732 val_loss: 0.3152860991880737\n",
      "epoch: 689\n",
      "train oa: 88.0 train loss 0.2669413530848137\n",
      " val oa: 0.8814 val_loss: 0.3084162813008333\n",
      "epoch: 690\n",
      "train oa: 88.75 train loss 0.2482556414098805\n",
      " val oa: 0.8646 val_loss: 0.3342081019325238\n",
      "epoch: 691\n",
      "train oa: 88.0 train loss 0.2596413717556109\n",
      " val oa: 0.869 val_loss: 0.3264889504486302\n",
      "epoch: 692\n",
      "train oa: 88.6 train loss 0.25554323701915865\n",
      " val oa: 0.8902 val_loss: 0.28623175955567576\n",
      "epoch: 692 best val accuracy: 0.8902\n",
      "epoch: 693\n",
      "global steps 22200 running loss: 0.031\n",
      "train oa: 88.25 train loss 0.253161863208651\n",
      " val oa: 0.8904 val_loss: 0.2854160415544371\n",
      "epoch: 693 best val accuracy: 0.8904\n",
      "epoch: 694\n",
      "train oa: 87.65 train loss 0.25560500831288474\n",
      " val oa: 0.895 val_loss: 0.2905567519747221\n",
      "epoch: 694 best val accuracy: 0.895\n",
      "epoch: 695\n",
      "train oa: 88.35 train loss 0.24792444506224232\n",
      " val oa: 0.8806 val_loss: 0.30289846352301686\n",
      "epoch: 696\n",
      "train oa: 88.5 train loss 0.263580577027777\n",
      " val oa: 0.873 val_loss: 0.312817911719654\n",
      "epoch: 697\n",
      "train oa: 88.2 train loss 0.2507067914095144\n",
      " val oa: 0.881 val_loss: 0.32063496334261765\n",
      "epoch: 698\n",
      "train oa: 88.7 train loss 0.2562749916658171\n",
      " val oa: 0.8842 val_loss: 0.29329597386281053\n",
      "epoch: 699\n",
      "global steps 22400 running loss: 0.040\n",
      "train oa: 87.65 train loss 0.2525383874965395\n",
      " val oa: 0.8592 val_loss: 0.34022879303114756\n",
      "epoch: 700\n",
      "train oa: 89.3 train loss 0.24483383207157156\n",
      " val oa: 0.8872 val_loss: 0.29398127845518923\n",
      "epoch: 701\n",
      "train oa: 88.2 train loss 0.2518349918691668\n",
      " val oa: 0.888 val_loss: 0.2927491290151318\n",
      "epoch: 702\n",
      "train oa: 88.85 train loss 0.24514584894117764\n",
      " val oa: 0.8866 val_loss: 0.28705329729021534\n",
      "epoch: 703\n",
      "train oa: 88.9 train loss 0.2479257174593664\n",
      " val oa: 0.8682 val_loss: 0.32500520596135973\n",
      "epoch: 704\n",
      "train oa: 89.1 train loss 0.23945008472056462\n",
      " val oa: 0.8784 val_loss: 0.3059276304551447\n",
      "epoch: 705\n",
      "train oa: 89.0 train loss 0.24627001611348515\n",
      " val oa: 0.8786 val_loss: 0.31667554035171774\n",
      "epoch: 706\n",
      "global steps 22600 running loss: 0.009\n",
      "train oa: 89.25 train loss 0.2430634196858198\n",
      " val oa: 0.8738 val_loss: 0.32529771026782683\n",
      "epoch: 707\n",
      "train oa: 87.7 train loss 0.2767820222946365\n",
      " val oa: 0.877 val_loss: 0.32289834855780103\n",
      "epoch: 708\n",
      "train oa: 89.45 train loss 0.2465547410480643\n",
      " val oa: 0.8828 val_loss: 0.29821853029945483\n",
      "epoch: 709\n",
      "train oa: 88.8 train loss 0.25021449011612423\n",
      " val oa: 0.8838 val_loss: 0.30554546170661895\n",
      "epoch: 710\n",
      "train oa: 89.3 train loss 0.23448262997468755\n",
      " val oa: 0.8878 val_loss: 0.2918993883145973\n",
      "epoch: 711\n",
      "train oa: 88.75 train loss 0.24754852502634983\n",
      " val oa: 0.8836 val_loss: 0.31467565325190394\n",
      "epoch: 712\n",
      "global steps 22800 running loss: 0.019\n",
      "train oa: 88.7 train loss 0.24608951933813217\n",
      " val oa: 0.8724 val_loss: 0.3142878924440202\n",
      "epoch: 713\n",
      "train oa: 89.55 train loss 0.23241014357379838\n",
      " val oa: 0.8772 val_loss: 0.3113939703890095\n",
      "epoch: 714\n",
      "train oa: 88.35 train loss 0.24222008464216402\n",
      " val oa: 0.877 val_loss: 0.3071203676040369\n",
      "epoch: 715\n",
      "train oa: 87.95 train loss 0.2538844629407296\n",
      " val oa: 0.876 val_loss: 0.31277470300153437\n",
      "epoch: 716\n",
      "train oa: 89.7 train loss 0.24777690107717007\n",
      " val oa: 0.875 val_loss: 0.33295340957437924\n",
      "epoch: 717\n",
      "train oa: 89.25 train loss 0.24549495329587895\n",
      " val oa: 0.8686 val_loss: 0.3354785532465878\n",
      "epoch: 718\n",
      "global steps 23000 running loss: 0.028\n",
      "train oa: 89.2 train loss 0.2419457641642678\n",
      " val oa: 0.8804 val_loss: 0.30815700475326685\n",
      "epoch: 719\n",
      "train oa: 89.0 train loss 0.23706387536616833\n",
      " val oa: 0.8768 val_loss: 0.3160786505879363\n",
      "epoch: 720\n",
      "train oa: 88.65 train loss 0.25138035489548316\n",
      " val oa: 0.884 val_loss: 0.296182307685362\n",
      "epoch: 721\n",
      "train oa: 88.9 train loss 0.243957352599374\n",
      " val oa: 0.8668 val_loss: 0.32870531626081734\n",
      "epoch: 722\n",
      "train oa: 88.85 train loss 0.23653203287311406\n",
      " val oa: 0.878 val_loss: 0.31818882813510435\n",
      "epoch: 723\n",
      "train oa: 88.4 train loss 0.2502587612004996\n",
      " val oa: 0.8842 val_loss: 0.316096429577889\n",
      "epoch: 724\n",
      "global steps 23200 running loss: 0.043\n",
      "train oa: 88.4 train loss 0.26569789563446894\n",
      " val oa: 0.8558 val_loss: 0.3720958581444562\n",
      "epoch: 725\n",
      "train oa: 88.3 train loss 0.2519814999342137\n",
      " val oa: 0.8798 val_loss: 0.29951505345104984\n",
      "epoch: 726\n",
      "train oa: 89.1 train loss 0.237661599576086\n",
      " val oa: 0.8718 val_loss: 0.343533983334924\n",
      "epoch: 727\n",
      "train oa: 88.2 train loss 0.26204203947337124\n",
      " val oa: 0.8768 val_loss: 0.3163640561287673\n",
      "epoch: 728\n",
      "train oa: 89.05 train loss 0.23785826615360361\n",
      " val oa: 0.873 val_loss: 0.3205936899965717\n",
      "epoch: 729\n",
      "train oa: 88.6 train loss 0.2559090722128403\n",
      " val oa: 0.8862 val_loss: 0.309006235901375\n",
      "epoch: 730\n",
      "train oa: 86.65 train loss 0.278866984211803\n",
      " val oa: 0.8784 val_loss: 0.30406314603545986\n",
      "epoch: 731\n",
      "global steps 23400 running loss: 0.011\n",
      "train oa: 88.35 train loss 0.25375791094786093\n",
      " val oa: 0.8732 val_loss: 0.3131159396774359\n",
      "epoch: 732\n",
      "train oa: 89.05 train loss 0.23814275536063792\n",
      " val oa: 0.8758 val_loss: 0.32910387038498007\n",
      "epoch: 733\n",
      "train oa: 89.95 train loss 0.22616685256088057\n",
      " val oa: 0.8826 val_loss: 0.30652825613775536\n",
      "epoch: 734\n",
      "train oa: 89.85 train loss 0.23149702605885997\n",
      " val oa: 0.8864 val_loss: 0.30734875647345744\n",
      "epoch: 735\n",
      "train oa: 89.05 train loss 0.24409203773421503\n",
      " val oa: 0.8794 val_loss: 0.30808450336588183\n",
      "epoch: 736\n",
      "train oa: 88.15 train loss 0.24956386537184244\n",
      " val oa: 0.8802 val_loss: 0.30630186377835517\n",
      "epoch: 737\n",
      "global steps 23600 running loss: 0.018\n",
      "train oa: 89.55 train loss 0.22909818224185757\n",
      " val oa: 0.8698 val_loss: 0.32498310436864486\n",
      "epoch: 738\n",
      "train oa: 88.25 train loss 0.2512349221045354\n",
      " val oa: 0.876 val_loss: 0.33408572494307387\n",
      "epoch: 739\n",
      "train oa: 89.3 train loss 0.23951869873960385\n",
      " val oa: 0.8702 val_loss: 0.33099130935632887\n",
      "epoch: 740\n",
      "train oa: 87.65 train loss 0.25228343492412003\n",
      " val oa: 0.8784 val_loss: 0.30454337999030934\n",
      "epoch: 741\n",
      "train oa: 88.9 train loss 0.24554085297514233\n",
      " val oa: 0.8806 val_loss: 0.30350821379637233\n",
      "epoch: 742\n",
      "train oa: 89.6 train loss 0.2355831103635661\n",
      " val oa: 0.883 val_loss: 0.3087177624085687\n",
      "epoch: 743\n",
      "global steps 23800 running loss: 0.029\n",
      "train oa: 88.05 train loss 0.24611023190411715\n",
      " val oa: 0.885 val_loss: 0.30652222066622214\n",
      "epoch: 744\n",
      "train oa: 88.8 train loss 0.2490506851810911\n",
      " val oa: 0.8884 val_loss: 0.29044037878069684\n",
      "epoch: 745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 89.25 train loss 0.2324252268878747\n",
      " val oa: 0.8758 val_loss: 0.33202488472465425\n",
      "epoch: 746\n",
      "train oa: 88.25 train loss 0.2784703011648244\n",
      " val oa: 0.8676 val_loss: 0.31886012630788074\n",
      "epoch: 747\n",
      "train oa: 88.75 train loss 0.26002363431260395\n",
      " val oa: 0.8832 val_loss: 0.3036772638348996\n",
      "epoch: 748\n",
      "train oa: 88.55 train loss 0.24771350139864814\n",
      " val oa: 0.8904 val_loss: 0.28860984323077127\n",
      "epoch: 749\n",
      "global steps 24000 running loss: 0.037\n",
      "train oa: 89.5 train loss 0.22818380622903203\n",
      " val oa: 0.8836 val_loss: 0.30883233132972265\n",
      "epoch: 750\n",
      "train oa: 89.55 train loss 0.22674784291603572\n",
      " val oa: 0.8766 val_loss: 0.3216704443865001\n",
      "epoch: 751\n",
      "train oa: 89.85 train loss 0.23706086254828523\n",
      " val oa: 0.8804 val_loss: 0.31873640901193445\n",
      "epoch: 752\n",
      "train oa: 88.85 train loss 0.23581156274398657\n",
      " val oa: 0.8882 val_loss: 0.3001836575155716\n",
      "epoch: 753\n",
      "train oa: 90.0 train loss 0.23299907767841502\n",
      " val oa: 0.8892 val_loss: 0.29061285314818913\n",
      "epoch: 754\n",
      "train oa: 89.0 train loss 0.23232668444706467\n",
      " val oa: 0.8916 val_loss: 0.2853240600749119\n",
      "epoch: 755\n",
      "train oa: 88.1 train loss 0.24695214022737885\n",
      " val oa: 0.8982 val_loss: 0.2959306156961096\n",
      "epoch: 755 best val accuracy: 0.8982\n",
      "epoch: 756\n",
      "global steps 24200 running loss: 0.009\n",
      "train oa: 89.25 train loss 0.23519949818495064\n",
      " val oa: 0.8736 val_loss: 0.3104075606202186\n",
      "epoch: 757\n",
      "train oa: 88.75 train loss 0.23748979964310923\n",
      " val oa: 0.8932 val_loss: 0.2797682591884546\n",
      "epoch: 758\n",
      "train oa: 88.75 train loss 0.23750613434726534\n",
      " val oa: 0.872 val_loss: 0.3289927783666093\n",
      "epoch: 759\n",
      "train oa: 89.25 train loss 0.23460002737886326\n",
      " val oa: 0.89 val_loss: 0.3014049754189394\n",
      "epoch: 760\n",
      "train oa: 89.1 train loss 0.24102015271376542\n",
      " val oa: 0.8824 val_loss: 0.29842397263013437\n",
      "epoch: 761\n",
      "train oa: 89.45 train loss 0.2294277397782214\n",
      " val oa: 0.8836 val_loss: 0.30759688115100176\n",
      "epoch: 762\n",
      "global steps 24400 running loss: 0.019\n",
      "train oa: 89.3 train loss 0.2348288434918941\n",
      " val oa: 0.888 val_loss: 0.3064335997161851\n",
      "epoch: 763\n",
      "train oa: 88.75 train loss 0.23409012367458606\n",
      " val oa: 0.882 val_loss: 0.30925659715986853\n",
      "epoch: 764\n",
      "train oa: 89.2 train loss 0.2437696874625246\n",
      " val oa: 0.883 val_loss: 0.30845384223647604\n",
      "epoch: 765\n",
      "train oa: 88.85 train loss 0.24455311406469799\n",
      " val oa: 0.8816 val_loss: 0.30530570096809234\n",
      "epoch: 766\n",
      "train oa: 88.9 train loss 0.24218544673790648\n",
      " val oa: 0.8754 val_loss: 0.33724551264791086\n",
      "epoch: 767\n",
      "train oa: 89.6 train loss 0.2297169910774267\n",
      " val oa: 0.8844 val_loss: 0.3264103731112889\n",
      "epoch: 768\n",
      "global steps 24600 running loss: 0.030\n",
      "train oa: 88.35 train loss 0.24741308617026334\n",
      " val oa: 0.8614 val_loss: 0.34851832299140817\n",
      "epoch: 769\n",
      "train oa: 89.5 train loss 0.23023546042959506\n",
      " val oa: 0.8734 val_loss: 0.32176448794604556\n",
      "epoch: 770\n",
      "train oa: 88.75 train loss 0.2388883827905741\n",
      " val oa: 0.876 val_loss: 0.3253786351506325\n",
      "epoch: 771\n",
      "train oa: 89.55 train loss 0.23373602820433204\n",
      " val oa: 0.8752 val_loss: 0.32362363367326197\n",
      "epoch: 772\n",
      "train oa: 88.95 train loss 0.24367586819043546\n",
      " val oa: 0.8852 val_loss: 0.29569253787154776\n",
      "epoch: 773\n",
      "train oa: 89.55 train loss 0.2283403504626111\n",
      " val oa: 0.8882 val_loss: 0.3080039421487754\n",
      "epoch: 774\n",
      "global steps 24800 running loss: 0.037\n",
      "train oa: 89.6 train loss 0.23081908031451007\n",
      " val oa: 0.8866 val_loss: 0.29400190323829195\n",
      "epoch: 775\n",
      "train oa: 88.35 train loss 0.23676639966255955\n",
      " val oa: 0.8746 val_loss: 0.3377154375165881\n",
      "epoch: 776\n",
      "train oa: 89.2 train loss 0.24245588406656746\n",
      " val oa: 0.8748 val_loss: 0.3383252557690529\n",
      "epoch: 777\n",
      "train oa: 90.15 train loss 0.21771802904023552\n",
      " val oa: 0.8868 val_loss: 0.31357547459311197\n",
      "epoch: 778\n",
      "train oa: 89.4 train loss 0.2374248875628267\n",
      " val oa: 0.8854 val_loss: 0.3107773374017752\n",
      "epoch: 779\n",
      "train oa: 89.2 train loss 0.24110411214427477\n",
      " val oa: 0.896 val_loss: 0.2814642576480963\n",
      "epoch: 780\n",
      "train oa: 88.2 train loss 0.2558248644694224\n",
      " val oa: 0.891 val_loss: 0.296861904963551\n",
      "epoch: 781\n",
      "global steps 25000 running loss: 0.008\n",
      "train oa: 89.9 train loss 0.23045157656622642\n",
      " val oa: 0.8834 val_loss: 0.3183131769068936\n",
      "epoch: 782\n",
      "train oa: 89.85 train loss 0.22269414300637114\n",
      " val oa: 0.8802 val_loss: 0.3165041773676776\n",
      "epoch: 783\n",
      "train oa: 88.65 train loss 0.2567728213338528\n",
      " val oa: 0.8764 val_loss: 0.30107729551090123\n",
      "epoch: 784\n",
      "train oa: 90.35 train loss 0.2166098781574315\n",
      " val oa: 0.8906 val_loss: 0.2964841465211237\n",
      "epoch: 785\n",
      "train oa: 88.7 train loss 0.240195277455726\n",
      " val oa: 0.8824 val_loss: 0.30688905636969266\n",
      "epoch: 786\n",
      "train oa: 89.4 train loss 0.2296300968675428\n",
      " val oa: 0.8868 val_loss: 0.30854258609826835\n",
      "epoch: 787\n",
      "global steps 25200 running loss: 0.019\n",
      "train oa: 89.0 train loss 0.23029555876082577\n",
      " val oa: 0.8788 val_loss: 0.3193042425424155\n",
      "epoch: 788\n",
      "train oa: 89.6 train loss 0.23700925233359157\n",
      " val oa: 0.897 val_loss: 0.28948326865363505\n",
      "epoch: 789\n",
      "train oa: 89.4 train loss 0.24957951661607777\n",
      " val oa: 0.8782 val_loss: 0.31312207027476685\n",
      "epoch: 790\n",
      "train oa: 89.15 train loss 0.24107448930467656\n",
      " val oa: 0.8788 val_loss: 0.30475876643515665\n",
      "epoch: 791\n",
      "train oa: 88.5 train loss 0.2465444316074331\n",
      " val oa: 0.8832 val_loss: 0.30762147127215633\n",
      "epoch: 792\n",
      "train oa: 90.1 train loss 0.21952906346567616\n",
      " val oa: 0.8832 val_loss: 0.32804451709603394\n",
      "epoch: 793\n",
      "global steps 25400 running loss: 0.026\n",
      "train oa: 89.55 train loss 0.21975414679163913\n",
      " val oa: 0.8772 val_loss: 0.3320581711375381\n",
      "epoch: 794\n",
      "train oa: 89.4 train loss 0.22312828047150512\n",
      " val oa: 0.8716 val_loss: 0.3380906208724251\n",
      "epoch: 795\n",
      "train oa: 89.15 train loss 0.2386629154122049\n",
      " val oa: 0.8934 val_loss: 0.30102373684148276\n",
      "epoch: 796\n",
      "train oa: 89.5 train loss 0.23304555722544468\n",
      " val oa: 0.8942 val_loss: 0.29549190342025244\n",
      "epoch: 797\n",
      "train oa: 89.95 train loss 0.22640674228471877\n",
      " val oa: 0.8694 val_loss: 0.34465770105396354\n",
      "epoch: 798\n",
      "train oa: 90.1 train loss 0.22872181573469752\n",
      " val oa: 0.8756 val_loss: 0.32188798437917704\n",
      "epoch: 799\n",
      "global steps 25600 running loss: 0.037\n",
      "train oa: 89.1 train loss 0.2288865162740371\n",
      " val oa: 0.8872 val_loss: 0.31730444116947487\n",
      "epoch: 800\n",
      "train oa: 89.6 train loss 0.21840415376426336\n",
      " val oa: 0.873 val_loss: 0.33318446945655367\n",
      "epoch: 801\n",
      "train oa: 88.8 train loss 0.23297283587940815\n",
      " val oa: 0.877 val_loss: 0.3289568167754551\n",
      "epoch: 802\n",
      "train oa: 89.35 train loss 0.2350002287039413\n",
      " val oa: 0.8712 val_loss: 0.3433646725439452\n",
      "epoch: 803\n",
      "train oa: 87.45 train loss 0.2635910603707614\n",
      " val oa: 0.8686 val_loss: 0.35252118772210095\n",
      "epoch: 804\n",
      "train oa: 88.8 train loss 0.24725522132164243\n",
      " val oa: 0.8858 val_loss: 0.2983867948622919\n",
      "epoch: 805\n",
      "train oa: 89.25 train loss 0.23788429413851025\n",
      " val oa: 0.8892 val_loss: 0.293437882509398\n",
      "epoch: 806\n",
      "global steps 25800 running loss: 0.009\n",
      "train oa: 89.1 train loss 0.24040010494240216\n",
      " val oa: 0.8922 val_loss: 0.27854134915713286\n",
      "epoch: 807\n",
      "train oa: 89.35 train loss 0.23437811053114774\n",
      " val oa: 0.8762 val_loss: 0.30437020882439236\n",
      "epoch: 808\n",
      "train oa: 90.0 train loss 0.2232189963457427\n",
      " val oa: 0.8904 val_loss: 0.3093903602269216\n",
      "epoch: 809\n",
      "train oa: 89.1 train loss 0.23449104243312996\n",
      " val oa: 0.8682 val_loss: 0.34602405125178975\n",
      "epoch: 810\n",
      "train oa: 90.6 train loss 0.2193202234473552\n",
      " val oa: 0.8936 val_loss: 0.2931468639525551\n",
      "epoch: 811\n",
      "train oa: 88.5 train loss 0.24273664039562629\n",
      " val oa: 0.886 val_loss: 0.3066245362696604\n",
      "epoch: 812\n",
      "global steps 26000 running loss: 0.017\n",
      "train oa: 90.25 train loss 0.2196525392763179\n",
      " val oa: 0.8776 val_loss: 0.3161504605421649\n",
      "epoch: 813\n",
      "train oa: 89.5 train loss 0.23114844513938765\n",
      " val oa: 0.885 val_loss: 0.31040339270760003\n",
      "epoch: 814\n",
      "train oa: 90.2 train loss 0.21506082239831537\n",
      " val oa: 0.891 val_loss: 0.3005025129358346\n",
      "epoch: 815\n",
      "train oa: 90.6 train loss 0.21397137475050965\n",
      " val oa: 0.8842 val_loss: 0.30973023025595725\n",
      "epoch: 816\n",
      "train oa: 89.75 train loss 0.22403265201880523\n",
      " val oa: 0.8864 val_loss: 0.3163260738931257\n",
      "epoch: 817\n",
      "train oa: 89.65 train loss 0.22628124570076072\n",
      " val oa: 0.8888 val_loss: 0.3163325104125988\n",
      "epoch: 818\n",
      "global steps 26200 running loss: 0.025\n",
      "train oa: 90.05 train loss 0.21018756262454005\n",
      " val oa: 0.8826 val_loss: 0.3222079922220695\n",
      "epoch: 819\n",
      "train oa: 89.95 train loss 0.216708623656003\n",
      " val oa: 0.887 val_loss: 0.32358205330893175\n",
      "epoch: 820\n",
      "train oa: 89.9 train loss 0.23056480287760237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.8894 val_loss: 0.3093848152236796\n",
      "epoch: 821\n",
      "train oa: 89.1 train loss 0.23687027916898143\n",
      " val oa: 0.891 val_loss: 0.3005886865372827\n",
      "epoch: 822\n",
      "train oa: 89.75 train loss 0.2218733747520837\n",
      " val oa: 0.8878 val_loss: 0.2993914524143261\n",
      "epoch: 823\n",
      "train oa: 90.05 train loss 0.21893324302426834\n",
      " val oa: 0.883 val_loss: 0.3225706269676224\n",
      "epoch: 824\n",
      "global steps 26400 running loss: 0.039\n",
      "train oa: 88.6 train loss 0.2460538665987521\n",
      " val oa: 0.8842 val_loss: 0.32532454972614827\n",
      "epoch: 825\n",
      "train oa: 90.1 train loss 0.2261316727687292\n",
      " val oa: 0.8888 val_loss: 0.29270528497589315\n",
      "epoch: 826\n",
      "train oa: 90.8 train loss 0.21535323255691435\n",
      " val oa: 0.8734 val_loss: 0.3448578867294825\n",
      "epoch: 827\n",
      "train oa: 89.0 train loss 0.2437206114982296\n",
      " val oa: 0.8622 val_loss: 0.33585668814396535\n",
      "epoch: 828\n",
      "train oa: 89.45 train loss 0.23854442918481436\n",
      " val oa: 0.8706 val_loss: 0.3550105884590045\n",
      "epoch: 829\n",
      "train oa: 89.2 train loss 0.2333622993150933\n",
      " val oa: 0.886 val_loss: 0.30676454228828387\n",
      "epoch: 830\n",
      "train oa: 89.55 train loss 0.2256783417617507\n",
      " val oa: 0.8908 val_loss: 0.3070137579219199\n",
      "epoch: 831\n",
      "global steps 26600 running loss: 0.009\n",
      "train oa: 90.9 train loss 0.20516715957837872\n",
      " val oa: 0.883 val_loss: 0.3205495292892098\n",
      "epoch: 832\n",
      "train oa: 89.7 train loss 0.22665498608648119\n",
      " val oa: 0.8712 val_loss: 0.33610247583155073\n",
      "epoch: 833\n",
      "train oa: 89.45 train loss 0.22834639376740026\n",
      " val oa: 0.8796 val_loss: 0.31482122668727625\n",
      "epoch: 834\n",
      "train oa: 89.35 train loss 0.22728934678461624\n",
      " val oa: 0.8886 val_loss: 0.30220600048966306\n",
      "epoch: 835\n",
      "train oa: 90.4 train loss 0.21712438006312657\n",
      " val oa: 0.8758 val_loss: 0.3352184406547474\n",
      "epoch: 836\n",
      "train oa: 90.95 train loss 0.21469662258448222\n",
      " val oa: 0.8872 val_loss: 0.307086485115724\n",
      "epoch: 837\n",
      "global steps 26800 running loss: 0.019\n",
      "train oa: 89.45 train loss 0.22875118669249545\n",
      " val oa: 0.8798 val_loss: 0.30197283637293104\n",
      "epoch: 838\n",
      "train oa: 89.0 train loss 0.24473924273895023\n",
      " val oa: 0.8818 val_loss: 0.3104356324655466\n",
      "epoch: 839\n",
      "train oa: 90.45 train loss 0.21202976059893555\n",
      " val oa: 0.8832 val_loss: 0.30486612436548194\n",
      "epoch: 840\n",
      "train oa: 90.75 train loss 0.22026306124590528\n",
      " val oa: 0.8892 val_loss: 0.3030176455380623\n",
      "epoch: 841\n",
      "train oa: 89.15 train loss 0.23990525376734625\n",
      " val oa: 0.8912 val_loss: 0.28244568283466415\n",
      "epoch: 842\n",
      "train oa: 90.75 train loss 0.2121400505038813\n",
      " val oa: 0.8932 val_loss: 0.28699923478796957\n",
      "epoch: 843\n",
      "global steps 27000 running loss: 0.027\n",
      "train oa: 90.05 train loss 0.2238013752435483\n",
      " val oa: 0.8912 val_loss: 0.30458594350933504\n",
      "epoch: 844\n",
      "train oa: 89.75 train loss 0.21893979075798636\n",
      " val oa: 0.8882 val_loss: 0.31182073372188157\n",
      "epoch: 845\n",
      "train oa: 89.8 train loss 0.2112498190579725\n",
      " val oa: 0.878 val_loss: 0.3300354053364467\n",
      "epoch: 846\n",
      "train oa: 90.2 train loss 0.22969648892185782\n",
      " val oa: 0.888 val_loss: 0.3091022115909199\n",
      "epoch: 847\n",
      "train oa: 89.45 train loss 0.23419018826621985\n",
      " val oa: 0.8874 val_loss: 0.34450495763000455\n",
      "epoch: 848\n",
      "train oa: 89.05 train loss 0.24640479857394773\n",
      " val oa: 0.8972 val_loss: 0.28806481619277857\n",
      "epoch: 849\n",
      "global steps 27200 running loss: 0.034\n",
      "train oa: 90.65 train loss 0.21083044791240244\n",
      " val oa: 0.8926 val_loss: 0.28942952546208944\n",
      "epoch: 850\n",
      "train oa: 90.5 train loss 0.2090752059985823\n",
      " val oa: 0.8874 val_loss: 0.3236478698656162\n",
      "epoch: 851\n",
      "train oa: 90.65 train loss 0.20502037116376448\n",
      " val oa: 0.8878 val_loss: 0.3214657126079483\n",
      "epoch: 852\n",
      "train oa: 89.75 train loss 0.22870051320241558\n",
      " val oa: 0.8858 val_loss: 0.33232180197304667\n",
      "epoch: 853\n",
      "train oa: 90.85 train loss 0.2288559046236987\n",
      " val oa: 0.881 val_loss: 0.33267428078050215\n",
      "epoch: 854\n",
      "train oa: 88.25 train loss 0.24761203903657641\n",
      " val oa: 0.8664 val_loss: 0.35781751487658287\n",
      "epoch: 855\n",
      "train oa: 89.35 train loss 0.22866884547135444\n",
      " val oa: 0.8924 val_loss: 0.3036733480252961\n",
      "epoch: 856\n",
      "global steps 27400 running loss: 0.009\n",
      "train oa: 89.9 train loss 0.21731045837217486\n",
      " val oa: 0.8878 val_loss: 0.3031123690995773\n",
      "epoch: 857\n",
      "train oa: 90.0 train loss 0.22341721325825628\n",
      " val oa: 0.8908 val_loss: 0.31191765713572767\n",
      "epoch: 858\n",
      "train oa: 90.6 train loss 0.20579808490733295\n",
      " val oa: 0.8888 val_loss: 0.3151389643221508\n",
      "epoch: 859\n",
      "train oa: 89.95 train loss 0.2274299359913429\n",
      " val oa: 0.8876 val_loss: 0.30818040421418696\n",
      "epoch: 860\n",
      "train oa: 89.3 train loss 0.23397563988538475\n",
      " val oa: 0.8868 val_loss: 0.3048694504386339\n",
      "epoch: 861\n",
      "train oa: 90.15 train loss 0.21958510385772748\n",
      " val oa: 0.8932 val_loss: 0.3042053171435598\n",
      "epoch: 862\n",
      "global steps 27600 running loss: 0.018\n",
      "train oa: 89.35 train loss 0.23146665415834214\n",
      " val oa: 0.886 val_loss: 0.3030942725009765\n",
      "epoch: 863\n",
      "train oa: 89.55 train loss 0.22129265531371967\n",
      " val oa: 0.8884 val_loss: 0.3076353009529981\n",
      "epoch: 864\n",
      "train oa: 88.45 train loss 0.2280189691277562\n",
      " val oa: 0.887 val_loss: 0.29907696073207235\n",
      "epoch: 865\n",
      "train oa: 90.3 train loss 0.21435757715115472\n",
      " val oa: 0.8868 val_loss: 0.30969984571606896\n",
      "epoch: 866\n",
      "train oa: 89.3 train loss 0.22926974941468353\n",
      " val oa: 0.8894 val_loss: 0.3111709191293035\n",
      "epoch: 867\n",
      "train oa: 89.4 train loss 0.23170261409113682\n",
      " val oa: 0.8958 val_loss: 0.29514014736934446\n",
      "epoch: 868\n",
      "global steps 27800 running loss: 0.024\n",
      "train oa: 90.7 train loss 0.20578729885718935\n",
      " val oa: 0.8922 val_loss: 0.2826682063885142\n",
      "epoch: 869\n",
      "train oa: 90.3 train loss 0.2238988060466842\n",
      " val oa: 0.8966 val_loss: 0.29836278433857744\n",
      "epoch: 870\n",
      "train oa: 89.05 train loss 0.24263639622199923\n",
      " val oa: 0.88 val_loss: 0.3094420509407428\n",
      "epoch: 871\n",
      "train oa: 90.5 train loss 0.2099774876372755\n",
      " val oa: 0.891 val_loss: 0.28877791303446504\n",
      "epoch: 872\n",
      "train oa: 90.5 train loss 0.210154279171162\n",
      " val oa: 0.8766 val_loss: 0.32039907176743476\n",
      "epoch: 873\n",
      "train oa: 89.9 train loss 0.22378826776120822\n",
      " val oa: 0.881 val_loss: 0.3146492535582149\n",
      "epoch: 874\n",
      "global steps 28000 running loss: 0.037\n",
      "train oa: 88.95 train loss 0.23424857266223134\n",
      " val oa: 0.8836 val_loss: 0.32063094689900484\n",
      "epoch: 875\n",
      "train oa: 90.8 train loss 0.20542717339585312\n",
      " val oa: 0.8906 val_loss: 0.3162052720803638\n",
      "epoch: 876\n",
      "train oa: 90.4 train loss 0.21560819510277743\n",
      " val oa: 0.8914 val_loss: 0.32464770255132813\n",
      "epoch: 877\n",
      "train oa: 89.65 train loss 0.2305071065068755\n",
      " val oa: 0.8812 val_loss: 0.3234340258822475\n",
      "epoch: 878\n",
      "train oa: 90.35 train loss 0.20956432562183155\n",
      " val oa: 0.8944 val_loss: 0.2898544253297499\n",
      "epoch: 879\n",
      "train oa: 90.05 train loss 0.22238871787139658\n",
      " val oa: 0.8848 val_loss: 0.302376982022372\n",
      "epoch: 880\n",
      "train oa: 91.25 train loss 0.19441951767053048\n",
      " val oa: 0.878 val_loss: 0.30983335990058014\n",
      "epoch: 881\n",
      "global steps 28200 running loss: 0.009\n",
      "train oa: 89.1 train loss 0.22859371795148867\n",
      " val oa: 0.8878 val_loss: 0.3332158312577312\n",
      "epoch: 882\n",
      "train oa: 89.45 train loss 0.2242027550920919\n",
      " val oa: 0.8884 val_loss: 0.29691900146955913\n",
      "epoch: 883\n",
      "train oa: 88.95 train loss 0.24111047752046388\n",
      " val oa: 0.89 val_loss: 0.2919200408936283\n",
      "epoch: 884\n",
      "train oa: 89.7 train loss 0.2220486519437433\n",
      " val oa: 0.8856 val_loss: 0.3040764926795907\n",
      "epoch: 885\n",
      "train oa: 90.1 train loss 0.217380693002636\n",
      " val oa: 0.8966 val_loss: 0.28986129077905953\n",
      "epoch: 886\n",
      "train oa: 89.75 train loss 0.22101444425693667\n",
      " val oa: 0.8824 val_loss: 0.3040214734802389\n",
      "epoch: 887\n",
      "global steps 28400 running loss: 0.020\n",
      "train oa: 89.0 train loss 0.23039435169010436\n",
      " val oa: 0.8906 val_loss: 0.28650033817794296\n",
      "epoch: 888\n",
      "train oa: 90.55 train loss 0.21347792852005862\n",
      " val oa: 0.886 val_loss: 0.2964570870422472\n",
      "epoch: 889\n",
      "train oa: 90.9 train loss 0.20552268889708705\n",
      " val oa: 0.8916 val_loss: 0.3044002523197294\n",
      "epoch: 890\n",
      "train oa: 90.7 train loss 0.21174010270395002\n",
      " val oa: 0.8896 val_loss: 0.30531613633780696\n",
      "epoch: 891\n",
      "train oa: 89.75 train loss 0.21838599919314328\n",
      " val oa: 0.8934 val_loss: 0.3058039423506811\n",
      "epoch: 892\n",
      "train oa: 89.95 train loss 0.21243361245970682\n",
      " val oa: 0.8908 val_loss: 0.3103239535958855\n",
      "epoch: 893\n",
      "global steps 28600 running loss: 0.025\n",
      "train oa: 90.55 train loss 0.20876443223123423\n",
      " val oa: 0.8972 val_loss: 0.30373788598705015\n",
      "epoch: 894\n",
      "train oa: 90.4 train loss 0.1991033287773268\n",
      " val oa: 0.8866 val_loss: 0.3360796109136276\n",
      "epoch: 895\n",
      "train oa: 89.4 train loss 0.219803236499705\n",
      " val oa: 0.8808 val_loss: 0.32727257400062765\n",
      "epoch: 896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 89.35 train loss 0.24210036049625386\n",
      " val oa: 0.8892 val_loss: 0.30559956557785156\n",
      "epoch: 897\n",
      "train oa: 89.55 train loss 0.22928248142417454\n",
      " val oa: 0.8938 val_loss: 0.2868347640532927\n",
      "epoch: 898\n",
      "train oa: 89.65 train loss 0.21848750998981764\n",
      " val oa: 0.8876 val_loss: 0.30823655819453694\n",
      "epoch: 899\n",
      "global steps 28800 running loss: 0.035\n",
      "train oa: 90.65 train loss 0.22093573699819957\n",
      " val oa: 0.8884 val_loss: 0.30716370280363076\n",
      "epoch: 900\n",
      "train oa: 90.8 train loss 0.21094239363790634\n",
      " val oa: 0.8902 val_loss: 0.3021898605599414\n",
      "epoch: 901\n",
      "train oa: 90.35 train loss 0.21918103207271192\n",
      " val oa: 0.885 val_loss: 0.33731455423071294\n",
      "epoch: 902\n",
      "train oa: 90.3 train loss 0.20813718733947437\n",
      " val oa: 0.877 val_loss: 0.3529351239845604\n",
      "epoch: 903\n",
      "train oa: 91.4 train loss 0.21169822728800075\n",
      " val oa: 0.8888 val_loss: 0.3076892684880435\n",
      "epoch: 904\n",
      "train oa: 88.3 train loss 0.23503660861616377\n",
      " val oa: 0.8754 val_loss: 0.3403993635871276\n",
      "epoch: 905\n",
      "train oa: 89.25 train loss 0.2384163097488246\n",
      " val oa: 0.8906 val_loss: 0.3123257180837893\n",
      "epoch: 906\n",
      "global steps 29000 running loss: 0.009\n",
      "train oa: 90.05 train loss 0.22382401593207138\n",
      " val oa: 0.8918 val_loss: 0.29945576214729047\n",
      "epoch: 907\n",
      "train oa: 90.3 train loss 0.21507568875525385\n",
      " val oa: 0.8912 val_loss: 0.30192260384921416\n",
      "epoch: 908\n",
      "train oa: 90.35 train loss 0.20524318335066555\n",
      " val oa: 0.8908 val_loss: 0.30736627600315175\n",
      "epoch: 909\n",
      "train oa: 90.75 train loss 0.2044582607260328\n",
      " val oa: 0.8858 val_loss: 0.32080444110279066\n",
      "epoch: 910\n",
      "train oa: 90.95 train loss 0.22571208006750954\n",
      " val oa: 0.8806 val_loss: 0.32396931983670607\n",
      "epoch: 911\n",
      "train oa: 90.2 train loss 0.21859124219966197\n",
      " val oa: 0.8906 val_loss: 0.31336058530258853\n",
      "epoch: 912\n",
      "global steps 29200 running loss: 0.019\n",
      "train oa: 89.3 train loss 0.23585312493430466\n",
      " val oa: 0.8796 val_loss: 0.32769212915934604\n",
      "epoch: 913\n",
      "train oa: 88.85 train loss 0.21852120919595525\n",
      " val oa: 0.8938 val_loss: 0.3022490686029644\n",
      "epoch: 914\n",
      "train oa: 89.9 train loss 0.21586404250084637\n",
      " val oa: 0.8894 val_loss: 0.30162605758884414\n",
      "epoch: 915\n",
      "train oa: 89.95 train loss 0.21842128819255316\n",
      " val oa: 0.8888 val_loss: 0.29580261157544063\n",
      "epoch: 916\n",
      "train oa: 91.2 train loss 0.2022948616844865\n",
      " val oa: 0.887 val_loss: 0.3246035758541804\n",
      "epoch: 917\n",
      "train oa: 90.05 train loss 0.21891527441659114\n",
      " val oa: 0.8794 val_loss: 0.32181071941624295\n",
      "epoch: 918\n",
      "global steps 29400 running loss: 0.026\n",
      "train oa: 90.3 train loss 0.2152038504714612\n",
      " val oa: 0.883 val_loss: 0.3228826387399817\n",
      "epoch: 919\n",
      "train oa: 90.2 train loss 0.19684998567050488\n",
      " val oa: 0.8754 val_loss: 0.32738210064177437\n",
      "epoch: 920\n",
      "train oa: 89.8 train loss 0.21527682628848266\n",
      " val oa: 0.8838 val_loss: 0.3289529759200211\n",
      "epoch: 921\n",
      "train oa: 90.25 train loss 0.21648611341451574\n",
      " val oa: 0.8962 val_loss: 0.3006194429321009\n",
      "epoch: 922\n",
      "train oa: 90.7 train loss 0.22016477306721413\n",
      " val oa: 0.8946 val_loss: 0.3026309472433397\n",
      "epoch: 923\n",
      "train oa: 89.75 train loss 0.21785111483064493\n",
      " val oa: 0.8826 val_loss: 0.34364632914114845\n",
      "epoch: 924\n",
      "global steps 29600 running loss: 0.034\n",
      "train oa: 89.95 train loss 0.21232805645995906\n",
      " val oa: 0.8984 val_loss: 0.3002836058872334\n",
      "epoch: 924 best val accuracy: 0.8984\n",
      "epoch: 925\n",
      "train oa: 91.3 train loss 0.1945150265772998\n",
      " val oa: 0.8894 val_loss: 0.3081049865847662\n",
      "epoch: 926\n",
      "train oa: 91.05 train loss 0.19783383319230247\n",
      " val oa: 0.8996 val_loss: 0.2919480718427434\n",
      "epoch: 926 best val accuracy: 0.8996\n",
      "epoch: 927\n",
      "train oa: 90.5 train loss 0.21634926220745324\n",
      " val oa: 0.8834 val_loss: 0.32410816082845517\n",
      "epoch: 928\n",
      "train oa: 90.4 train loss 0.2224948077376772\n",
      " val oa: 0.892 val_loss: 0.3099786128760988\n",
      "epoch: 929\n",
      "train oa: 90.45 train loss 0.21375736705228546\n",
      " val oa: 0.89 val_loss: 0.3008977720334525\n",
      "epoch: 930\n",
      "train oa: 91.8 train loss 0.19648982170118065\n",
      " val oa: 0.8832 val_loss: 0.31880381148033987\n",
      "epoch: 931\n",
      "global steps 29800 running loss: 0.008\n",
      "train oa: 90.55 train loss 0.21354898542386533\n",
      " val oa: 0.885 val_loss: 0.33198101050032136\n",
      "epoch: 932\n",
      "train oa: 90.35 train loss 0.22005071330433656\n",
      " val oa: 0.9012 val_loss: 0.27239828379288344\n",
      "epoch: 932 best val accuracy: 0.9012\n",
      "epoch: 933\n",
      "train oa: 90.4 train loss 0.2192479715530224\n",
      " val oa: 0.8892 val_loss: 0.3026694534602577\n",
      "epoch: 934\n",
      "train oa: 91.05 train loss 0.20733345704763892\n",
      " val oa: 0.8818 val_loss: 0.3283778649725516\n",
      "epoch: 935\n",
      "train oa: 90.75 train loss 0.2086782553776207\n",
      " val oa: 0.8792 val_loss: 0.3275092704418185\n",
      "epoch: 936\n",
      "train oa: 89.2 train loss 0.21887978777803138\n",
      " val oa: 0.8878 val_loss: 0.30724621186978274\n",
      "epoch: 937\n",
      "global steps 30000 running loss: 0.016\n",
      "train oa: 90.9 train loss 0.2022560253300072\n",
      " val oa: 0.889 val_loss: 0.3001264633057282\n",
      "epoch: 938\n",
      "train oa: 90.2 train loss 0.2185451119656241\n",
      " val oa: 0.891 val_loss: 0.31027198537224737\n",
      "epoch: 939\n",
      "train oa: 90.85 train loss 0.2071546968528787\n",
      " val oa: 0.8936 val_loss: 0.3079426412784847\n",
      "epoch: 940\n",
      "train oa: 90.7 train loss 0.20688483553668743\n",
      " val oa: 0.8942 val_loss: 0.30855898905008966\n",
      "epoch: 941\n",
      "train oa: 91.2 train loss 0.20957430435318564\n",
      " val oa: 0.885 val_loss: 0.3197201743856874\n",
      "epoch: 942\n",
      "train oa: 90.55 train loss 0.20552169404174483\n",
      " val oa: 0.8816 val_loss: 0.3579025646833801\n",
      "epoch: 943\n",
      "global steps 30200 running loss: 0.024\n",
      "train oa: 89.9 train loss 0.20909032016354134\n",
      " val oa: 0.8948 val_loss: 0.30130805851814324\n",
      "epoch: 944\n",
      "train oa: 90.6 train loss 0.21026747263489676\n",
      " val oa: 0.8828 val_loss: 0.3242903006403087\n",
      "epoch: 945\n",
      "train oa: 90.25 train loss 0.20552768330979052\n",
      " val oa: 0.8922 val_loss: 0.3199038766854923\n",
      "epoch: 946\n",
      "train oa: 89.5 train loss 0.22939098225140966\n",
      " val oa: 0.892 val_loss: 0.29757957637338933\n",
      "epoch: 947\n",
      "train oa: 90.65 train loss 0.2080018007181023\n",
      " val oa: 0.8956 val_loss: 0.28841229507424687\n",
      "epoch: 948\n",
      "train oa: 90.6 train loss 0.210699754469507\n",
      " val oa: 0.8938 val_loss: 0.3079445169430803\n",
      "epoch: 949\n",
      "global steps 30400 running loss: 0.034\n",
      "train oa: 90.8 train loss 0.21086917686949697\n",
      " val oa: 0.8896 val_loss: 0.3285893197618888\n",
      "epoch: 950\n",
      "train oa: 90.95 train loss 0.19994068583594002\n",
      " val oa: 0.896 val_loss: 0.30850681770712884\n",
      "epoch: 951\n",
      "train oa: 90.15 train loss 0.21169889833422548\n",
      " val oa: 0.8952 val_loss: 0.30289024496779055\n",
      "epoch: 952\n",
      "train oa: 90.4 train loss 0.20139919590802802\n",
      " val oa: 0.8962 val_loss: 0.30726504347763495\n",
      "epoch: 953\n",
      "train oa: 90.95 train loss 0.19225011390910687\n",
      " val oa: 0.8926 val_loss: 0.3178695272166879\n",
      "epoch: 954\n",
      "train oa: 91.05 train loss 0.2001200232847094\n",
      " val oa: 0.8832 val_loss: 0.3330134893830017\n",
      "epoch: 955\n",
      "train oa: 89.7 train loss 0.22386869350749364\n",
      " val oa: 0.8754 val_loss: 0.3490737981871702\n",
      "epoch: 956\n",
      "global steps 30600 running loss: 0.007\n",
      "train oa: 90.75 train loss 0.20588561250370901\n",
      " val oa: 0.8992 val_loss: 0.2963761105163737\n",
      "epoch: 957\n",
      "train oa: 90.5 train loss 0.20183270991355023\n",
      " val oa: 0.8906 val_loss: 0.318558527933081\n",
      "epoch: 958\n",
      "train oa: 90.75 train loss 0.2046860237282067\n",
      " val oa: 0.8832 val_loss: 0.3210612739662493\n",
      "epoch: 959\n",
      "train oa: 92.0 train loss 0.1840075438905415\n",
      " val oa: 0.8862 val_loss: 0.348910240890209\n",
      "epoch: 960\n",
      "train oa: 91.25 train loss 0.20344103400388225\n",
      " val oa: 0.8898 val_loss: 0.318286505065584\n",
      "epoch: 961\n",
      "train oa: 89.7 train loss 0.23016847643976432\n",
      " val oa: 0.889 val_loss: 0.30159803630413945\n",
      "epoch: 962\n",
      "global steps 30800 running loss: 0.016\n",
      "train oa: 90.6 train loss 0.21241844456485598\n",
      " val oa: 0.8892 val_loss: 0.31401229871442\n",
      "epoch: 963\n",
      "train oa: 90.7 train loss 0.2101674066693097\n",
      " val oa: 0.8926 val_loss: 0.29647685926901096\n",
      "epoch: 964\n",
      "train oa: 90.5 train loss 0.19990602632946836\n",
      " val oa: 0.8882 val_loss: 0.3086052597312793\n",
      "epoch: 965\n",
      "train oa: 91.65 train loss 0.19488642638883844\n",
      " val oa: 0.8906 val_loss: 0.3093125460809182\n",
      "epoch: 966\n",
      "train oa: 90.0 train loss 0.20867278315841728\n",
      " val oa: 0.8756 val_loss: 0.33931808209078074\n",
      "epoch: 967\n",
      "train oa: 91.05 train loss 0.20132551876793015\n",
      " val oa: 0.8976 val_loss: 0.29073896553239004\n",
      "epoch: 968\n",
      "global steps 31000 running loss: 0.026\n",
      "train oa: 89.9 train loss 0.22064906554084857\n",
      " val oa: 0.8874 val_loss: 0.3183874701712461\n",
      "epoch: 969\n",
      "train oa: 90.6 train loss 0.19908549188492858\n",
      " val oa: 0.8766 val_loss: 0.350660091449446\n",
      "epoch: 970\n",
      "train oa: 90.05 train loss 0.223912004591434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.888 val_loss: 0.311679951530891\n",
      "epoch: 971\n",
      "train oa: 90.55 train loss 0.20351361175027988\n",
      " val oa: 0.8912 val_loss: 0.3083279144095989\n",
      "epoch: 972\n",
      "train oa: 90.7 train loss 0.20379842442452525\n",
      " val oa: 0.89 val_loss: 0.3215456015596081\n",
      "epoch: 973\n",
      "train oa: 91.3 train loss 0.19954661798220066\n",
      " val oa: 0.8956 val_loss: 0.2992073497102979\n",
      "epoch: 974\n",
      "global steps 31200 running loss: 0.032\n",
      "train oa: 91.85 train loss 0.20080944246828866\n",
      " val oa: 0.8842 val_loss: 0.33589257204645845\n",
      "epoch: 975\n",
      "train oa: 90.25 train loss 0.1970797755594722\n",
      " val oa: 0.8926 val_loss: 0.3178154014867907\n",
      "epoch: 976\n",
      "train oa: 91.2 train loss 0.1976261450242089\n",
      " val oa: 0.8868 val_loss: 0.3349746755437957\n",
      "epoch: 977\n",
      "train oa: 88.9 train loss 0.24299375767744633\n",
      " val oa: 0.8786 val_loss: 0.303815748435298\n",
      "epoch: 978\n",
      "train oa: 90.75 train loss 0.20879077132097024\n",
      " val oa: 0.8892 val_loss: 0.313757340748383\n",
      "epoch: 979\n",
      "train oa: 91.45 train loss 0.19330047844369203\n",
      " val oa: 0.887 val_loss: 0.33564827201844877\n",
      "epoch: 980\n",
      "train oa: 89.55 train loss 0.21118351874326632\n",
      " val oa: 0.8852 val_loss: 0.3268574013876774\n",
      "epoch: 981\n",
      "global steps 31400 running loss: 0.009\n",
      "train oa: 89.85 train loss 0.20621844224677477\n",
      " val oa: 0.8942 val_loss: 0.3052317115804543\n",
      "epoch: 982\n",
      "train oa: 90.05 train loss 0.21306473575108198\n",
      " val oa: 0.89 val_loss: 0.31320571740716374\n",
      "epoch: 983\n",
      "train oa: 91.35 train loss 0.20731414044306265\n",
      " val oa: 0.8888 val_loss: 0.3204106321345087\n",
      "epoch: 984\n",
      "train oa: 89.5 train loss 0.21874682771262824\n",
      " val oa: 0.8904 val_loss: 0.31569741909267146\n",
      "epoch: 985\n",
      "train oa: 90.85 train loss 0.20604699864175063\n",
      " val oa: 0.8924 val_loss: 0.3067937113542512\n",
      "epoch: 986\n",
      "train oa: 91.6 train loss 0.18726395756011505\n",
      " val oa: 0.8838 val_loss: 0.3319969892188897\n",
      "epoch: 987\n",
      "global steps 31600 running loss: 0.015\n",
      "train oa: 91.55 train loss 0.19347758731099485\n",
      " val oa: 0.8858 val_loss: 0.34342608019171916\n",
      "epoch: 988\n",
      "train oa: 90.25 train loss 0.20939166248289795\n",
      " val oa: 0.8832 val_loss: 0.3320754817328638\n",
      "epoch: 989\n",
      "train oa: 91.1 train loss 0.18705520917552948\n",
      " val oa: 0.8814 val_loss: 0.34278505096649364\n",
      "epoch: 990\n",
      "train oa: 90.9 train loss 0.189010322533927\n",
      " val oa: 0.886 val_loss: 0.325008785672141\n",
      "epoch: 991\n",
      "train oa: 90.25 train loss 0.2004396956346437\n",
      " val oa: 0.8734 val_loss: 0.35786028449137053\n",
      "epoch: 992\n",
      "train oa: 91.2 train loss 0.185543350016551\n",
      " val oa: 0.8884 val_loss: 0.3250761896493022\n",
      "epoch: 993\n",
      "global steps 31800 running loss: 0.023\n",
      "train oa: 91.7 train loss 0.19254929659999462\n",
      " val oa: 0.8908 val_loss: 0.33025832263630495\n",
      "epoch: 994\n",
      "train oa: 91.15 train loss 0.19349389545012946\n",
      " val oa: 0.8912 val_loss: 0.3157630623903369\n",
      "epoch: 995\n",
      "train oa: 90.7 train loss 0.21717548829592553\n",
      " val oa: 0.8884 val_loss: 0.30263509736264865\n",
      "epoch: 996\n",
      "train oa: 90.65 train loss 0.21116590282359357\n",
      " val oa: 0.891 val_loss: 0.3152148539461167\n",
      "epoch: 997\n",
      "train oa: 91.25 train loss 0.198499120925282\n",
      " val oa: 0.8894 val_loss: 0.3098797228124913\n",
      "epoch: 998\n",
      "train oa: 91.8 train loss 0.2050641573864715\n",
      " val oa: 0.889 val_loss: 0.31869232470615305\n",
      "epoch: 999\n",
      "global steps 32000 running loss: 0.030\n",
      "train oa: 91.9 train loss 0.18860189156159624\n",
      " val oa: 0.8938 val_loss: 0.31644978434516374\n",
      "epoch: 1000\n",
      "train oa: 90.1 train loss 0.21460439645495163\n",
      " val oa: 0.8864 val_loss: 0.3271788388439039\n",
      "epoch: 1001\n",
      "train oa: 90.8 train loss 0.2000780578493198\n",
      " val oa: 0.894 val_loss: 0.31824383746746054\n",
      "epoch: 1002\n",
      "train oa: 90.55 train loss 0.20134322270499921\n",
      " val oa: 0.887 val_loss: 0.3089406014671687\n",
      "epoch: 1003\n",
      "train oa: 90.7 train loss 0.20058685548329475\n",
      " val oa: 0.8878 val_loss: 0.3171633223149275\n",
      "epoch: 1004\n",
      "train oa: 90.2 train loss 0.20085849083793422\n",
      " val oa: 0.8882 val_loss: 0.31944429762015614\n",
      "epoch: 1005\n",
      "train oa: 90.1 train loss 0.20679422909340733\n",
      " val oa: 0.8894 val_loss: 0.31985947111135904\n",
      "epoch: 1006\n",
      "global steps 32200 running loss: 0.008\n",
      "train oa: 91.05 train loss 0.1987577558456615\n",
      " val oa: 0.8902 val_loss: 0.3082600672978022\n",
      "epoch: 1007\n",
      "train oa: 91.15 train loss 0.19574787298395868\n",
      " val oa: 0.8982 val_loss: 0.30138386856881294\n",
      "epoch: 1008\n",
      "train oa: 91.05 train loss 0.19544270879858835\n",
      " val oa: 0.8898 val_loss: 0.33883267602526745\n",
      "epoch: 1009\n",
      "train oa: 90.5 train loss 0.19817402087314476\n",
      " val oa: 0.8878 val_loss: 0.3093067765440218\n",
      "epoch: 1010\n",
      "train oa: 91.25 train loss 0.2029667797172476\n",
      " val oa: 0.893 val_loss: 0.302626704976676\n",
      "epoch: 1011\n",
      "train oa: 90.65 train loss 0.1970534382913081\n",
      " val oa: 0.8916 val_loss: 0.3199358015328523\n",
      "epoch: 1012\n",
      "global steps 32400 running loss: 0.015\n",
      "train oa: 92.7 train loss 0.1873202971010377\n",
      " val oa: 0.89 val_loss: 0.3173702202639264\n",
      "epoch: 1013\n",
      "train oa: 89.75 train loss 0.21714929564262156\n",
      " val oa: 0.8966 val_loss: 0.29960819724890314\n",
      "epoch: 1014\n",
      "train oa: 91.4 train loss 0.18893295782894445\n",
      " val oa: 0.892 val_loss: 0.3013068970478339\n",
      "epoch: 1015\n",
      "train oa: 90.9 train loss 0.2009194534258244\n",
      " val oa: 0.8922 val_loss: 0.2995871813151602\n",
      "epoch: 1016\n",
      "train oa: 91.65 train loss 0.20634868070562873\n",
      " val oa: 0.8904 val_loss: 0.3163525285489262\n",
      "epoch: 1017\n",
      "train oa: 89.55 train loss 0.22825353363816858\n",
      " val oa: 0.869 val_loss: 0.379024435851816\n",
      "epoch: 1018\n",
      "global steps 32600 running loss: 0.025\n",
      "train oa: 90.7 train loss 0.20590652063278073\n",
      " val oa: 0.896 val_loss: 0.31480497082764436\n",
      "epoch: 1019\n",
      "train oa: 90.4 train loss 0.21785629162452477\n",
      " val oa: 0.8788 val_loss: 0.3362331234386629\n",
      "epoch: 1020\n",
      "train oa: 91.0 train loss 0.20830379966224669\n",
      " val oa: 0.8826 val_loss: 0.3332868040130579\n",
      "epoch: 1021\n",
      "train oa: 90.9 train loss 0.19900742974488914\n",
      " val oa: 0.9006 val_loss: 0.2936570538784384\n",
      "epoch: 1022\n",
      "train oa: 91.45 train loss 0.19533517879802414\n",
      " val oa: 0.8958 val_loss: 0.29903861789503255\n",
      "epoch: 1023\n",
      "train oa: 91.9 train loss 0.1884510526831366\n",
      " val oa: 0.891 val_loss: 0.3059400079759794\n",
      "epoch: 1024\n",
      "global steps 32800 running loss: 0.035\n",
      "train oa: 90.4 train loss 0.21746467925802784\n",
      " val oa: 0.8884 val_loss: 0.32349709022791145\n",
      "epoch: 1025\n",
      "train oa: 91.45 train loss 0.19692186525814315\n",
      " val oa: 0.8932 val_loss: 0.3218915805586888\n",
      "epoch: 1026\n",
      "train oa: 90.6 train loss 0.19288140913817417\n",
      " val oa: 0.8926 val_loss: 0.30673284627486014\n",
      "epoch: 1027\n",
      "train oa: 90.5 train loss 0.20662336271749934\n",
      " val oa: 0.8936 val_loss: 0.3026327222100049\n",
      "epoch: 1028\n",
      "train oa: 91.85 train loss 0.18464719478247105\n",
      " val oa: 0.8836 val_loss: 0.32536763960390125\n",
      "epoch: 1029\n",
      "train oa: 91.7 train loss 0.19573525880869846\n",
      " val oa: 0.8868 val_loss: 0.3184200701003726\n",
      "epoch: 1030\n",
      "train oa: 90.5 train loss 0.21032513624538493\n",
      " val oa: 0.8858 val_loss: 0.340128607413925\n",
      "epoch: 1031\n",
      "global steps 33000 running loss: 0.007\n",
      "train oa: 90.9 train loss 0.19940517847725947\n",
      " val oa: 0.8892 val_loss: 0.32200766010471965\n",
      "epoch: 1032\n",
      "train oa: 90.85 train loss 0.20470880649000933\n",
      " val oa: 0.8854 val_loss: 0.3252034807137046\n",
      "epoch: 1033\n",
      "train oa: 90.5 train loss 0.20382384420466373\n",
      " val oa: 0.8952 val_loss: 0.3021232959063844\n",
      "epoch: 1034\n",
      "train oa: 91.35 train loss 0.18867318692829968\n",
      " val oa: 0.897 val_loss: 0.305741169195942\n",
      "epoch: 1035\n",
      "train oa: 91.25 train loss 0.19631323158601638\n",
      " val oa: 0.8968 val_loss: 0.3012314559884311\n",
      "epoch: 1036\n",
      "train oa: 89.95 train loss 0.21814347635862916\n",
      " val oa: 0.8912 val_loss: 0.3080419261597762\n",
      "epoch: 1037\n",
      "global steps 33200 running loss: 0.017\n",
      "train oa: 90.65 train loss 0.20540322845286904\n",
      " val oa: 0.8978 val_loss: 0.30537286678710346\n",
      "epoch: 1038\n",
      "train oa: 91.3 train loss 0.20067026617631015\n",
      " val oa: 0.894 val_loss: 0.3068661205934749\n",
      "epoch: 1039\n",
      "train oa: 91.35 train loss 0.18979382871379735\n",
      " val oa: 0.8894 val_loss: 0.3328630905981152\n",
      "epoch: 1040\n",
      "train oa: 91.55 train loss 0.18934355977757686\n",
      " val oa: 0.8824 val_loss: 0.3437669242532263\n",
      "epoch: 1041\n",
      "train oa: 90.25 train loss 0.21152046128787322\n",
      " val oa: 0.8824 val_loss: 0.3146601732462449\n",
      "epoch: 1042\n",
      "train oa: 91.45 train loss 0.19888442261441025\n",
      " val oa: 0.885 val_loss: 0.33209615104855883\n",
      "epoch: 1043\n",
      "global steps 33400 running loss: 0.024\n",
      "train oa: 91.55 train loss 0.19311425149221523\n",
      " val oa: 0.9 val_loss: 0.2986674904553335\n",
      "epoch: 1044\n",
      "train oa: 91.45 train loss 0.20272423937477335\n",
      " val oa: 0.8978 val_loss: 0.30968220379885664\n",
      "epoch: 1045\n",
      "train oa: 91.75 train loss 0.18178379661221372\n",
      " val oa: 0.8932 val_loss: 0.32032542590228513\n",
      "epoch: 1046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 90.7 train loss 0.2032393321221284\n",
      " val oa: 0.888 val_loss: 0.32392576274236773\n",
      "epoch: 1047\n",
      "train oa: 90.95 train loss 0.19959900449432433\n",
      " val oa: 0.8814 val_loss: 0.32622208873632036\n",
      "epoch: 1048\n",
      "train oa: 90.95 train loss 0.19570839695136785\n",
      " val oa: 0.8936 val_loss: 0.32551949605986075\n",
      "epoch: 1049\n",
      "global steps 33600 running loss: 0.029\n",
      "train oa: 92.15 train loss 0.18411103961357284\n",
      " val oa: 0.8966 val_loss: 0.2942555415995947\n",
      "epoch: 1050\n",
      "train oa: 91.8 train loss 0.18233052270370334\n",
      " val oa: 0.8912 val_loss: 0.3277252103785821\n",
      "epoch: 1051\n",
      "train oa: 91.45 train loss 0.19679126552196505\n",
      " val oa: 0.8754 val_loss: 0.365838465571775\n",
      "epoch: 1052\n",
      "train oa: 90.45 train loss 0.20033267094142923\n",
      " val oa: 0.8914 val_loss: 0.32133898075993556\n",
      "epoch: 1053\n",
      "train oa: 91.65 train loss 0.18963450673739454\n",
      " val oa: 0.9038 val_loss: 0.28918285616933437\n",
      "epoch: 1053 best val accuracy: 0.9038\n",
      "epoch: 1054\n",
      "train oa: 92.35 train loss 0.17406140097383846\n",
      " val oa: 0.8892 val_loss: 0.3403526151312025\n",
      "epoch: 1055\n",
      "train oa: 90.5 train loss 0.19743390260685248\n",
      " val oa: 0.9 val_loss: 0.30936382647712013\n",
      "epoch: 1056\n",
      "global steps 33800 running loss: 0.011\n",
      "train oa: 91.15 train loss 0.21561988909474816\n",
      " val oa: 0.89 val_loss: 0.3081382953236664\n",
      "epoch: 1057\n",
      "train oa: 90.95 train loss 0.19687574149068351\n",
      " val oa: 0.8994 val_loss: 0.29551868828248223\n",
      "epoch: 1058\n",
      "train oa: 91.45 train loss 0.18541505344745443\n",
      " val oa: 0.9032 val_loss: 0.2856196242455706\n",
      "epoch: 1059\n",
      "train oa: 91.8 train loss 0.18542263907459264\n",
      " val oa: 0.8836 val_loss: 0.34040374974359927\n",
      "epoch: 1060\n",
      "train oa: 92.15 train loss 0.17605836653646548\n",
      " val oa: 0.8996 val_loss: 0.31419765379063164\n",
      "epoch: 1061\n",
      "train oa: 92.1 train loss 0.1849002071966527\n",
      " val oa: 0.8852 val_loss: 0.33490486220464155\n",
      "epoch: 1062\n",
      "global steps 34000 running loss: 0.016\n",
      "train oa: 91.3 train loss 0.20219535367282335\n",
      " val oa: 0.8904 val_loss: 0.33432298400705934\n",
      "epoch: 1063\n",
      "train oa: 91.05 train loss 0.2047608299577371\n",
      " val oa: 0.8958 val_loss: 0.31647551601350804\n",
      "epoch: 1064\n",
      "train oa: 90.7 train loss 0.1962329413246162\n",
      " val oa: 0.8872 val_loss: 0.3309907896499764\n",
      "epoch: 1065\n",
      "train oa: 90.55 train loss 0.20678639406857058\n",
      " val oa: 0.8958 val_loss: 0.313732767976262\n",
      "epoch: 1066\n",
      "train oa: 91.45 train loss 0.19939810375092595\n",
      " val oa: 0.895 val_loss: 0.3059420959440843\n",
      "epoch: 1067\n",
      "train oa: 91.75 train loss 0.1878753615050243\n",
      " val oa: 0.8934 val_loss: 0.3272483152858137\n",
      "epoch: 1068\n",
      "global steps 34200 running loss: 0.024\n",
      "train oa: 90.7 train loss 0.204795119902985\n",
      " val oa: 0.889 val_loss: 0.3199463177819398\n",
      "epoch: 1069\n",
      "train oa: 92.0 train loss 0.18207101006095738\n",
      " val oa: 0.8914 val_loss: 0.31416286782793407\n",
      "epoch: 1070\n",
      "train oa: 91.2 train loss 0.190215128402976\n",
      " val oa: 0.888 val_loss: 0.3343103037571719\n",
      "epoch: 1071\n",
      "train oa: 90.35 train loss 0.20904833518094332\n",
      " val oa: 0.897 val_loss: 0.3099950856965445\n",
      "epoch: 1072\n",
      "train oa: 90.7 train loss 0.2060525586912144\n",
      " val oa: 0.8914 val_loss: 0.31397765809415384\n",
      "epoch: 1073\n",
      "train oa: 91.35 train loss 0.19558800170203788\n",
      " val oa: 0.893 val_loss: 0.337858094956621\n",
      "epoch: 1074\n",
      "global steps 34400 running loss: 0.031\n",
      "train oa: 91.25 train loss 0.19300094747924706\n",
      " val oa: 0.8958 val_loss: 0.3052499011842216\n",
      "epoch: 1075\n",
      "train oa: 90.95 train loss 0.1904801934499547\n",
      " val oa: 0.894 val_loss: 0.32824937472159854\n",
      "epoch: 1076\n",
      "train oa: 91.2 train loss 0.19748934461411366\n",
      " val oa: 0.8862 val_loss: 0.3450255645402926\n",
      "epoch: 1077\n",
      "train oa: 92.0 train loss 0.18450416935880465\n",
      " val oa: 0.8968 val_loss: 0.32132191231669766\n",
      "epoch: 1078\n",
      "train oa: 92.55 train loss 0.1799987297269024\n",
      " val oa: 0.8932 val_loss: 0.31868206948994604\n",
      "epoch: 1079\n",
      "train oa: 90.25 train loss 0.20430183787716605\n",
      " val oa: 0.895 val_loss: 0.31828481517140383\n",
      "epoch: 1080\n",
      "train oa: 91.05 train loss 0.19099909920592661\n",
      " val oa: 0.8908 val_loss: 0.31897942032445586\n",
      "epoch: 1081\n",
      "global steps 34600 running loss: 0.007\n",
      "train oa: 91.2 train loss 0.1955863967185906\n",
      " val oa: 0.8928 val_loss: 0.31099685767520896\n",
      "epoch: 1082\n",
      "train oa: 91.2 train loss 0.18972739675224204\n",
      " val oa: 0.8918 val_loss: 0.33251669786032273\n",
      "epoch: 1083\n",
      "train oa: 90.5 train loss 0.20284468559652616\n",
      " val oa: 0.8848 val_loss: 0.3329762857905495\n",
      "epoch: 1084\n",
      "train oa: 90.6 train loss 0.20767914389920128\n",
      " val oa: 0.8918 val_loss: 0.3183311501709187\n",
      "epoch: 1085\n",
      "train oa: 91.65 train loss 0.18869801876789954\n",
      " val oa: 0.8914 val_loss: 0.3213479837890826\n",
      "epoch: 1086\n",
      "train oa: 90.2 train loss 0.2130290609027746\n",
      " val oa: 0.8916 val_loss: 0.3314145274715898\n",
      "epoch: 1087\n",
      "global steps 34800 running loss: 0.016\n",
      "train oa: 91.5 train loss 0.18844674023618319\n",
      " val oa: 0.8962 val_loss: 0.3304164422463853\n",
      "epoch: 1088\n",
      "train oa: 92.6 train loss 0.17385567549561184\n",
      " val oa: 0.8922 val_loss: 0.3252430275117596\n",
      "epoch: 1089\n",
      "train oa: 92.3 train loss 0.1751510764013217\n",
      " val oa: 0.8872 val_loss: 0.3595018439694742\n",
      "epoch: 1090\n",
      "train oa: 90.9 train loss 0.20121927470269044\n",
      " val oa: 0.884 val_loss: 0.33256387754615174\n",
      "epoch: 1091\n",
      "train oa: 91.7 train loss 0.19460670280286654\n",
      " val oa: 0.89 val_loss: 0.3294244740025694\n",
      "epoch: 1092\n",
      "train oa: 91.05 train loss 0.20302051756752562\n",
      " val oa: 0.8948 val_loss: 0.30195734923183937\n",
      "epoch: 1093\n",
      "global steps 35000 running loss: 0.024\n",
      "train oa: 90.55 train loss 0.19731247242736827\n",
      " val oa: 0.8914 val_loss: 0.31589972134351035\n",
      "epoch: 1094\n",
      "train oa: 92.7 train loss 0.16958109647206307\n",
      " val oa: 0.8896 val_loss: 0.3428226099648682\n",
      "epoch: 1095\n",
      "train oa: 92.5 train loss 0.17076728513217587\n",
      " val oa: 0.895 val_loss: 0.3093428600391143\n",
      "epoch: 1096\n",
      "train oa: 91.35 train loss 0.184827632137276\n",
      " val oa: 0.896 val_loss: 0.3238691893140544\n",
      "epoch: 1097\n",
      "train oa: 90.85 train loss 0.2021340946501557\n",
      " val oa: 0.8922 val_loss: 0.32346059314812386\n",
      "epoch: 1098\n",
      "train oa: 91.3 train loss 0.18358377509897064\n",
      " val oa: 0.898 val_loss: 0.3154620165996385\n",
      "epoch: 1099\n",
      "global steps 35200 running loss: 0.031\n",
      "train oa: 91.9 train loss 0.19255152299714107\n",
      " val oa: 0.8988 val_loss: 0.31341578237420836\n",
      "epoch: 1100\n",
      "train oa: 91.95 train loss 0.19383444108647566\n",
      " val oa: 0.8844 val_loss: 0.3490473077116724\n",
      "epoch: 1101\n",
      "train oa: 91.3 train loss 0.20119141290131617\n",
      " val oa: 0.8856 val_loss: 0.3288605484417095\n",
      "epoch: 1102\n",
      "train oa: 90.8 train loss 0.200044276301717\n",
      " val oa: 0.8874 val_loss: 0.3563843503828802\n",
      "epoch: 1103\n",
      "train oa: 91.65 train loss 0.19881780490663054\n",
      " val oa: 0.877 val_loss: 0.3814941881761437\n",
      "epoch: 1104\n",
      "train oa: 90.0 train loss 0.21274539018615002\n",
      " val oa: 0.8898 val_loss: 0.31697465522436435\n",
      "epoch: 1105\n",
      "train oa: 91.55 train loss 0.19422195008264678\n",
      " val oa: 0.8798 val_loss: 0.35268982414799555\n",
      "epoch: 1106\n",
      "global steps 35400 running loss: 0.009\n",
      "train oa: 90.15 train loss 0.21267854837599978\n",
      " val oa: 0.8938 val_loss: 0.2984011078976368\n",
      "epoch: 1107\n",
      "train oa: 91.6 train loss 0.1858470695996749\n",
      " val oa: 0.8962 val_loss: 0.30625163217670637\n",
      "epoch: 1108\n",
      "train oa: 92.25 train loss 0.18139770548787054\n",
      " val oa: 0.8926 val_loss: 0.31782787158093345\n",
      "epoch: 1109\n",
      "train oa: 90.65 train loss 0.2099158694581017\n",
      " val oa: 0.898 val_loss: 0.30242302315601094\n",
      "epoch: 1110\n",
      "train oa: 91.5 train loss 0.18609385200318193\n",
      " val oa: 0.8942 val_loss: 0.3139527056872088\n",
      "epoch: 1111\n",
      "train oa: 91.15 train loss 0.19212861904783504\n",
      " val oa: 0.8868 val_loss: 0.3370822817738084\n",
      "epoch: 1112\n",
      "global steps 35600 running loss: 0.016\n",
      "train oa: 92.45 train loss 0.18271206930317724\n",
      " val oa: 0.8928 val_loss: 0.35824355193506635\n",
      "epoch: 1113\n",
      "train oa: 90.85 train loss 0.20818388436901336\n",
      " val oa: 0.8934 val_loss: 0.3127109907797176\n",
      "epoch: 1114\n",
      "train oa: 90.8 train loss 0.21545811720671332\n",
      " val oa: 0.8832 val_loss: 0.343708818240548\n",
      "epoch: 1115\n",
      "train oa: 92.05 train loss 0.1868248433277864\n",
      " val oa: 0.9 val_loss: 0.3087201364260994\n",
      "epoch: 1116\n",
      "train oa: 91.55 train loss 0.2036006667399442\n",
      " val oa: 0.8962 val_loss: 0.2933640855068148\n",
      "epoch: 1117\n",
      "train oa: 91.6 train loss 0.18952956366529014\n",
      " val oa: 0.8954 val_loss: 0.314201773901182\n",
      "epoch: 1118\n",
      "global steps 35800 running loss: 0.024\n",
      "train oa: 91.45 train loss 0.19258079325535832\n",
      " val oa: 0.8968 val_loss: 0.3097230673336212\n",
      "epoch: 1119\n",
      "train oa: 92.45 train loss 0.18130161706080222\n",
      " val oa: 0.8904 val_loss: 0.3231047671725939\n",
      "epoch: 1120\n",
      "train oa: 93.2 train loss 0.16000292287263965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.8906 val_loss: 0.32178685699868087\n",
      "epoch: 1121\n",
      "train oa: 92.7 train loss 0.17436804281906831\n",
      " val oa: 0.8938 val_loss: 0.32885168565235373\n",
      "epoch: 1122\n",
      "train oa: 91.7 train loss 0.18099474465904875\n",
      " val oa: 0.8958 val_loss: 0.30990283953053854\n",
      "epoch: 1123\n",
      "train oa: 91.75 train loss 0.20053781291972567\n",
      " val oa: 0.8952 val_loss: 0.31553559988541463\n",
      "epoch: 1124\n",
      "global steps 36000 running loss: 0.029\n",
      "train oa: 91.9 train loss 0.1840322184909235\n",
      " val oa: 0.8958 val_loss: 0.30896321690068085\n",
      "epoch: 1125\n",
      "train oa: 91.6 train loss 0.18463262340284792\n",
      " val oa: 0.8976 val_loss: 0.31168624175465026\n",
      "epoch: 1126\n",
      "train oa: 92.35 train loss 0.16676284261992821\n",
      " val oa: 0.881 val_loss: 0.35715817579646314\n",
      "epoch: 1127\n",
      "train oa: 91.45 train loss 0.17894103745159273\n",
      " val oa: 0.8988 val_loss: 0.3010191994049252\n",
      "epoch: 1128\n",
      "train oa: 91.6 train loss 0.18155853586399534\n",
      " val oa: 0.8848 val_loss: 0.3244566031756953\n",
      "epoch: 1129\n",
      "train oa: 91.85 train loss 0.18176797727896993\n",
      " val oa: 0.8974 val_loss: 0.3120725433748074\n",
      "epoch: 1130\n",
      "train oa: 91.25 train loss 0.18103199423947744\n",
      " val oa: 0.8952 val_loss: 0.32630910915247746\n",
      "epoch: 1131\n",
      "global steps 36200 running loss: 0.008\n",
      "train oa: 93.05 train loss 0.1729990206883554\n",
      " val oa: 0.8938 val_loss: 0.3201084431078423\n",
      "epoch: 1132\n",
      "train oa: 90.95 train loss 0.1950137377161487\n",
      " val oa: 0.9046 val_loss: 0.3025727419891525\n",
      "epoch: 1132 best val accuracy: 0.9046\n",
      "epoch: 1133\n",
      "train oa: 91.6 train loss 0.18202617438581234\n",
      " val oa: 0.8978 val_loss: 0.30182921272374713\n",
      "epoch: 1134\n",
      "train oa: 92.05 train loss 0.1776445987667982\n",
      " val oa: 0.8964 val_loss: 0.3225343681470387\n",
      "epoch: 1135\n",
      "train oa: 92.1 train loss 0.17678152391880178\n",
      " val oa: 0.8848 val_loss: 0.3507191251200987\n",
      "epoch: 1136\n",
      "train oa: 90.7 train loss 0.1868674024980768\n",
      " val oa: 0.8922 val_loss: 0.3153123268471994\n",
      "epoch: 1137\n",
      "global steps 36400 running loss: 0.014\n",
      "train oa: 91.15 train loss 0.19823007603714576\n",
      " val oa: 0.89 val_loss: 0.3222867083449795\n",
      "epoch: 1138\n",
      "train oa: 90.95 train loss 0.19853483764823224\n",
      " val oa: 0.8904 val_loss: 0.3384534773282396\n",
      "epoch: 1139\n",
      "train oa: 91.9 train loss 0.17596081781906467\n",
      " val oa: 0.8826 val_loss: 0.33455729367148285\n",
      "epoch: 1140\n",
      "train oa: 91.9 train loss 0.1821035068730134\n",
      " val oa: 0.8984 val_loss: 0.3186144221679464\n",
      "epoch: 1141\n",
      "train oa: 91.7 train loss 0.18095188890240327\n",
      " val oa: 0.8968 val_loss: 0.31477378523956523\n",
      "epoch: 1142\n",
      "train oa: 92.45 train loss 0.17010251325129386\n",
      " val oa: 0.893 val_loss: 0.32444769794744804\n",
      "epoch: 1143\n",
      "global steps 36600 running loss: 0.021\n",
      "train oa: 92.3 train loss 0.17254840820968997\n",
      " val oa: 0.8966 val_loss: 0.31907511290713386\n",
      "epoch: 1144\n",
      "train oa: 91.0 train loss 0.1970689771507036\n",
      " val oa: 0.8962 val_loss: 0.30878003493200995\n",
      "epoch: 1145\n",
      "train oa: 91.5 train loss 0.19197530097446688\n",
      " val oa: 0.8862 val_loss: 0.36624726936177693\n",
      "epoch: 1146\n",
      "train oa: 91.25 train loss 0.19936613215855675\n",
      " val oa: 0.8918 val_loss: 0.30361656083958183\n",
      "epoch: 1147\n",
      "train oa: 91.7 train loss 0.1955525641576376\n",
      " val oa: 0.8896 val_loss: 0.31852305165876593\n",
      "epoch: 1148\n",
      "train oa: 91.3 train loss 0.20231463195267826\n",
      " val oa: 0.8804 val_loss: 0.3332320139599068\n",
      "epoch: 1149\n",
      "global steps 36800 running loss: 0.031\n",
      "train oa: 91.2 train loss 0.19444311388781096\n",
      " val oa: 0.8772 val_loss: 0.3572040199422883\n",
      "epoch: 1150\n",
      "train oa: 91.45 train loss 0.1822317487706006\n",
      " val oa: 0.8912 val_loss: 0.34078586772194525\n",
      "epoch: 1151\n",
      "train oa: 91.05 train loss 0.18343130670265984\n",
      " val oa: 0.8946 val_loss: 0.3198298704075778\n",
      "epoch: 1152\n",
      "train oa: 90.7 train loss 0.20948863806565018\n",
      " val oa: 0.8932 val_loss: 0.3063364331319415\n",
      "epoch: 1153\n",
      "train oa: 92.85 train loss 0.16555152080000043\n",
      " val oa: 0.897 val_loss: 0.3246486553974218\n",
      "epoch: 1154\n",
      "train oa: 92.25 train loss 0.1862930504735363\n",
      " val oa: 0.8944 val_loss: 0.32044822276551516\n",
      "epoch: 1155\n",
      "train oa: 91.25 train loss 0.19541933467926267\n",
      " val oa: 0.896 val_loss: 0.2986410009314626\n",
      "epoch: 1156\n",
      "global steps 37000 running loss: 0.008\n",
      "train oa: 92.1 train loss 0.19194343962045907\n",
      " val oa: 0.8898 val_loss: 0.3226906997986614\n",
      "epoch: 1157\n",
      "train oa: 92.65 train loss 0.17240597408301023\n",
      " val oa: 0.8968 val_loss: 0.32041120973851384\n",
      "epoch: 1158\n",
      "train oa: 91.25 train loss 0.18920549594714878\n",
      " val oa: 0.8858 val_loss: 0.33699707370623294\n",
      "epoch: 1159\n",
      "train oa: 93.6 train loss 0.16627159201913713\n",
      " val oa: 0.891 val_loss: 0.32918992180584566\n",
      "epoch: 1160\n",
      "train oa: 91.75 train loss 0.19194928796990107\n",
      " val oa: 0.8996 val_loss: 0.3052859714070239\n",
      "epoch: 1161\n",
      "train oa: 91.6 train loss 0.18746276546925944\n",
      " val oa: 0.8872 val_loss: 0.3300035161816392\n",
      "epoch: 1162\n",
      "global steps 37200 running loss: 0.016\n",
      "train oa: 91.25 train loss 0.19156155301181166\n",
      " val oa: 0.898 val_loss: 0.32030131821431723\n",
      "epoch: 1163\n",
      "train oa: 91.4 train loss 0.17855794832857458\n",
      " val oa: 0.8892 val_loss: 0.3170122931251984\n",
      "epoch: 1164\n",
      "train oa: 91.6 train loss 0.18339439393137782\n",
      " val oa: 0.888 val_loss: 0.3355640027295877\n",
      "epoch: 1165\n",
      "train oa: 90.95 train loss 0.1951882404198648\n",
      " val oa: 0.8886 val_loss: 0.3218695855013023\n",
      "epoch: 1166\n",
      "train oa: 91.65 train loss 0.1798760550007963\n",
      " val oa: 0.8876 val_loss: 0.33942989930376216\n",
      "epoch: 1167\n",
      "train oa: 91.75 train loss 0.17601211542598288\n",
      " val oa: 0.8896 val_loss: 0.34422047768176905\n",
      "epoch: 1168\n",
      "global steps 37400 running loss: 0.022\n",
      "train oa: 91.85 train loss 0.19600216186746927\n",
      " val oa: 0.8926 val_loss: 0.3191518732525674\n",
      "epoch: 1169\n",
      "train oa: 92.1 train loss 0.17896602505153325\n",
      " val oa: 0.8878 val_loss: 0.31778994916874115\n",
      "epoch: 1170\n",
      "train oa: 90.6 train loss 0.1853999449074684\n",
      " val oa: 0.8896 val_loss: 0.3452917586902994\n",
      "epoch: 1171\n",
      "train oa: 91.95 train loss 0.18316205117004286\n",
      " val oa: 0.8956 val_loss: 0.3335336283865787\n",
      "epoch: 1172\n",
      "train oa: 91.2 train loss 0.18575295764004812\n",
      " val oa: 0.8884 val_loss: 0.3221993287628086\n",
      "epoch: 1173\n",
      "train oa: 91.55 train loss 0.18147307839599724\n",
      " val oa: 0.887 val_loss: 0.3402760768608323\n",
      "epoch: 1174\n",
      "global steps 37600 running loss: 0.033\n",
      "train oa: 91.0 train loss 0.20783900325864252\n",
      " val oa: 0.8832 val_loss: 0.3474273017475456\n",
      "epoch: 1175\n",
      "train oa: 91.25 train loss 0.19372327926051583\n",
      " val oa: 0.8966 val_loss: 0.29282685924407653\n",
      "epoch: 1176\n",
      "train oa: 91.75 train loss 0.18265377921028136\n",
      " val oa: 0.896 val_loss: 0.30455426391583995\n",
      "epoch: 1177\n",
      "train oa: 92.15 train loss 0.17787163689052793\n",
      " val oa: 0.8928 val_loss: 0.3151590244220504\n",
      "epoch: 1178\n",
      "train oa: 91.5 train loss 0.19435625790495972\n",
      " val oa: 0.8898 val_loss: 0.33019683924081117\n",
      "epoch: 1179\n",
      "train oa: 91.5 train loss 0.1888264217473386\n",
      " val oa: 0.8964 val_loss: 0.3054421094660306\n",
      "epoch: 1180\n",
      "train oa: 92.2 train loss 0.19323807663933296\n",
      " val oa: 0.8896 val_loss: 0.31174216426667434\n",
      "epoch: 1181\n",
      "global steps 37800 running loss: 0.007\n",
      "train oa: 91.65 train loss 0.1852342166065515\n",
      " val oa: 0.894 val_loss: 0.31741241326406555\n",
      "epoch: 1182\n",
      "train oa: 91.7 train loss 0.19075705876888932\n",
      " val oa: 0.8978 val_loss: 0.317360857257796\n",
      "epoch: 1183\n",
      "train oa: 88.55 train loss 0.23651178344468812\n",
      " val oa: 0.9012 val_loss: 0.2843906856974829\n",
      "epoch: 1184\n",
      "train oa: 91.5 train loss 0.18751292228038957\n",
      " val oa: 0.9002 val_loss: 0.29796020722084365\n",
      "epoch: 1185\n",
      "train oa: 91.35 train loss 0.18800241630979544\n",
      " val oa: 0.9014 val_loss: 0.30233682108604015\n",
      "epoch: 1186\n",
      "train oa: 92.45 train loss 0.1765806184269594\n",
      " val oa: 0.8934 val_loss: 0.3314308925183351\n",
      "epoch: 1187\n",
      "global steps 38000 running loss: 0.015\n",
      "train oa: 91.85 train loss 0.18769153066665356\n",
      " val oa: 0.8982 val_loss: 0.30584961408346056\n",
      "epoch: 1188\n",
      "train oa: 92.0 train loss 0.1875023668658131\n",
      " val oa: 0.8878 val_loss: 0.3301864627033004\n",
      "epoch: 1189\n",
      "train oa: 90.95 train loss 0.1907290010742196\n",
      " val oa: 0.9018 val_loss: 0.3023733690797067\n",
      "epoch: 1190\n",
      "train oa: 92.2 train loss 0.17960470845242862\n",
      " val oa: 0.8906 val_loss: 0.3552546597059829\n",
      "epoch: 1191\n",
      "train oa: 92.35 train loss 0.17429187831000073\n",
      " val oa: 0.8838 val_loss: 0.37103286153658455\n",
      "epoch: 1192\n",
      "train oa: 90.85 train loss 0.2102239788184154\n",
      " val oa: 0.878 val_loss: 0.38554287913270835\n",
      "epoch: 1193\n",
      "global steps 38200 running loss: 0.022\n",
      "train oa: 91.45 train loss 0.1846065010391275\n",
      " val oa: 0.897 val_loss: 0.3231123026224519\n",
      "epoch: 1194\n",
      "train oa: 92.2 train loss 0.16662091456569225\n",
      " val oa: 0.888 val_loss: 0.3349513903781531\n",
      "epoch: 1195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 91.55 train loss 0.1785993008908412\n",
      " val oa: 0.8892 val_loss: 0.35610393454937006\n",
      "epoch: 1196\n",
      "train oa: 91.75 train loss 0.17761047128237137\n",
      " val oa: 0.8852 val_loss: 0.34489980864423586\n",
      "epoch: 1197\n",
      "train oa: 91.2 train loss 0.19666349575007389\n",
      " val oa: 0.8912 val_loss: 0.3230245101060052\n",
      "epoch: 1198\n",
      "train oa: 93.2 train loss 0.1591897599509958\n",
      " val oa: 0.8848 val_loss: 0.35492446826796864\n",
      "epoch: 1199\n",
      "global steps 38400 running loss: 0.031\n",
      "train oa: 91.45 train loss 0.1962402524368902\n",
      " val oa: 0.8956 val_loss: 0.3365997997579544\n",
      "epoch: 1200\n",
      "train oa: 91.45 train loss 0.18721201539549906\n",
      " val oa: 0.891 val_loss: 0.33765896508292736\n",
      "epoch: 1201\n",
      "train oa: 91.3 train loss 0.18917760350244583\n",
      " val oa: 0.8918 val_loss: 0.33170255819829825\n",
      "epoch: 1202\n",
      "train oa: 91.85 train loss 0.18027293189840152\n",
      " val oa: 0.894 val_loss: 0.3296196811633273\n",
      "epoch: 1203\n",
      "train oa: 91.85 train loss 0.17233272839272756\n",
      " val oa: 0.8912 val_loss: 0.32436261688119766\n",
      "epoch: 1204\n",
      "train oa: 91.7 train loss 0.17426122311288622\n",
      " val oa: 0.8974 val_loss: 0.32892891511194144\n",
      "epoch: 1205\n",
      "train oa: 92.6 train loss 0.16991380520218216\n",
      " val oa: 0.8884 val_loss: 0.32473543984600756\n",
      "epoch: 1206\n",
      "global steps 38600 running loss: 0.007\n",
      "train oa: 92.0 train loss 0.19104252776940098\n",
      " val oa: 0.8876 val_loss: 0.32200415725381354\n",
      "epoch: 1207\n",
      "train oa: 90.85 train loss 0.18754252953380487\n",
      " val oa: 0.8798 val_loss: 0.3710748646025586\n",
      "epoch: 1208\n",
      "train oa: 92.25 train loss 0.17890930560380502\n",
      " val oa: 0.8902 val_loss: 0.3284125976712231\n",
      "epoch: 1209\n",
      "train oa: 91.7 train loss 0.17483010547306102\n",
      " val oa: 0.886 val_loss: 0.35767187230903813\n",
      "epoch: 1210\n",
      "train oa: 91.35 train loss 0.18569276531074935\n",
      " val oa: 0.8872 val_loss: 0.34458857984038105\n",
      "epoch: 1211\n",
      "train oa: 91.7 train loss 0.18321629608228626\n",
      " val oa: 0.8894 val_loss: 0.32049101763391413\n",
      "epoch: 1212\n",
      "global steps 38800 running loss: 0.014\n",
      "train oa: 92.6 train loss 0.1677631122142504\n",
      " val oa: 0.8932 val_loss: 0.33482990455665457\n",
      "epoch: 1213\n",
      "train oa: 92.5 train loss 0.17047802689152394\n",
      " val oa: 0.891 val_loss: 0.334189348157007\n",
      "epoch: 1214\n",
      "train oa: 91.5 train loss 0.17386334602720116\n",
      " val oa: 0.8898 val_loss: 0.34178385470411404\n",
      "epoch: 1215\n",
      "train oa: 92.5 train loss 0.16578859771674012\n",
      " val oa: 0.8914 val_loss: 0.336020072580729\n",
      "epoch: 1216\n",
      "train oa: 92.2 train loss 0.17653166293283315\n",
      " val oa: 0.892 val_loss: 0.35041194783225593\n",
      "epoch: 1217\n",
      "train oa: 92.05 train loss 0.18018510051023576\n",
      " val oa: 0.8926 val_loss: 0.32430982890633403\n",
      "epoch: 1218\n",
      "global steps 39000 running loss: 0.021\n",
      "train oa: 92.1 train loss 0.17304200533939407\n",
      " val oa: 0.8982 val_loss: 0.317516767804695\n",
      "epoch: 1219\n",
      "train oa: 91.9 train loss 0.16438399478198104\n",
      " val oa: 0.8966 val_loss: 0.32550394393190846\n",
      "epoch: 1220\n",
      "train oa: 92.75 train loss 0.1653160419789842\n",
      " val oa: 0.9018 val_loss: 0.32250211273398616\n",
      "epoch: 1221\n",
      "train oa: 92.85 train loss 0.15832023527641606\n",
      " val oa: 0.897 val_loss: 0.3226671545822553\n",
      "epoch: 1222\n",
      "train oa: 93.1 train loss 0.1681975870500386\n",
      " val oa: 0.9 val_loss: 0.327906550931923\n",
      "epoch: 1223\n",
      "train oa: 91.1 train loss 0.19821571775799732\n",
      " val oa: 0.898 val_loss: 0.30322286454901964\n",
      "epoch: 1224\n",
      "global steps 39200 running loss: 0.028\n",
      "train oa: 92.35 train loss 0.17512459904922803\n",
      " val oa: 0.8968 val_loss: 0.3256630620302606\n",
      "epoch: 1225\n",
      "train oa: 91.2 train loss 0.19856066885022955\n",
      " val oa: 0.8862 val_loss: 0.3553153316269467\n",
      "epoch: 1226\n",
      "train oa: 92.3 train loss 0.19300289838006243\n",
      " val oa: 0.8926 val_loss: 0.3180945991310906\n",
      "epoch: 1227\n",
      "train oa: 92.0 train loss 0.1789404300933908\n",
      " val oa: 0.8912 val_loss: 0.32332430509407417\n",
      "epoch: 1228\n",
      "train oa: 92.2 train loss 0.1683723200738661\n",
      " val oa: 0.8946 val_loss: 0.33323712542747597\n",
      "epoch: 1229\n",
      "train oa: 92.35 train loss 0.1781259722738525\n",
      " val oa: 0.8866 val_loss: 0.3450437898101636\n",
      "epoch: 1230\n",
      "train oa: 92.45 train loss 0.16336947180929265\n",
      " val oa: 0.8904 val_loss: 0.3065844663669863\n",
      "epoch: 1231\n",
      "global steps 39400 running loss: 0.007\n",
      "train oa: 92.85 train loss 0.16781446269586314\n",
      " val oa: 0.8858 val_loss: 0.3579709757412246\n",
      "epoch: 1232\n",
      "train oa: 92.15 train loss 0.18725730140903216\n",
      " val oa: 0.8942 val_loss: 0.3129753057788327\n",
      "epoch: 1233\n",
      "train oa: 92.9 train loss 0.15997656350681072\n",
      " val oa: 0.9 val_loss: 0.2979387844054526\n",
      "epoch: 1234\n",
      "train oa: 91.6 train loss 0.19545057215873274\n",
      " val oa: 0.8798 val_loss: 0.37258367843385465\n",
      "epoch: 1235\n",
      "train oa: 90.85 train loss 0.19756767269965558\n",
      " val oa: 0.8888 val_loss: 0.32716213616689166\n",
      "epoch: 1236\n",
      "train oa: 92.05 train loss 0.1794771489440538\n",
      " val oa: 0.8886 val_loss: 0.31475917776373535\n",
      "epoch: 1237\n",
      "global steps 39600 running loss: 0.013\n",
      "train oa: 92.05 train loss 0.18107546534206376\n",
      " val oa: 0.8862 val_loss: 0.33107642861583714\n",
      "epoch: 1238\n",
      "train oa: 91.65 train loss 0.20030236525004164\n",
      " val oa: 0.8938 val_loss: 0.3308220271915054\n",
      "epoch: 1239\n",
      "train oa: 92.7 train loss 0.17815052210287938\n",
      " val oa: 0.8974 val_loss: 0.3020655359689892\n",
      "epoch: 1240\n",
      "train oa: 91.45 train loss 0.2086395852465027\n",
      " val oa: 0.8888 val_loss: 0.35416689166401466\n",
      "epoch: 1241\n",
      "train oa: 91.2 train loss 0.17816980831449333\n",
      " val oa: 0.8946 val_loss: 0.3231349827771836\n",
      "epoch: 1242\n",
      "train oa: 92.7 train loss 0.16164064942378875\n",
      " val oa: 0.8868 val_loss: 0.3629852593088148\n",
      "epoch: 1243\n",
      "global steps 39800 running loss: 0.021\n",
      "train oa: 92.25 train loss 0.17381228877246838\n",
      " val oa: 0.8922 val_loss: 0.3412374577954287\n",
      "epoch: 1244\n",
      "train oa: 92.5 train loss 0.17496067851293753\n",
      " val oa: 0.898 val_loss: 0.3014463268395832\n",
      "epoch: 1245\n",
      "train oa: 92.05 train loss 0.17745661499906262\n",
      " val oa: 0.8938 val_loss: 0.3435691432699796\n",
      "epoch: 1246\n",
      "train oa: 92.4 train loss 0.1656742667302805\n",
      " val oa: 0.8944 val_loss: 0.3186721333588817\n",
      "epoch: 1247\n",
      "train oa: 92.95 train loss 0.1592307294773233\n",
      " val oa: 0.8932 val_loss: 0.34001259981958504\n",
      "epoch: 1248\n",
      "train oa: 92.05 train loss 0.17753155188816747\n",
      " val oa: 0.8874 val_loss: 0.33721395613224203\n",
      "epoch: 1249\n",
      "global steps 40000 running loss: 0.028\n",
      "train oa: 92.45 train loss 0.17509136438014183\n",
      " val oa: 0.8922 val_loss: 0.3171453905824687\n",
      "epoch: 1250\n",
      "train oa: 93.25 train loss 0.16527745328863191\n",
      " val oa: 0.8974 val_loss: 0.31144826446458157\n",
      "epoch: 1251\n",
      "train oa: 92.25 train loss 0.1651860141344733\n",
      " val oa: 0.8762 val_loss: 0.3794930655871062\n",
      "epoch: 1252\n",
      "train oa: 92.2 train loss 0.17021698276682126\n",
      " val oa: 0.898 val_loss: 0.3300757192329212\n",
      "epoch: 1253\n",
      "train oa: 92.2 train loss 0.18467684116702923\n",
      " val oa: 0.8924 val_loss: 0.33795921683362323\n",
      "epoch: 1254\n",
      "train oa: 92.05 train loss 0.18809115671328427\n",
      " val oa: 0.892 val_loss: 0.3189327910959922\n",
      "epoch: 1255\n",
      "train oa: 92.5 train loss 0.16825561239545117\n",
      " val oa: 0.8952 val_loss: 0.32015597106871285\n",
      "epoch: 1256\n",
      "global steps 40200 running loss: 0.006\n",
      "train oa: 92.4 train loss 0.1681154036325127\n",
      " val oa: 0.8942 val_loss: 0.33238353916529917\n",
      "epoch: 1257\n",
      "train oa: 92.55 train loss 0.15763005787273646\n",
      " val oa: 0.892 val_loss: 0.3546435041392719\n",
      "epoch: 1258\n",
      "train oa: 92.35 train loss 0.17073926069621329\n",
      " val oa: 0.88 val_loss: 0.40916780535231967\n",
      "epoch: 1259\n",
      "train oa: 92.9 train loss 0.15474636494071953\n",
      " val oa: 0.896 val_loss: 0.3397536533550318\n",
      "epoch: 1260\n",
      "train oa: 93.3 train loss 0.14677713949502671\n",
      " val oa: 0.9022 val_loss: 0.32569292584177717\n",
      "epoch: 1261\n",
      "train oa: 92.45 train loss 0.17237016455472468\n",
      " val oa: 0.8938 val_loss: 0.36424098948805517\n",
      "epoch: 1262\n",
      "global steps 40400 running loss: 0.015\n",
      "train oa: 92.55 train loss 0.1744228249415494\n",
      " val oa: 0.887 val_loss: 0.3479285273486518\n",
      "epoch: 1263\n",
      "train oa: 92.2 train loss 0.17960000444805063\n",
      " val oa: 0.8994 val_loss: 0.2876506624685738\n",
      "epoch: 1264\n",
      "train oa: 91.4 train loss 0.18572784858147448\n",
      " val oa: 0.8984 val_loss: 0.3052668340934458\n",
      "epoch: 1265\n",
      "train oa: 92.65 train loss 0.16417119839056385\n",
      " val oa: 0.8952 val_loss: 0.31579787918410035\n",
      "epoch: 1266\n",
      "train oa: 92.8 train loss 0.17451927852152738\n",
      " val oa: 0.8976 val_loss: 0.33117682638703994\n",
      "epoch: 1267\n",
      "train oa: 91.65 train loss 0.18532342987184372\n",
      " val oa: 0.8956 val_loss: 0.31619352047107085\n",
      "epoch: 1268\n",
      "global steps 40600 running loss: 0.018\n",
      "train oa: 92.8 train loss 0.1630804062972123\n",
      " val oa: 0.8962 val_loss: 0.32459559175540587\n",
      "epoch: 1269\n",
      "train oa: 92.3 train loss 0.17785502050987245\n",
      " val oa: 0.8974 val_loss: 0.32479944197984767\n",
      "epoch: 1270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 92.65 train loss 0.18823936515960982\n",
      " val oa: 0.8948 val_loss: 0.32175641010628375\n",
      "epoch: 1271\n",
      "train oa: 91.55 train loss 0.1807254614731457\n",
      " val oa: 0.892 val_loss: 0.32621912965812494\n",
      "epoch: 1272\n",
      "train oa: 91.95 train loss 0.16994237211557825\n",
      " val oa: 0.8984 val_loss: 0.31404719954555255\n",
      "epoch: 1273\n",
      "train oa: 93.3 train loss 0.16670343249580627\n",
      " val oa: 0.8952 val_loss: 0.3313105323663006\n",
      "epoch: 1274\n",
      "global steps 40800 running loss: 0.027\n",
      "train oa: 92.4 train loss 0.16869334368922756\n",
      " val oa: 0.897 val_loss: 0.317239968547061\n",
      "epoch: 1275\n",
      "train oa: 92.7 train loss 0.16021682876730262\n",
      " val oa: 0.881 val_loss: 0.3519434634088341\n",
      "epoch: 1276\n",
      "train oa: 92.45 train loss 0.16201527916608133\n",
      " val oa: 0.8888 val_loss: 0.35487168212641185\n",
      "epoch: 1277\n",
      "train oa: 92.5 train loss 0.17979734582874812\n",
      " val oa: 0.8906 val_loss: 0.3213441266907836\n",
      "epoch: 1278\n",
      "train oa: 92.8 train loss 0.16968468049887772\n",
      " val oa: 0.8892 val_loss: 0.34597528877837247\n",
      "epoch: 1279\n",
      "train oa: 93.15 train loss 0.15947904022626297\n",
      " val oa: 0.8964 val_loss: 0.3260024371212858\n",
      "epoch: 1280\n",
      "train oa: 92.05 train loss 0.18511038610539104\n",
      " val oa: 0.8988 val_loss: 0.3138597924366764\n",
      "epoch: 1281\n",
      "global steps 41000 running loss: 0.006\n",
      "train oa: 92.4 train loss 0.16984740466021103\n",
      " val oa: 0.9006 val_loss: 0.3274023941098675\n",
      "epoch: 1282\n",
      "train oa: 92.95 train loss 0.15337278982659247\n",
      " val oa: 0.8956 val_loss: 0.3427544129786573\n",
      "epoch: 1283\n",
      "train oa: 92.75 train loss 0.16695351874321873\n",
      " val oa: 0.8832 val_loss: 0.37573795772508317\n",
      "epoch: 1284\n",
      "train oa: 91.8 train loss 0.18425860481749923\n",
      " val oa: 0.8922 val_loss: 0.3232986172012482\n",
      "epoch: 1285\n",
      "train oa: 91.45 train loss 0.18716060010883148\n",
      " val oa: 0.8896 val_loss: 0.3261984254050914\n",
      "epoch: 1286\n",
      "train oa: 92.3 train loss 0.16107207780329866\n",
      " val oa: 0.8992 val_loss: 0.32990266613146\n",
      "epoch: 1287\n",
      "global steps 41200 running loss: 0.013\n",
      "train oa: 92.55 train loss 0.16737260168604348\n",
      " val oa: 0.8944 val_loss: 0.34768855826939543\n",
      "epoch: 1288\n",
      "train oa: 92.55 train loss 0.1631484435216433\n",
      " val oa: 0.8948 val_loss: 0.3190897453006489\n",
      "epoch: 1289\n",
      "train oa: 93.25 train loss 0.16363655238341562\n",
      " val oa: 0.894 val_loss: 0.34055037368265717\n",
      "epoch: 1290\n",
      "train oa: 93.05 train loss 0.15402485253336706\n",
      " val oa: 0.8922 val_loss: 0.336684226866242\n",
      "epoch: 1291\n",
      "train oa: 93.75 train loss 0.1442771725585735\n",
      " val oa: 0.8972 val_loss: 0.3337383903897055\n",
      "epoch: 1292\n",
      "train oa: 93.2 train loss 0.15180704425126187\n",
      " val oa: 0.8928 val_loss: 0.3577227898443709\n",
      "epoch: 1293\n",
      "global steps 41400 running loss: 0.018\n",
      "train oa: 93.4 train loss 0.1598881697230236\n",
      " val oa: 0.8962 val_loss: 0.3355975161845074\n",
      "epoch: 1294\n",
      "train oa: 92.6 train loss 0.16797540205603056\n",
      " val oa: 0.8916 val_loss: 0.3704851326785425\n",
      "epoch: 1295\n",
      "train oa: 91.6 train loss 0.18119731215619445\n",
      " val oa: 0.889 val_loss: 0.35041140702436097\n",
      "epoch: 1296\n",
      "train oa: 90.65 train loss 0.2035392056931262\n",
      " val oa: 0.8914 val_loss: 0.3335464873909355\n",
      "epoch: 1297\n",
      "train oa: 92.15 train loss 0.17531517147940523\n",
      " val oa: 0.8952 val_loss: 0.3049081592208826\n",
      "epoch: 1298\n",
      "train oa: 93.2 train loss 0.16656694058211188\n",
      " val oa: 0.8936 val_loss: 0.3264593570230126\n",
      "epoch: 1299\n",
      "global steps 41600 running loss: 0.028\n",
      "train oa: 91.85 train loss 0.17334212389424405\n",
      " val oa: 0.8972 val_loss: 0.31647145093793644\n",
      "epoch: 1300\n",
      "train oa: 92.05 train loss 0.17592952369539872\n",
      " val oa: 0.8898 val_loss: 0.3330684820756494\n",
      "epoch: 1301\n",
      "train oa: 91.85 train loss 0.18129283700903714\n",
      " val oa: 0.903 val_loss: 0.3064489121209114\n",
      "epoch: 1302\n",
      "train oa: 92.3 train loss 0.19845858957763476\n",
      " val oa: 0.8954 val_loss: 0.3273333316588255\n",
      "epoch: 1303\n",
      "train oa: 92.65 train loss 0.16445950637171114\n",
      " val oa: 0.8982 val_loss: 0.3134502264736403\n",
      "epoch: 1304\n",
      "train oa: 93.7 train loss 0.1545600109208191\n",
      " val oa: 0.8948 val_loss: 0.3231004893846978\n",
      "epoch: 1305\n",
      "train oa: 91.9 train loss 0.18500746023198425\n",
      " val oa: 0.881 val_loss: 0.40333417917432884\n",
      "epoch: 1306\n",
      "global steps 41800 running loss: 0.009\n",
      "train oa: 90.85 train loss 0.2080182191435915\n",
      " val oa: 0.893 val_loss: 0.32486466330065994\n",
      "epoch: 1307\n",
      "train oa: 92.1 train loss 0.1758846867375595\n",
      " val oa: 0.8966 val_loss: 0.3132984984550378\n",
      "epoch: 1308\n",
      "train oa: 93.1 train loss 0.15831504223272613\n",
      " val oa: 0.8928 val_loss: 0.34169698024860934\n",
      "epoch: 1309\n",
      "train oa: 92.05 train loss 0.175500994191892\n",
      " val oa: 0.8858 val_loss: 0.3492037194142547\n",
      "epoch: 1310\n",
      "train oa: 93.3 train loss 0.15969479382263937\n",
      " val oa: 0.896 val_loss: 0.34231948402599655\n",
      "epoch: 1311\n",
      "train oa: 92.3 train loss 0.15678822809991957\n",
      " val oa: 0.898 val_loss: 0.33354052142044777\n",
      "epoch: 1312\n",
      "global steps 42000 running loss: 0.015\n",
      "train oa: 92.0 train loss 0.17492015314182816\n",
      " val oa: 0.888 val_loss: 0.3684344950473416\n",
      "epoch: 1313\n",
      "train oa: 92.95 train loss 0.15956910424612103\n",
      " val oa: 0.8914 val_loss: 0.344432108508578\n",
      "epoch: 1314\n",
      "train oa: 91.5 train loss 0.18637347542727334\n",
      " val oa: 0.8868 val_loss: 0.3580870691976366\n",
      "epoch: 1315\n",
      "train oa: 92.8 train loss 0.17220432714699058\n",
      " val oa: 0.8956 val_loss: 0.3043980466253668\n",
      "epoch: 1316\n",
      "train oa: 92.25 train loss 0.18712242595568215\n",
      " val oa: 0.8892 val_loss: 0.3391973960247253\n",
      "epoch: 1317\n",
      "train oa: 91.55 train loss 0.18839146935276535\n",
      " val oa: 0.896 val_loss: 0.3196672065415753\n",
      "epoch: 1318\n",
      "global steps 42200 running loss: 0.020\n",
      "train oa: 92.6 train loss 0.16482684031077163\n",
      " val oa: 0.9028 val_loss: 0.31284695190566403\n",
      "epoch: 1319\n",
      "train oa: 92.35 train loss 0.17335476028620608\n",
      " val oa: 0.8906 val_loss: 0.3286981469855932\n",
      "epoch: 1320\n",
      "train oa: 92.65 train loss 0.16550260098391098\n",
      " val oa: 0.8908 val_loss: 0.3385177530191499\n",
      "epoch: 1321\n",
      "train oa: 92.4 train loss 0.168393958484384\n",
      " val oa: 0.8978 val_loss: 0.3474443712813714\n",
      "epoch: 1322\n",
      "train oa: 92.7 train loss 0.16704359407805897\n",
      " val oa: 0.8872 val_loss: 0.35365162701266245\n",
      "epoch: 1323\n",
      "train oa: 92.35 train loss 0.1706608565188712\n",
      " val oa: 0.8972 val_loss: 0.32513406847687043\n",
      "epoch: 1324\n",
      "global steps 42400 running loss: 0.027\n",
      "train oa: 92.85 train loss 0.16615201768281543\n",
      " val oa: 0.8946 val_loss: 0.33530351598916486\n",
      "epoch: 1325\n",
      "train oa: 93.05 train loss 0.16095551502851507\n",
      " val oa: 0.8972 val_loss: 0.32453252505085006\n",
      "epoch: 1326\n",
      "train oa: 92.8 train loss 0.16435778125559936\n",
      " val oa: 0.8958 val_loss: 0.3441824689915051\n",
      "epoch: 1327\n",
      "train oa: 91.75 train loss 0.18253199159496414\n",
      " val oa: 0.8784 val_loss: 0.38124012817684244\n",
      "epoch: 1328\n",
      "train oa: 92.25 train loss 0.18124603392459707\n",
      " val oa: 0.8912 val_loss: 0.33185747469255183\n",
      "epoch: 1329\n",
      "train oa: 92.55 train loss 0.16436418859938978\n",
      " val oa: 0.8936 val_loss: 0.32314722278789115\n",
      "epoch: 1330\n",
      "train oa: 92.85 train loss 0.1568309444640218\n",
      " val oa: 0.8834 val_loss: 0.3684586008828362\n",
      "epoch: 1331\n",
      "global steps 42600 running loss: 0.006\n",
      "train oa: 93.5 train loss 0.15036308230005474\n",
      " val oa: 0.8964 val_loss: 0.3482947997044551\n",
      "epoch: 1332\n",
      "train oa: 93.3 train loss 0.15921882972655949\n",
      " val oa: 0.9004 val_loss: 0.3267203637745892\n",
      "epoch: 1333\n",
      "train oa: 92.9 train loss 0.15960478528312208\n",
      " val oa: 0.8958 val_loss: 0.3161628400296697\n",
      "epoch: 1334\n",
      "train oa: 93.0 train loss 0.15561312591384044\n",
      " val oa: 0.9046 val_loss: 0.32287470962204984\n",
      "epoch: 1335\n",
      "train oa: 93.1 train loss 0.15875188468457352\n",
      " val oa: 0.8958 val_loss: 0.31798039400886285\n",
      "epoch: 1336\n",
      "train oa: 93.1 train loss 0.15516281291121686\n",
      " val oa: 0.8876 val_loss: 0.3673956932206877\n",
      "epoch: 1337\n",
      "global steps 42800 running loss: 0.012\n",
      "train oa: 93.3 train loss 0.15594293679282895\n",
      " val oa: 0.8914 val_loss: 0.3554233768612687\n",
      "epoch: 1338\n",
      "train oa: 92.5 train loss 0.16480315669771897\n",
      " val oa: 0.8906 val_loss: 0.32704508595521525\n",
      "epoch: 1339\n",
      "train oa: 93.15 train loss 0.15639652018832484\n",
      " val oa: 0.8844 val_loss: 0.38079902301215174\n",
      "epoch: 1340\n",
      "train oa: 92.35 train loss 0.1683992738745792\n",
      " val oa: 0.8924 val_loss: 0.3495164693867545\n",
      "epoch: 1341\n",
      "train oa: 92.65 train loss 0.16601449180899647\n",
      " val oa: 0.891 val_loss: 0.3563989693258246\n",
      "epoch: 1342\n",
      "train oa: 92.45 train loss 0.1720228131055584\n",
      " val oa: 0.8852 val_loss: 0.35060265831337467\n",
      "epoch: 1343\n",
      "global steps 43000 running loss: 0.022\n",
      "train oa: 92.2 train loss 0.16589180088923505\n",
      " val oa: 0.8964 val_loss: 0.32823418952991273\n",
      "epoch: 1344\n",
      "train oa: 93.9 train loss 0.14614785096642524\n",
      " val oa: 0.889 val_loss: 0.3588754594865841\n",
      "epoch: 1345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 93.05 train loss 0.15959578556692403\n",
      " val oa: 0.8948 val_loss: 0.3297665982163519\n",
      "epoch: 1346\n",
      "train oa: 92.75 train loss 0.16604156156120548\n",
      " val oa: 0.8852 val_loss: 0.39220273670050765\n",
      "epoch: 1347\n",
      "train oa: 92.35 train loss 0.1715614908162479\n",
      " val oa: 0.9042 val_loss: 0.29369501887918176\n",
      "epoch: 1348\n",
      "train oa: 92.25 train loss 0.1763832618792806\n",
      " val oa: 0.9024 val_loss: 0.3109827517507461\n",
      "epoch: 1349\n",
      "global steps 43200 running loss: 0.025\n",
      "train oa: 93.4 train loss 0.15905383990237443\n",
      " val oa: 0.9024 val_loss: 0.3153251056082595\n",
      "epoch: 1350\n",
      "train oa: 92.75 train loss 0.16519172499591162\n",
      " val oa: 0.887 val_loss: 0.349662080122193\n",
      "epoch: 1351\n",
      "train oa: 92.75 train loss 0.1642056553921629\n",
      " val oa: 0.8892 val_loss: 0.32801473373638634\n",
      "epoch: 1352\n",
      "train oa: 93.05 train loss 0.15708732796788955\n",
      " val oa: 0.8928 val_loss: 0.32917017854748054\n",
      "epoch: 1353\n",
      "train oa: 92.55 train loss 0.17266554627581984\n",
      " val oa: 0.8968 val_loss: 0.31753174897040765\n",
      "epoch: 1354\n",
      "train oa: 92.05 train loss 0.18715202766779118\n",
      " val oa: 0.8892 val_loss: 0.3264440560644433\n",
      "epoch: 1355\n",
      "train oa: 93.25 train loss 0.16293046152288865\n",
      " val oa: 0.8896 val_loss: 0.32942569031192204\n",
      "epoch: 1356\n",
      "global steps 43400 running loss: 0.007\n",
      "train oa: 93.35 train loss 0.16747596046856758\n",
      " val oa: 0.9012 val_loss: 0.3059314791958051\n",
      "epoch: 1357\n",
      "train oa: 93.35 train loss 0.1517452956665998\n",
      " val oa: 0.896 val_loss: 0.3258722669734105\n",
      "epoch: 1358\n",
      "train oa: 94.05 train loss 0.1451714266524319\n",
      " val oa: 0.9036 val_loss: 0.31854995964344573\n",
      "epoch: 1359\n",
      "train oa: 93.45 train loss 0.1583965963735795\n",
      " val oa: 0.903 val_loss: 0.30954295168037976\n",
      "epoch: 1360\n",
      "train oa: 92.2 train loss 0.16442996590268152\n",
      " val oa: 0.8944 val_loss: 0.3349089929979368\n",
      "epoch: 1361\n",
      "train oa: 92.35 train loss 0.15925394356842876\n",
      " val oa: 0.8972 val_loss: 0.33057210228102046\n",
      "epoch: 1362\n",
      "global steps 43600 running loss: 0.014\n",
      "train oa: 92.6 train loss 0.1739278862794181\n",
      " val oa: 0.8926 val_loss: 0.3253125562813588\n",
      "epoch: 1363\n",
      "train oa: 92.0 train loss 0.18233370411701497\n",
      " val oa: 0.8946 val_loss: 0.33621993530840744\n",
      "epoch: 1364\n",
      "train oa: 92.6 train loss 0.16346929237798935\n",
      " val oa: 0.8994 val_loss: 0.32513788621023554\n",
      "epoch: 1365\n",
      "train oa: 92.5 train loss 0.16842608309540916\n",
      " val oa: 0.889 val_loss: 0.34738442138925896\n",
      "epoch: 1366\n",
      "train oa: 92.5 train loss 0.16088200383993195\n",
      " val oa: 0.8952 val_loss: 0.32635621639541645\n",
      "epoch: 1367\n",
      "train oa: 93.05 train loss 0.15506494688607386\n",
      " val oa: 0.8976 val_loss: 0.31877984023067313\n",
      "epoch: 1368\n",
      "global steps 43800 running loss: 0.020\n",
      "train oa: 92.4 train loss 0.1643618614019871\n",
      " val oa: 0.8982 val_loss: 0.34915730247619964\n",
      "epoch: 1369\n",
      "train oa: 93.05 train loss 0.15871172373592138\n",
      " val oa: 0.8884 val_loss: 0.33815366278420467\n",
      "epoch: 1370\n",
      "train oa: 92.4 train loss 0.18392083568455134\n",
      " val oa: 0.8982 val_loss: 0.3118806501649958\n",
      "epoch: 1371\n",
      "train oa: 92.35 train loss 0.15469032079798567\n",
      " val oa: 0.8868 val_loss: 0.3712329387432365\n",
      "epoch: 1372\n",
      "train oa: 93.3 train loss 0.15453299101914672\n",
      " val oa: 0.8936 val_loss: 0.3572113970930799\n",
      "epoch: 1373\n",
      "train oa: 93.2 train loss 0.16772821016471337\n",
      " val oa: 0.8868 val_loss: 0.38333180726139593\n",
      "epoch: 1374\n",
      "global steps 44000 running loss: 0.029\n",
      "train oa: 92.05 train loss 0.18064882846884872\n",
      " val oa: 0.8956 val_loss: 0.33852256834362066\n",
      "epoch: 1375\n",
      "train oa: 92.9 train loss 0.16784988171400156\n",
      " val oa: 0.8842 val_loss: 0.3564527841745795\n",
      "epoch: 1376\n",
      "train oa: 91.5 train loss 0.19758731589184161\n",
      " val oa: 0.8928 val_loss: 0.3355343008125295\n",
      "epoch: 1377\n",
      "train oa: 92.75 train loss 0.17541926150723686\n",
      " val oa: 0.8932 val_loss: 0.31838766340777075\n",
      "epoch: 1378\n",
      "train oa: 92.95 train loss 0.16174473575598783\n",
      " val oa: 0.891 val_loss: 0.3353548759035836\n",
      "epoch: 1379\n",
      "train oa: 93.1 train loss 0.16056294215270625\n",
      " val oa: 0.8954 val_loss: 0.3112555202119138\n",
      "epoch: 1380\n",
      "train oa: 93.1 train loss 0.15765402910636586\n",
      " val oa: 0.898 val_loss: 0.3265791154020468\n",
      "epoch: 1381\n",
      "global steps 44200 running loss: 0.007\n",
      "train oa: 93.4 train loss 0.15455636039124587\n",
      " val oa: 0.8954 val_loss: 0.3395200548264673\n",
      "epoch: 1382\n",
      "train oa: 92.5 train loss 0.15377292682143548\n",
      " val oa: 0.9014 val_loss: 0.31833890754656063\n",
      "epoch: 1383\n",
      "train oa: 94.25 train loss 0.1388596342019841\n",
      " val oa: 0.8916 val_loss: 0.3561798778459829\n",
      "epoch: 1384\n",
      "train oa: 92.8 train loss 0.16721471948255529\n",
      " val oa: 0.8924 val_loss: 0.32704864573221737\n",
      "epoch: 1385\n",
      "train oa: 93.3 train loss 0.1512291105970247\n",
      " val oa: 0.897 val_loss: 0.3300094904521588\n",
      "epoch: 1386\n",
      "train oa: 92.8 train loss 0.16152619206993213\n",
      " val oa: 0.892 val_loss: 0.33242904198722534\n",
      "epoch: 1387\n",
      "global steps 44400 running loss: 0.012\n",
      "train oa: 92.4 train loss 0.16497497650356388\n",
      " val oa: 0.9018 val_loss: 0.3165945027127355\n",
      "epoch: 1388\n",
      "train oa: 92.75 train loss 0.15676419318981008\n",
      " val oa: 0.8926 val_loss: 0.3430042613853569\n",
      "epoch: 1389\n",
      "train oa: 91.95 train loss 0.17205016611569515\n",
      " val oa: 0.8978 val_loss: 0.32660406580258805\n",
      "epoch: 1390\n",
      "train oa: 94.65 train loss 0.14498449890350432\n",
      " val oa: 0.8946 val_loss: 0.3457369689051117\n",
      "epoch: 1391\n",
      "train oa: 93.2 train loss 0.1637675916867735\n",
      " val oa: 0.902 val_loss: 0.3067845439789362\n",
      "epoch: 1392\n",
      "train oa: 93.75 train loss 0.1562393350444554\n",
      " val oa: 0.8852 val_loss: 0.3786833473392106\n",
      "epoch: 1393\n",
      "global steps 44600 running loss: 0.018\n",
      "train oa: 93.65 train loss 0.1519200164574651\n",
      " val oa: 0.8958 val_loss: 0.341542206483421\n",
      "epoch: 1394\n",
      "train oa: 93.0 train loss 0.14500340856990263\n",
      " val oa: 0.8922 val_loss: 0.35268010995208376\n",
      "epoch: 1395\n",
      "train oa: 92.8 train loss 0.15381204290372538\n",
      " val oa: 0.9022 val_loss: 0.3279402209012963\n",
      "epoch: 1396\n",
      "train oa: 91.2 train loss 0.20977853779955705\n",
      " val oa: 0.8816 val_loss: 0.37095890266123055\n",
      "epoch: 1397\n",
      "train oa: 91.85 train loss 0.17870226148704757\n",
      " val oa: 0.9046 val_loss: 0.299004105142917\n",
      "epoch: 1398\n",
      "train oa: 93.35 train loss 0.1547769211323939\n",
      " val oa: 0.895 val_loss: 0.3465963429543472\n",
      "epoch: 1399\n",
      "global steps 44800 running loss: 0.024\n",
      "train oa: 92.85 train loss 0.15156154106988454\n",
      " val oa: 0.8932 val_loss: 0.3154921025364524\n",
      "epoch: 1400\n",
      "train oa: 92.45 train loss 0.16567599097999502\n",
      " val oa: 0.9 val_loss: 0.3206570815196883\n",
      "epoch: 1401\n",
      "train oa: 92.9 train loss 0.16454998608040647\n",
      " val oa: 0.8986 val_loss: 0.31702384585335874\n",
      "epoch: 1402\n",
      "train oa: 93.6 train loss 0.15666757861120098\n",
      " val oa: 0.9018 val_loss: 0.32232839185981743\n",
      "epoch: 1403\n",
      "train oa: 93.4 train loss 0.14580794340567022\n",
      " val oa: 0.9022 val_loss: 0.30954196757345737\n",
      "epoch: 1404\n",
      "train oa: 93.15 train loss 0.16160391213602737\n",
      " val oa: 0.892 val_loss: 0.3298817170428645\n",
      "epoch: 1405\n",
      "train oa: 92.35 train loss 0.18659424782702083\n",
      " val oa: 0.875 val_loss: 0.39528091504547347\n",
      "epoch: 1406\n",
      "global steps 45000 running loss: 0.008\n",
      "train oa: 92.65 train loss 0.1688316423536578\n",
      " val oa: 0.884 val_loss: 0.37264681867083677\n",
      "epoch: 1407\n",
      "train oa: 93.4 train loss 0.16147845226822752\n",
      " val oa: 0.8898 val_loss: 0.34321499149332674\n",
      "epoch: 1408\n",
      "train oa: 92.15 train loss 0.1703458034932246\n",
      " val oa: 0.9028 val_loss: 0.35087504619372617\n",
      "epoch: 1409\n",
      "train oa: 92.85 train loss 0.17530518577151485\n",
      " val oa: 0.9018 val_loss: 0.3204562660334452\n",
      "epoch: 1410\n",
      "train oa: 92.5 train loss 0.16498508375187224\n",
      " val oa: 0.8876 val_loss: 0.3509443703737983\n",
      "epoch: 1411\n",
      "train oa: 92.15 train loss 0.1793995309334382\n",
      " val oa: 0.8928 val_loss: 0.3308898394736275\n",
      "epoch: 1412\n",
      "global steps 45200 running loss: 0.014\n",
      "train oa: 92.5 train loss 0.16447114789495118\n",
      " val oa: 0.8976 val_loss: 0.31986838757250324\n",
      "epoch: 1413\n",
      "train oa: 93.6 train loss 0.14775740067459267\n",
      " val oa: 0.8894 val_loss: 0.35104996542015154\n",
      "epoch: 1414\n",
      "train oa: 94.15 train loss 0.1418814115053128\n",
      " val oa: 0.8904 val_loss: 0.3779785129106022\n",
      "epoch: 1415\n",
      "train oa: 92.55 train loss 0.1754742910020412\n",
      " val oa: 0.8938 val_loss: 0.3380206598946839\n",
      "epoch: 1416\n",
      "train oa: 92.4 train loss 0.1645711781988887\n",
      " val oa: 0.8976 val_loss: 0.3290652511841011\n",
      "epoch: 1417\n",
      "train oa: 93.1 train loss 0.15249624263405534\n",
      " val oa: 0.8962 val_loss: 0.36456291909270544\n",
      "epoch: 1418\n",
      "global steps 45400 running loss: 0.016\n",
      "train oa: 94.2 train loss 0.14095200193491642\n",
      " val oa: 0.878 val_loss: 0.3920835274884062\n",
      "epoch: 1419\n",
      "train oa: 93.45 train loss 0.15125898399764132\n",
      " val oa: 0.8912 val_loss: 0.34811991951176974\n",
      "epoch: 1420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 92.95 train loss 0.15165928484827446\n",
      " val oa: 0.8972 val_loss: 0.35010489690508184\n",
      "epoch: 1421\n",
      "train oa: 93.55 train loss 0.14928844641814756\n",
      " val oa: 0.8956 val_loss: 0.3552143646993955\n",
      "epoch: 1422\n",
      "train oa: 90.95 train loss 0.20197690866003898\n",
      " val oa: 0.8888 val_loss: 0.32132524402874796\n",
      "epoch: 1423\n",
      "train oa: 92.9 train loss 0.1627560392431372\n",
      " val oa: 0.8886 val_loss: 0.3327446990405959\n",
      "epoch: 1424\n",
      "global steps 45600 running loss: 0.024\n",
      "train oa: 93.4 train loss 0.14703856212196167\n",
      " val oa: 0.8918 val_loss: 0.35143462765700284\n",
      "epoch: 1425\n",
      "train oa: 93.4 train loss 0.13540351785510685\n",
      " val oa: 0.9024 val_loss: 0.33150118081524627\n",
      "epoch: 1426\n",
      "train oa: 93.75 train loss 0.15390813879824947\n",
      " val oa: 0.8972 val_loss: 0.3556160688012169\n",
      "epoch: 1427\n",
      "train oa: 93.4 train loss 0.14526036567419692\n",
      " val oa: 0.8926 val_loss: 0.36075249082102695\n",
      "epoch: 1428\n",
      "train oa: 91.4 train loss 0.1969071762050956\n",
      " val oa: 0.8904 val_loss: 0.3510472467778967\n",
      "epoch: 1429\n",
      "train oa: 92.9 train loss 0.16599845641849165\n",
      " val oa: 0.8914 val_loss: 0.3525570055703359\n",
      "epoch: 1430\n",
      "train oa: 93.0 train loss 0.16352736614062008\n",
      " val oa: 0.9004 val_loss: 0.3417468121209472\n",
      "epoch: 1431\n",
      "global steps 45800 running loss: 0.005\n",
      "train oa: 93.7 train loss 0.15187690219364106\n",
      " val oa: 0.8958 val_loss: 0.3378583818327023\n",
      "epoch: 1432\n",
      "train oa: 94.25 train loss 0.14153724627898664\n",
      " val oa: 0.8968 val_loss: 0.3505366457120221\n",
      "epoch: 1433\n",
      "train oa: 93.35 train loss 0.14715194397092143\n",
      " val oa: 0.899 val_loss: 0.3207665531419794\n",
      "epoch: 1434\n",
      "train oa: 93.85 train loss 0.14102194558724668\n",
      " val oa: 0.893 val_loss: 0.348515194917089\n",
      "epoch: 1435\n",
      "train oa: 92.65 train loss 0.1639096248740718\n",
      " val oa: 0.8792 val_loss: 0.37867279560855366\n",
      "epoch: 1436\n",
      "train oa: 92.65 train loss 0.17428533970693832\n",
      " val oa: 0.8906 val_loss: 0.3464419634894886\n",
      "epoch: 1437\n",
      "global steps 46000 running loss: 0.014\n",
      "train oa: 93.2 train loss 0.1555196703871634\n",
      " val oa: 0.89 val_loss: 0.3454471726009039\n",
      "epoch: 1438\n",
      "train oa: 93.6 train loss 0.14554164357874358\n",
      " val oa: 0.8958 val_loss: 0.3727327853545848\n",
      "epoch: 1439\n",
      "train oa: 93.05 train loss 0.15246124774723802\n",
      " val oa: 0.8916 val_loss: 0.33362090439952974\n",
      "epoch: 1440\n",
      "train oa: 92.65 train loss 0.17251010263972447\n",
      " val oa: 0.8996 val_loss: 0.3227914074372194\n",
      "epoch: 1441\n",
      "train oa: 92.5 train loss 0.17794237717481287\n",
      " val oa: 0.893 val_loss: 0.3203735278785012\n",
      "epoch: 1442\n",
      "train oa: 92.4 train loss 0.16887056575725057\n",
      " val oa: 0.8884 val_loss: 0.3432679396332847\n",
      "epoch: 1443\n",
      "global steps 46200 running loss: 0.019\n",
      "train oa: 92.7 train loss 0.16477693504440466\n",
      " val oa: 0.9008 val_loss: 0.3131974515188862\n",
      "epoch: 1444\n",
      "train oa: 92.8 train loss 0.1678160735856981\n",
      " val oa: 0.897 val_loss: 0.32242832806677996\n",
      "epoch: 1445\n",
      "train oa: 93.3 train loss 0.13719397195807437\n",
      " val oa: 0.8906 val_loss: 0.3912192965798816\n",
      "epoch: 1446\n",
      "train oa: 93.3 train loss 0.1469449923358648\n",
      " val oa: 0.8984 val_loss: 0.3528449470456971\n",
      "epoch: 1447\n",
      "train oa: 94.25 train loss 0.13933455002393685\n",
      " val oa: 0.897 val_loss: 0.34275119750511995\n",
      "epoch: 1448\n",
      "train oa: 93.0 train loss 0.15741482844079605\n",
      " val oa: 0.8914 val_loss: 0.39059001271262556\n",
      "epoch: 1449\n",
      "global steps 46400 running loss: 0.023\n",
      "train oa: 93.95 train loss 0.14465512910624173\n",
      " val oa: 0.8922 val_loss: 0.35006269733348544\n",
      "epoch: 1450\n",
      "train oa: 93.25 train loss 0.1624974111156438\n",
      " val oa: 0.8828 val_loss: 0.3660286674758668\n",
      "epoch: 1451\n",
      "train oa: 92.45 train loss 0.1693783638553732\n",
      " val oa: 0.8982 val_loss: 0.3164125505195341\n",
      "epoch: 1452\n",
      "train oa: 93.2 train loss 0.14819885851381046\n",
      " val oa: 0.8964 val_loss: 0.32492746702816006\n",
      "epoch: 1453\n",
      "train oa: 93.3 train loss 0.15022748244577996\n",
      " val oa: 0.9002 val_loss: 0.3214294313126332\n",
      "epoch: 1454\n",
      "train oa: 93.65 train loss 0.13665719387839384\n",
      " val oa: 0.895 val_loss: 0.357754378850703\n",
      "epoch: 1455\n",
      "train oa: 93.95 train loss 0.13922353290716719\n",
      " val oa: 0.8836 val_loss: 0.40127829403836324\n",
      "epoch: 1456\n",
      "global steps 46600 running loss: 0.006\n",
      "train oa: 93.15 train loss 0.16705739283953494\n",
      " val oa: 0.8992 val_loss: 0.32045395172048075\n",
      "epoch: 1457\n",
      "train oa: 92.65 train loss 0.15176712449037072\n",
      " val oa: 0.8942 val_loss: 0.34812761014848803\n",
      "epoch: 1458\n",
      "train oa: 92.95 train loss 0.15676332862050982\n",
      " val oa: 0.8992 val_loss: 0.33882551869996586\n",
      "epoch: 1459\n",
      "train oa: 93.35 train loss 0.13679833404210062\n",
      " val oa: 0.8988 val_loss: 0.34050670450097076\n",
      "epoch: 1460\n",
      "train oa: 93.55 train loss 0.1421509296398686\n",
      " val oa: 0.8896 val_loss: 0.3549047326395251\n",
      "epoch: 1461\n",
      "train oa: 93.5 train loss 0.1454577617735127\n",
      " val oa: 0.8928 val_loss: 0.3630362779307126\n",
      "epoch: 1462\n",
      "global steps 46800 running loss: 0.011\n",
      "train oa: 93.75 train loss 0.14603404235616638\n",
      " val oa: 0.8926 val_loss: 0.36911032860242593\n",
      "epoch: 1463\n",
      "train oa: 93.5 train loss 0.14710978014646464\n",
      " val oa: 0.901 val_loss: 0.32397118661346497\n",
      "epoch: 1464\n",
      "train oa: 92.6 train loss 0.17569756366515354\n",
      " val oa: 0.893 val_loss: 0.33311409927307606\n",
      "epoch: 1465\n",
      "train oa: 93.3 train loss 0.14957053014622368\n",
      " val oa: 0.8962 val_loss: 0.34163233533790277\n",
      "epoch: 1466\n",
      "train oa: 92.9 train loss 0.15473241898829093\n",
      " val oa: 0.905 val_loss: 0.32711739010925256\n",
      "epoch: 1466 best val accuracy: 0.905\n",
      "epoch: 1467\n",
      "train oa: 92.85 train loss 0.17214999202111836\n",
      " val oa: 0.8934 val_loss: 0.3420591100484634\n",
      "epoch: 1468\n",
      "global steps 47000 running loss: 0.020\n",
      "train oa: 92.65 train loss 0.16157686432600762\n",
      " val oa: 0.8982 val_loss: 0.3259097806174816\n",
      "epoch: 1469\n",
      "train oa: 92.4 train loss 0.16455594426090864\n",
      " val oa: 0.8824 val_loss: 0.40302457547446674\n",
      "epoch: 1470\n",
      "train oa: 93.05 train loss 0.15598993235929032\n",
      " val oa: 0.887 val_loss: 0.36930809804934356\n",
      "epoch: 1471\n",
      "train oa: 93.6 train loss 0.15368339774748382\n",
      " val oa: 0.8952 val_loss: 0.3484107622117056\n",
      "epoch: 1472\n",
      "train oa: 94.25 train loss 0.14701282009246652\n",
      " val oa: 0.9014 val_loss: 0.34172271088836625\n",
      "epoch: 1473\n",
      "train oa: 93.45 train loss 0.14437203135263543\n",
      " val oa: 0.8992 val_loss: 0.3338829083241129\n",
      "epoch: 1474\n",
      "global steps 47200 running loss: 0.023\n",
      "train oa: 93.6 train loss 0.14275709228144237\n",
      " val oa: 0.9018 val_loss: 0.3251997833577947\n",
      "epoch: 1475\n",
      "train oa: 92.85 train loss 0.16185370687430836\n",
      " val oa: 0.898 val_loss: 0.3691140208926948\n",
      "epoch: 1476\n",
      "train oa: 92.75 train loss 0.15527901575163797\n",
      " val oa: 0.8982 val_loss: 0.3601624941157488\n",
      "epoch: 1477\n",
      "train oa: 92.9 train loss 0.14994650429409057\n",
      " val oa: 0.8906 val_loss: 0.35867953087213295\n",
      "epoch: 1478\n",
      "train oa: 92.35 train loss 0.17241624512189244\n",
      " val oa: 0.8934 val_loss: 0.33131453530231725\n",
      "epoch: 1479\n",
      "train oa: 93.5 train loss 0.1477843411316154\n",
      " val oa: 0.8962 val_loss: 0.33367702165221413\n",
      "epoch: 1480\n",
      "train oa: 92.55 train loss 0.15256816978761456\n",
      " val oa: 0.8942 val_loss: 0.35596812454671195\n",
      "epoch: 1481\n",
      "global steps 47400 running loss: 0.006\n",
      "train oa: 93.75 train loss 0.14280280324459904\n",
      " val oa: 0.8918 val_loss: 0.372465351916316\n",
      "epoch: 1482\n",
      "train oa: 93.3 train loss 0.1531755950285417\n",
      " val oa: 0.9004 val_loss: 0.35061621522798286\n",
      "epoch: 1483\n",
      "train oa: 93.0 train loss 0.1642000595125245\n",
      " val oa: 0.8966 val_loss: 0.36293308842417177\n",
      "epoch: 1484\n",
      "train oa: 92.75 train loss 0.1737874104641974\n",
      " val oa: 0.8934 val_loss: 0.3221107805583507\n",
      "epoch: 1485\n",
      "train oa: 93.2 train loss 0.15809554270785156\n",
      " val oa: 0.9032 val_loss: 0.3196426492645818\n",
      "epoch: 1486\n",
      "train oa: 92.55 train loss 0.16369649378196863\n",
      " val oa: 0.8952 val_loss: 0.35593941980687327\n",
      "epoch: 1487\n",
      "global steps 47600 running loss: 0.012\n",
      "train oa: 93.1 train loss 0.1554538281183324\n",
      " val oa: 0.9024 val_loss: 0.33173430963236056\n",
      "epoch: 1488\n",
      "train oa: 92.85 train loss 0.16012944812287427\n",
      " val oa: 0.9016 val_loss: 0.34535932836990385\n",
      "epoch: 1489\n",
      "train oa: 94.3 train loss 0.13483751112210068\n",
      " val oa: 0.8996 val_loss: 0.3500569624913988\n",
      "epoch: 1490\n",
      "train oa: 92.8 train loss 0.1428449933301199\n",
      " val oa: 0.9032 val_loss: 0.33704652845577177\n",
      "epoch: 1491\n",
      "train oa: 93.6 train loss 0.1440574758547353\n",
      " val oa: 0.8896 val_loss: 0.363132343699942\n",
      "epoch: 1492\n",
      "train oa: 92.6 train loss 0.15882281852523647\n",
      " val oa: 0.8958 val_loss: 0.36085516356520114\n",
      "epoch: 1493\n",
      "global steps 47800 running loss: 0.018\n",
      "train oa: 93.25 train loss 0.14551999229400983\n",
      " val oa: 0.8938 val_loss: 0.3617914129092847\n",
      "epoch: 1494\n",
      "train oa: 93.65 train loss 0.1458042540520816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.8978 val_loss: 0.36001360793785736\n",
      "epoch: 1495\n",
      "train oa: 93.65 train loss 0.15108339265763568\n",
      " val oa: 0.8906 val_loss: 0.36025599272119774\n",
      "epoch: 1496\n",
      "train oa: 92.95 train loss 0.15477569428911686\n",
      " val oa: 0.8926 val_loss: 0.3514913821582562\n",
      "epoch: 1497\n",
      "train oa: 92.95 train loss 0.15720983135076688\n",
      " val oa: 0.8916 val_loss: 0.34176276502072206\n",
      "epoch: 1498\n",
      "train oa: 93.3 train loss 0.15669031444661033\n",
      " val oa: 0.8976 val_loss: 0.33560162316293696\n",
      "epoch: 1499\n",
      "global steps 48000 running loss: 0.024\n",
      "train oa: 93.7 train loss 0.14707542609109822\n",
      " val oa: 0.8958 val_loss: 0.3633219209751209\n",
      "epoch: 1500\n",
      "train oa: 94.75 train loss 0.1203591302285788\n",
      " val oa: 0.8974 val_loss: 0.3697818354642449\n",
      "epoch: 1501\n",
      "train oa: 94.95 train loss 0.1261157450575098\n",
      " val oa: 0.893 val_loss: 0.4142595623738653\n",
      "epoch: 1502\n",
      "train oa: 92.75 train loss 0.16560421624059193\n",
      " val oa: 0.901 val_loss: 0.33497584734787417\n",
      "epoch: 1503\n",
      "train oa: 93.3 train loss 0.1630428300524658\n",
      " val oa: 0.8978 val_loss: 0.34352829206811625\n",
      "epoch: 1504\n",
      "train oa: 93.2 train loss 0.15627417436034902\n",
      " val oa: 0.8964 val_loss: 0.3605123867982975\n",
      "epoch: 1505\n",
      "train oa: 93.25 train loss 0.1439974002890404\n",
      " val oa: 0.8966 val_loss: 0.33199782653723037\n",
      "epoch: 1506\n",
      "global steps 48200 running loss: 0.005\n",
      "train oa: 93.05 train loss 0.15344716120384344\n",
      " val oa: 0.8966 val_loss: 0.3564167471223689\n",
      "epoch: 1507\n",
      "train oa: 92.8 train loss 0.15283600659797197\n",
      " val oa: 0.8978 val_loss: 0.3477662040129259\n",
      "epoch: 1508\n",
      "train oa: 93.4 train loss 0.15113374073686417\n",
      " val oa: 0.901 val_loss: 0.33050609542374026\n",
      "epoch: 1509\n",
      "train oa: 93.85 train loss 0.14453230757851884\n",
      " val oa: 0.8972 val_loss: 0.360405545898414\n",
      "epoch: 1510\n",
      "train oa: 92.75 train loss 0.17589405901389593\n",
      " val oa: 0.8994 val_loss: 0.32572815636771013\n",
      "epoch: 1511\n",
      "train oa: 93.55 train loss 0.15258427455766818\n",
      " val oa: 0.9032 val_loss: 0.3209417391506729\n",
      "epoch: 1512\n",
      "global steps 48400 running loss: 0.009\n",
      "train oa: 93.95 train loss 0.13263812111877113\n",
      " val oa: 0.8936 val_loss: 0.33147265819923266\n",
      "epoch: 1513\n",
      "train oa: 93.5 train loss 0.14743563621858957\n",
      " val oa: 0.897 val_loss: 0.3568167733180662\n",
      "epoch: 1514\n",
      "train oa: 93.7 train loss 0.14322608598846254\n",
      " val oa: 0.8954 val_loss: 0.3714765306018834\n",
      "epoch: 1515\n",
      "train oa: 92.85 train loss 0.16138260389584888\n",
      " val oa: 0.8966 val_loss: 0.36772209569953423\n",
      "epoch: 1516\n",
      "train oa: 93.3 train loss 0.14809467040190927\n",
      " val oa: 0.9006 val_loss: 0.3346466332173739\n",
      "epoch: 1517\n",
      "train oa: 93.45 train loss 0.15577956480547295\n",
      " val oa: 0.9004 val_loss: 0.34302924504842036\n",
      "epoch: 1518\n",
      "global steps 48600 running loss: 0.019\n",
      "train oa: 93.35 train loss 0.15430680794787432\n",
      " val oa: 0.903 val_loss: 0.32141756843284447\n",
      "epoch: 1519\n",
      "train oa: 94.0 train loss 0.14198248624889084\n",
      " val oa: 0.8984 val_loss: 0.36673423234701785\n",
      "epoch: 1520\n",
      "train oa: 94.5 train loss 0.13056839187390057\n",
      " val oa: 0.8916 val_loss: 0.3859754397325859\n",
      "epoch: 1521\n",
      "train oa: 92.4 train loss 0.16131834840981765\n",
      " val oa: 0.9016 val_loss: 0.34119706217091555\n",
      "epoch: 1522\n",
      "train oa: 93.45 train loss 0.15600215088345215\n",
      " val oa: 0.8944 val_loss: 0.36545750185342507\n",
      "epoch: 1523\n",
      "train oa: 93.5 train loss 0.14632105692512626\n",
      " val oa: 0.8964 val_loss: 0.34019329109614077\n",
      "epoch: 1524\n",
      "global steps 48800 running loss: 0.025\n",
      "train oa: 93.2 train loss 0.1585065797493498\n",
      " val oa: 0.8932 val_loss: 0.3670084773834267\n",
      "epoch: 1525\n",
      "train oa: 93.7 train loss 0.14525089548480552\n",
      " val oa: 0.9004 val_loss: 0.3245242138661336\n",
      "epoch: 1526\n",
      "train oa: 93.6 train loss 0.14962318561284524\n",
      " val oa: 0.8874 val_loss: 0.3766763860343788\n",
      "epoch: 1527\n",
      "train oa: 93.65 train loss 0.13964279258529244\n",
      " val oa: 0.895 val_loss: 0.36661050837307174\n",
      "epoch: 1528\n",
      "train oa: 93.05 train loss 0.1587261194545927\n",
      " val oa: 0.9002 val_loss: 0.3525970563517237\n",
      "epoch: 1529\n",
      "train oa: 94.15 train loss 0.14969999900249353\n",
      " val oa: 0.8906 val_loss: 0.33253332866319024\n",
      "epoch: 1530\n",
      "train oa: 93.3 train loss 0.14795243565941477\n",
      " val oa: 0.8966 val_loss: 0.34259702543569703\n",
      "epoch: 1531\n",
      "global steps 49000 running loss: 0.007\n",
      "train oa: 91.3 train loss 0.18702939852103406\n",
      " val oa: 0.8942 val_loss: 0.3124830915703547\n",
      "epoch: 1532\n",
      "train oa: 92.75 train loss 0.15888230069265916\n",
      " val oa: 0.8928 val_loss: 0.3472561479028491\n",
      "epoch: 1533\n",
      "train oa: 93.05 train loss 0.15344419816026886\n",
      " val oa: 0.8962 val_loss: 0.34064817593029967\n",
      "epoch: 1534\n",
      "train oa: 91.85 train loss 0.17182159374472689\n",
      " val oa: 0.8992 val_loss: 0.3182591348331683\n",
      "epoch: 1535\n",
      "train oa: 92.75 train loss 0.15541498100786477\n",
      " val oa: 0.894 val_loss: 0.3297271728738906\n",
      "epoch: 1536\n",
      "train oa: 94.3 train loss 0.13133973967322043\n",
      " val oa: 0.9 val_loss: 0.33424718247700513\n",
      "epoch: 1537\n",
      "global steps 49200 running loss: 0.009\n",
      "train oa: 94.65 train loss 0.12949640158377174\n",
      " val oa: 0.8958 val_loss: 0.3648805661843016\n",
      "epoch: 1538\n",
      "train oa: 93.65 train loss 0.14054112933344418\n",
      " val oa: 0.8982 val_loss: 0.35688823076059184\n",
      "epoch: 1539\n",
      "train oa: 94.0 train loss 0.1325076722685905\n",
      " val oa: 0.902 val_loss: 0.34166783200170986\n",
      "epoch: 1540\n",
      "train oa: 94.1 train loss 0.1416745497992152\n",
      " val oa: 0.9002 val_loss: 0.3392719259073249\n",
      "epoch: 1541\n",
      "train oa: 94.25 train loss 0.15181003222270972\n",
      " val oa: 0.9044 val_loss: 0.3294868860691441\n",
      "epoch: 1542\n",
      "train oa: 93.8 train loss 0.14898062795491387\n",
      " val oa: 0.89 val_loss: 0.3690846029125519\n",
      "epoch: 1543\n",
      "global steps 49400 running loss: 0.017\n",
      "train oa: 94.1 train loss 0.13689979491661897\n",
      " val oa: 0.9014 val_loss: 0.33744347658588797\n",
      "epoch: 1544\n",
      "train oa: 93.0 train loss 0.14748072188235692\n",
      " val oa: 0.8894 val_loss: 0.4038209369511126\n",
      "epoch: 1545\n",
      "train oa: 93.55 train loss 0.1461492565072735\n",
      " val oa: 0.9004 val_loss: 0.34100075922918366\n",
      "epoch: 1546\n",
      "train oa: 93.9 train loss 0.14157730381661526\n",
      " val oa: 0.9066 val_loss: 0.32015895786734905\n",
      "epoch: 1546 best val accuracy: 0.9066\n",
      "epoch: 1547\n",
      "train oa: 93.45 train loss 0.14503086450143649\n",
      " val oa: 0.8974 val_loss: 0.31572849674316017\n",
      "epoch: 1548\n",
      "train oa: 93.7 train loss 0.14545559691178886\n",
      " val oa: 0.8984 val_loss: 0.37606989077532466\n",
      "epoch: 1549\n",
      "global steps 49600 running loss: 0.024\n",
      "train oa: 93.6 train loss 0.15169210882886244\n",
      " val oa: 0.8922 val_loss: 0.3328787432438762\n",
      "epoch: 1550\n",
      "train oa: 93.0 train loss 0.15763903691610937\n",
      " val oa: 0.895 val_loss: 0.3510110162913117\n",
      "epoch: 1551\n",
      "train oa: 93.05 train loss 0.14358629612423648\n",
      " val oa: 0.883 val_loss: 0.37282685943578964\n",
      "epoch: 1552\n",
      "train oa: 94.05 train loss 0.13659800100039887\n",
      " val oa: 0.8984 val_loss: 0.33880512538505975\n",
      "epoch: 1553\n",
      "train oa: 94.05 train loss 0.13202221745838297\n",
      " val oa: 0.896 val_loss: 0.34070405796110775\n",
      "epoch: 1554\n",
      "train oa: 92.05 train loss 0.17134564846531453\n",
      " val oa: 0.9014 val_loss: 0.3265714391134471\n",
      "epoch: 1555\n",
      "train oa: 93.45 train loss 0.1399698483213604\n",
      " val oa: 0.8966 val_loss: 0.33100950255183503\n",
      "epoch: 1556\n",
      "global steps 49800 running loss: 0.005\n",
      "train oa: 94.35 train loss 0.13305475568169203\n",
      " val oa: 0.8918 val_loss: 0.3481430310853103\n",
      "epoch: 1557\n",
      "train oa: 94.2 train loss 0.1279665418747587\n",
      " val oa: 0.8922 val_loss: 0.37846759869176033\n",
      "epoch: 1558\n",
      "train oa: 94.05 train loss 0.1285206295845388\n",
      " val oa: 0.896 val_loss: 0.35640040531440215\n",
      "epoch: 1559\n",
      "train oa: 93.45 train loss 0.13299504340488122\n",
      " val oa: 0.8964 val_loss: 0.3368323407510413\n",
      "epoch: 1560\n",
      "train oa: 93.55 train loss 0.14093397740264016\n",
      " val oa: 0.895 val_loss: 0.37871264221757717\n",
      "epoch: 1561\n",
      "train oa: 94.05 train loss 0.14886322168478933\n",
      " val oa: 0.8902 val_loss: 0.36853808843313884\n",
      "epoch: 1562\n",
      "global steps 50000 running loss: 0.012\n",
      "train oa: 93.9 train loss 0.14543732649461047\n",
      " val oa: 0.898 val_loss: 0.3527174314303082\n",
      "epoch: 1563\n",
      "train oa: 92.85 train loss 0.16751687283394898\n",
      " val oa: 0.897 val_loss: 0.3328075484650965\n",
      "epoch: 1564\n",
      "train oa: 93.95 train loss 0.1503832142251612\n",
      " val oa: 0.8992 val_loss: 0.32658881726681355\n",
      "epoch: 1565\n",
      "train oa: 93.45 train loss 0.1397059653907225\n",
      " val oa: 0.891 val_loss: 0.36377204877003383\n",
      "epoch: 1566\n",
      "train oa: 93.65 train loss 0.1552731726700281\n",
      " val oa: 0.897 val_loss: 0.3469927549425538\n",
      "epoch: 1567\n",
      "train oa: 93.75 train loss 0.15461444196840918\n",
      " val oa: 0.9008 val_loss: 0.3238089003208256\n",
      "epoch: 1568\n",
      "global steps 50200 running loss: 0.018\n",
      "train oa: 93.25 train loss 0.15323298482861966\n",
      " val oa: 0.9024 val_loss: 0.3197626604371729\n",
      "epoch: 1569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 93.4 train loss 0.14956936331231782\n",
      " val oa: 0.9016 val_loss: 0.3328313531505252\n",
      "epoch: 1570\n",
      "train oa: 93.55 train loss 0.1363584955772882\n",
      " val oa: 0.8952 val_loss: 0.35209010234683785\n",
      "epoch: 1571\n",
      "train oa: 94.25 train loss 0.12471941199052798\n",
      " val oa: 0.8944 val_loss: 0.3570582458943092\n",
      "epoch: 1572\n",
      "train oa: 93.6 train loss 0.14530543116412237\n",
      " val oa: 0.8944 val_loss: 0.3838726803534863\n",
      "epoch: 1573\n",
      "train oa: 92.85 train loss 0.1641074317436607\n",
      " val oa: 0.8902 val_loss: 0.3522765358198194\n",
      "epoch: 1574\n",
      "global steps 50400 running loss: 0.024\n",
      "train oa: 93.05 train loss 0.15302858361929308\n",
      " val oa: 0.8962 val_loss: 0.341100061799073\n",
      "epoch: 1575\n",
      "train oa: 93.95 train loss 0.13953218324233194\n",
      " val oa: 0.8934 val_loss: 0.3587927417620176\n",
      "epoch: 1576\n",
      "train oa: 93.75 train loss 0.15931258332249706\n",
      " val oa: 0.904 val_loss: 0.33342060762777653\n",
      "epoch: 1577\n",
      "train oa: 94.6 train loss 0.12571506290491052\n",
      " val oa: 0.8964 val_loss: 0.3555260121727643\n",
      "epoch: 1578\n",
      "train oa: 94.75 train loss 0.12914434931570443\n",
      " val oa: 0.8878 val_loss: 0.36298846714992183\n",
      "epoch: 1579\n",
      "train oa: 93.4 train loss 0.1424144799394254\n",
      " val oa: 0.8968 val_loss: 0.360426017372934\n",
      "epoch: 1580\n",
      "train oa: 94.2 train loss 0.13058971942140837\n",
      " val oa: 0.9002 val_loss: 0.338961359873144\n",
      "epoch: 1581\n",
      "global steps 50600 running loss: 0.005\n",
      "train oa: 93.25 train loss 0.15527458734424568\n",
      " val oa: 0.903 val_loss: 0.3316671698363488\n",
      "epoch: 1582\n",
      "train oa: 93.65 train loss 0.1406044721043573\n",
      " val oa: 0.897 val_loss: 0.36901886763217845\n",
      "epoch: 1583\n",
      "train oa: 93.9 train loss 0.14484955599114047\n",
      " val oa: 0.8988 val_loss: 0.3385749010904984\n",
      "epoch: 1584\n",
      "train oa: 93.15 train loss 0.15607746789298832\n",
      " val oa: 0.8966 val_loss: 0.35406026082534375\n",
      "epoch: 1585\n",
      "train oa: 94.55 train loss 0.13608325525098902\n",
      " val oa: 0.8942 val_loss: 0.3732228737796104\n",
      "epoch: 1586\n",
      "train oa: 94.25 train loss 0.12560843234933697\n",
      " val oa: 0.89 val_loss: 0.3567514813470421\n",
      "epoch: 1587\n",
      "global steps 50800 running loss: 0.009\n",
      "train oa: 94.45 train loss 0.1233970453185962\n",
      " val oa: 0.8858 val_loss: 0.414788413346256\n",
      "epoch: 1588\n",
      "train oa: 93.45 train loss 0.14787627234940004\n",
      " val oa: 0.8978 val_loss: 0.349735788666823\n",
      "epoch: 1589\n",
      "train oa: 93.2 train loss 0.1564812048929403\n",
      " val oa: 0.9 val_loss: 0.3599915372757806\n",
      "epoch: 1590\n",
      "train oa: 93.9 train loss 0.15163041055549614\n",
      " val oa: 0.898 val_loss: 0.3360981351845762\n",
      "epoch: 1591\n",
      "train oa: 93.1 train loss 0.15141722497027654\n",
      " val oa: 0.8966 val_loss: 0.32528400919410494\n",
      "epoch: 1592\n",
      "train oa: 93.35 train loss 0.14521138997294328\n",
      " val oa: 0.8924 val_loss: 0.3438748506247588\n",
      "epoch: 1593\n",
      "global steps 51000 running loss: 0.017\n",
      "train oa: 93.95 train loss 0.1409061643527894\n",
      " val oa: 0.898 val_loss: 0.3285821303798498\n",
      "epoch: 1594\n",
      "train oa: 93.9 train loss 0.1392341537643534\n",
      " val oa: 0.8876 val_loss: 0.3863517777187443\n",
      "epoch: 1595\n",
      "train oa: 93.15 train loss 0.14995465628695115\n",
      " val oa: 0.8962 val_loss: 0.35320299601246663\n",
      "epoch: 1596\n",
      "train oa: 92.6 train loss 0.16183766317439796\n",
      " val oa: 0.8978 val_loss: 0.3086172748252836\n",
      "epoch: 1597\n",
      "train oa: 93.9 train loss 0.13458693307865396\n",
      " val oa: 0.9024 val_loss: 0.3189502247054362\n",
      "epoch: 1598\n",
      "train oa: 93.8 train loss 0.15618789243147685\n",
      " val oa: 0.9066 val_loss: 0.33080773134928254\n",
      "epoch: 1599\n",
      "global steps 51200 running loss: 0.021\n",
      "train oa: 93.95 train loss 0.13100069261624667\n",
      " val oa: 0.8964 val_loss: 0.3235520939563886\n",
      "epoch: 1600\n",
      "train oa: 93.8 train loss 0.1388403434373331\n",
      " val oa: 0.898 val_loss: 0.3382059059772067\n",
      "epoch: 1601\n",
      "train oa: 94.45 train loss 0.1468595077653152\n",
      " val oa: 0.893 val_loss: 0.335692778169208\n",
      "epoch: 1602\n",
      "train oa: 94.45 train loss 0.14057215694048716\n",
      " val oa: 0.8986 val_loss: 0.32156523828866845\n",
      "epoch: 1603\n",
      "train oa: 94.4 train loss 0.13732286399900556\n",
      " val oa: 0.8992 val_loss: 0.3218658186699032\n",
      "epoch: 1604\n",
      "train oa: 93.8 train loss 0.14225275570002466\n",
      " val oa: 0.8964 val_loss: 0.33228471281104927\n",
      "epoch: 1605\n",
      "train oa: 94.1 train loss 0.13017115258807307\n",
      " val oa: 0.8968 val_loss: 0.3463041949816562\n",
      "epoch: 1606\n",
      "global steps 51400 running loss: 0.005\n",
      "train oa: 93.7 train loss 0.13073850742963156\n",
      " val oa: 0.8918 val_loss: 0.3666245472035139\n",
      "epoch: 1607\n",
      "train oa: 93.95 train loss 0.13017793346315287\n",
      " val oa: 0.9034 val_loss: 0.3457926215974225\n",
      "epoch: 1608\n",
      "train oa: 93.65 train loss 0.14023155155679162\n",
      " val oa: 0.8932 val_loss: 0.3629096958278809\n",
      "epoch: 1609\n",
      "train oa: 94.35 train loss 0.13174593236254542\n",
      " val oa: 0.8984 val_loss: 0.33447125150964385\n",
      "epoch: 1610\n",
      "train oa: 94.1 train loss 0.13010349270939828\n",
      " val oa: 0.8974 val_loss: 0.38354105230019686\n",
      "epoch: 1611\n",
      "train oa: 94.35 train loss 0.1323373854793299\n",
      " val oa: 0.897 val_loss: 0.3367223790000397\n",
      "epoch: 1612\n",
      "global steps 51600 running loss: 0.011\n",
      "train oa: 93.45 train loss 0.13202548106555803\n",
      " val oa: 0.8982 val_loss: 0.3467673174408758\n",
      "epoch: 1613\n",
      "train oa: 93.55 train loss 0.13720784762316093\n",
      " val oa: 0.8972 val_loss: 0.33839836546631985\n",
      "epoch: 1614\n",
      "train oa: 93.85 train loss 0.14391517123175268\n",
      " val oa: 0.8958 val_loss: 0.35028599625283446\n",
      "epoch: 1615\n",
      "train oa: 93.7 train loss 0.14599232540920695\n",
      " val oa: 0.8994 val_loss: 0.365234641859843\n",
      "epoch: 1616\n",
      "train oa: 93.95 train loss 0.13674964569837456\n",
      " val oa: 0.906 val_loss: 0.32730186105390113\n",
      "epoch: 1617\n",
      "train oa: 94.65 train loss 0.12656861365937247\n",
      " val oa: 0.8996 val_loss: 0.35633969983487895\n",
      "epoch: 1618\n",
      "global steps 51800 running loss: 0.015\n",
      "train oa: 94.15 train loss 0.1269126420689801\n",
      " val oa: 0.8964 val_loss: 0.3510625989945144\n",
      "epoch: 1619\n",
      "train oa: 94.6 train loss 0.12471526152771585\n",
      " val oa: 0.8976 val_loss: 0.353369766492392\n",
      "epoch: 1620\n",
      "train oa: 94.1 train loss 0.1278930653957192\n",
      " val oa: 0.8946 val_loss: 0.36309733402700756\n",
      "epoch: 1621\n",
      "train oa: 93.7 train loss 0.14151514498163503\n",
      " val oa: 0.8908 val_loss: 0.3776785412042224\n",
      "epoch: 1622\n",
      "train oa: 94.2 train loss 0.13785500042025853\n",
      " val oa: 0.8974 val_loss: 0.3670988701605229\n",
      "epoch: 1623\n",
      "train oa: 94.5 train loss 0.1275858492337742\n",
      " val oa: 0.908 val_loss: 0.3294911033283301\n",
      "epoch: 1623 best val accuracy: 0.908\n",
      "epoch: 1624\n",
      "global steps 52000 running loss: 0.022\n",
      "train oa: 94.5 train loss 0.13722962046882145\n",
      " val oa: 0.8962 val_loss: 0.35508927000543394\n",
      "epoch: 1625\n",
      "train oa: 93.7 train loss 0.1414379627660419\n",
      " val oa: 0.8922 val_loss: 0.39194031046454625\n",
      "epoch: 1626\n",
      "train oa: 94.3 train loss 0.13359277131006908\n",
      " val oa: 0.9032 val_loss: 0.33543157131483964\n",
      "epoch: 1627\n",
      "train oa: 93.75 train loss 0.13812723103699162\n",
      " val oa: 0.9036 val_loss: 0.3394561710365767\n",
      "epoch: 1628\n",
      "train oa: 93.9 train loss 0.13688278285122785\n",
      " val oa: 0.8928 val_loss: 0.3677704457435668\n",
      "epoch: 1629\n",
      "train oa: 94.45 train loss 0.12641096702974544\n",
      " val oa: 0.8966 val_loss: 0.32822481969777173\n",
      "epoch: 1630\n",
      "train oa: 94.2 train loss 0.13154397152448802\n",
      " val oa: 0.9008 val_loss: 0.3431583642825071\n",
      "epoch: 1631\n",
      "global steps 52200 running loss: 0.006\n",
      "train oa: 94.4 train loss 0.1369643664254423\n",
      " val oa: 0.8918 val_loss: 0.39543280361597827\n",
      "epoch: 1632\n",
      "train oa: 93.75 train loss 0.14517959172721387\n",
      " val oa: 0.8996 val_loss: 0.3479172737151715\n",
      "epoch: 1633\n",
      "train oa: 93.75 train loss 0.13586495333483595\n",
      " val oa: 0.8958 val_loss: 0.3528649161272615\n",
      "epoch: 1634\n",
      "train oa: 93.7 train loss 0.1320401910994818\n",
      " val oa: 0.8974 val_loss: 0.3532076318702388\n",
      "epoch: 1635\n",
      "train oa: 93.6 train loss 0.14004302698269938\n",
      " val oa: 0.8976 val_loss: 0.3268119308565307\n",
      "epoch: 1636\n",
      "train oa: 94.25 train loss 0.13431083583787984\n",
      " val oa: 0.8916 val_loss: 0.34070508397510996\n",
      "epoch: 1637\n",
      "global steps 52400 running loss: 0.010\n",
      "train oa: 94.5 train loss 0.13602222190431204\n",
      " val oa: 0.9026 val_loss: 0.35155524204652866\n",
      "epoch: 1638\n",
      "train oa: 93.5 train loss 0.1473684474997507\n",
      " val oa: 0.8962 val_loss: 0.32137615174026085\n",
      "epoch: 1639\n",
      "train oa: 94.9 train loss 0.13282675580034742\n",
      " val oa: 0.89 val_loss: 0.35754929753192327\n",
      "epoch: 1640\n",
      "train oa: 93.9 train loss 0.13761160220477175\n",
      " val oa: 0.8922 val_loss: 0.3583840844618233\n",
      "epoch: 1641\n",
      "train oa: 94.45 train loss 0.13269113575192382\n",
      " val oa: 0.8942 val_loss: 0.34652889930189174\n",
      "epoch: 1642\n",
      "train oa: 95.4 train loss 0.11566327726000364\n",
      " val oa: 0.8918 val_loss: 0.3671849117006167\n",
      "epoch: 1643\n",
      "global steps 52600 running loss: 0.018\n",
      "train oa: 93.95 train loss 0.14635844134710393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.887 val_loss: 0.3553445055163487\n",
      "epoch: 1644\n",
      "train oa: 94.65 train loss 0.12497109608618769\n",
      " val oa: 0.8946 val_loss: 0.3479262400218842\n",
      "epoch: 1645\n",
      "train oa: 93.9 train loss 0.1434460244614183\n",
      " val oa: 0.8862 val_loss: 0.3842459593419109\n",
      "epoch: 1646\n",
      "train oa: 93.6 train loss 0.1426915272281732\n",
      " val oa: 0.8902 val_loss: 0.37349154172042015\n",
      "epoch: 1647\n",
      "train oa: 94.55 train loss 0.1285100419174796\n",
      " val oa: 0.895 val_loss: 0.3570088791798599\n",
      "epoch: 1648\n",
      "train oa: 93.55 train loss 0.14881122275931627\n",
      " val oa: 0.9 val_loss: 0.3546733995122475\n",
      "epoch: 1649\n",
      "global steps 52800 running loss: 0.020\n",
      "train oa: 94.7 train loss 0.12414749767269548\n",
      " val oa: 0.8968 val_loss: 0.3443338425532585\n",
      "epoch: 1650\n",
      "train oa: 93.55 train loss 0.1483681650592202\n",
      " val oa: 0.8942 val_loss: 0.3598674462932869\n",
      "epoch: 1651\n",
      "train oa: 93.1 train loss 0.14705924818205174\n",
      " val oa: 0.9014 val_loss: 0.3346796178205291\n",
      "epoch: 1652\n",
      "train oa: 93.4 train loss 0.16660560583006923\n",
      " val oa: 0.8966 val_loss: 0.3311513229569612\n",
      "epoch: 1653\n",
      "train oa: 94.4 train loss 0.14073029047298352\n",
      " val oa: 0.8966 val_loss: 0.3393462328202691\n",
      "epoch: 1654\n",
      "train oa: 94.55 train loss 0.13682867667478849\n",
      " val oa: 0.886 val_loss: 0.3482910799093154\n",
      "epoch: 1655\n",
      "train oa: 94.25 train loss 0.13684391100996465\n",
      " val oa: 0.8922 val_loss: 0.3542527561643047\n",
      "epoch: 1656\n",
      "global steps 53000 running loss: 0.006\n",
      "train oa: 94.2 train loss 0.1273661214485482\n",
      " val oa: 0.8996 val_loss: 0.35270575263980897\n",
      "epoch: 1657\n",
      "train oa: 94.25 train loss 0.1253916722729064\n",
      " val oa: 0.8928 val_loss: 0.3528904828308429\n",
      "epoch: 1658\n",
      "train oa: 94.8 train loss 0.12570933185209154\n",
      " val oa: 0.8976 val_loss: 0.37410037213530184\n",
      "epoch: 1659\n",
      "train oa: 94.5 train loss 0.1321603926649953\n",
      " val oa: 0.9028 val_loss: 0.3341062792953126\n",
      "epoch: 1660\n",
      "train oa: 94.0 train loss 0.12459804921817745\n",
      " val oa: 0.8942 val_loss: 0.35036763393383613\n",
      "epoch: 1661\n",
      "train oa: 94.65 train loss 0.12973877637350797\n",
      " val oa: 0.8992 val_loss: 0.3672663206026003\n",
      "epoch: 1662\n",
      "global steps 53200 running loss: 0.012\n",
      "train oa: 93.6 train loss 0.14363331223746273\n",
      " val oa: 0.903 val_loss: 0.33319021042846503\n",
      "epoch: 1663\n",
      "train oa: 93.65 train loss 0.14788734620818128\n",
      " val oa: 0.8858 val_loss: 0.3719912423989551\n",
      "epoch: 1664\n",
      "train oa: 92.5 train loss 0.14992314784522456\n",
      " val oa: 0.897 val_loss: 0.33028659610633976\n",
      "epoch: 1665\n",
      "train oa: 94.4 train loss 0.12560760902469306\n",
      " val oa: 0.906 val_loss: 0.3358426438596712\n",
      "epoch: 1666\n",
      "train oa: 93.95 train loss 0.12340154594417492\n",
      " val oa: 0.9 val_loss: 0.3276868885582737\n",
      "epoch: 1667\n",
      "train oa: 94.8 train loss 0.12970661466131345\n",
      " val oa: 0.9 val_loss: 0.33490539439908273\n",
      "epoch: 1668\n",
      "global steps 53400 running loss: 0.016\n",
      "train oa: 94.25 train loss 0.1341488339725424\n",
      " val oa: 0.8996 val_loss: 0.3542729678125274\n",
      "epoch: 1669\n",
      "train oa: 94.8 train loss 0.13078426268985982\n",
      " val oa: 0.8954 val_loss: 0.33922051168037587\n",
      "epoch: 1670\n",
      "train oa: 94.65 train loss 0.12438080525375797\n",
      " val oa: 0.8978 val_loss: 0.34344953202786727\n",
      "epoch: 1671\n",
      "train oa: 93.2 train loss 0.14809732089146094\n",
      " val oa: 0.8888 val_loss: 0.3762908089932708\n",
      "epoch: 1672\n",
      "train oa: 94.25 train loss 0.1297044942912232\n",
      " val oa: 0.8982 val_loss: 0.35167835518812257\n",
      "epoch: 1673\n",
      "train oa: 94.65 train loss 0.12531059564037808\n",
      " val oa: 0.8962 val_loss: 0.3525215524095487\n",
      "epoch: 1674\n",
      "global steps 53600 running loss: 0.022\n",
      "train oa: 93.5 train loss 0.1401236923701726\n",
      " val oa: 0.8992 val_loss: 0.34944740265781177\n",
      "epoch: 1675\n",
      "train oa: 94.9 train loss 0.11673752857223271\n",
      " val oa: 0.9002 val_loss: 0.32852370209812765\n",
      "epoch: 1676\n",
      "train oa: 94.65 train loss 0.1391845031603337\n",
      " val oa: 0.8976 val_loss: 0.35220982097945297\n",
      "epoch: 1677\n",
      "train oa: 94.7 train loss 0.12068100507985771\n",
      " val oa: 0.9028 val_loss: 0.3418213605170827\n",
      "epoch: 1678\n",
      "train oa: 94.45 train loss 0.13341314509390514\n",
      " val oa: 0.8984 val_loss: 0.35170169150801167\n",
      "epoch: 1679\n",
      "train oa: 92.65 train loss 0.174266307870623\n",
      " val oa: 0.8878 val_loss: 0.32360743832270167\n",
      "epoch: 1680\n",
      "train oa: 93.15 train loss 0.1564352979869227\n",
      " val oa: 0.9014 val_loss: 0.3704239035063044\n",
      "epoch: 1681\n",
      "global steps 53800 running loss: 0.006\n",
      "train oa: 94.5 train loss 0.1290619900800787\n",
      " val oa: 0.8952 val_loss: 0.38345564903732215\n",
      "epoch: 1682\n",
      "train oa: 94.35 train loss 0.1360823573221097\n",
      " val oa: 0.8998 val_loss: 0.32521844346913387\n",
      "epoch: 1683\n",
      "train oa: 94.55 train loss 0.12947559315832272\n",
      " val oa: 0.8994 val_loss: 0.34095608619775397\n",
      "epoch: 1684\n",
      "train oa: 94.95 train loss 0.12492726043457572\n",
      " val oa: 0.8942 val_loss: 0.36940891159354283\n",
      "epoch: 1685\n",
      "train oa: 94.0 train loss 0.1482424866176396\n",
      " val oa: 0.8806 val_loss: 0.39389351511285164\n",
      "epoch: 1686\n",
      "train oa: 94.05 train loss 0.14241678092242846\n",
      " val oa: 0.8896 val_loss: 0.36952824860386535\n",
      "epoch: 1687\n",
      "global steps 54000 running loss: 0.010\n",
      "train oa: 94.25 train loss 0.12437930899618452\n",
      " val oa: 0.8992 val_loss: 0.33894159916222527\n",
      "epoch: 1688\n",
      "train oa: 94.85 train loss 0.12014193186852824\n",
      " val oa: 0.8992 val_loss: 0.3471039952987436\n",
      "epoch: 1689\n",
      "train oa: 94.6 train loss 0.1264643721236809\n",
      " val oa: 0.8854 val_loss: 0.4094592137515054\n",
      "epoch: 1690\n",
      "train oa: 93.65 train loss 0.1273144786213455\n",
      " val oa: 0.9004 val_loss: 0.3740012528339552\n",
      "epoch: 1691\n",
      "train oa: 93.65 train loss 0.14627762474759212\n",
      " val oa: 0.9058 val_loss: 0.32875816818202364\n",
      "epoch: 1692\n",
      "train oa: 93.6 train loss 0.13983938704983762\n",
      " val oa: 0.8916 val_loss: 0.3834479122564188\n",
      "epoch: 1693\n",
      "global steps 54200 running loss: 0.015\n",
      "train oa: 94.7 train loss 0.12317361823364587\n",
      " val oa: 0.9068 val_loss: 0.3382063880432249\n",
      "epoch: 1694\n",
      "train oa: 94.25 train loss 0.1344706234491151\n",
      " val oa: 0.8898 val_loss: 0.3598027939035263\n",
      "epoch: 1695\n",
      "train oa: 93.7 train loss 0.1301619631158715\n",
      " val oa: 0.8994 val_loss: 0.32072201859425414\n",
      "epoch: 1696\n",
      "train oa: 94.85 train loss 0.1264681920676743\n",
      " val oa: 0.8986 val_loss: 0.3769139515458458\n",
      "epoch: 1697\n",
      "train oa: 93.6 train loss 0.1435300502493296\n",
      " val oa: 0.9056 val_loss: 0.33884357657731373\n",
      "epoch: 1698\n",
      "train oa: 94.25 train loss 0.14080389726026782\n",
      " val oa: 0.8932 val_loss: 0.36971326920204794\n",
      "epoch: 1699\n",
      "global steps 54400 running loss: 0.022\n",
      "train oa: 93.8 train loss 0.13481404320519635\n",
      " val oa: 0.9048 val_loss: 0.32852097136386144\n",
      "epoch: 1700\n",
      "train oa: 94.85 train loss 0.1265433727502862\n",
      " val oa: 0.9016 val_loss: 0.32924413208946546\n",
      "epoch: 1701\n",
      "train oa: 94.3 train loss 0.13122199135159862\n",
      " val oa: 0.9018 val_loss: 0.3395663561617116\n",
      "epoch: 1702\n",
      "train oa: 95.15 train loss 0.11451985265893297\n",
      " val oa: 0.9064 val_loss: 0.36901468258255504\n",
      "epoch: 1703\n",
      "train oa: 93.95 train loss 0.14641744666720805\n",
      " val oa: 0.892 val_loss: 0.3469201699373682\n",
      "epoch: 1704\n",
      "train oa: 94.8 train loss 0.12594879203429218\n",
      " val oa: 0.907 val_loss: 0.3260761179235613\n",
      "epoch: 1705\n",
      "train oa: 94.2 train loss 0.13529668789271684\n",
      " val oa: 0.8924 val_loss: 0.3665280182498453\n",
      "epoch: 1706\n",
      "global steps 54600 running loss: 0.005\n",
      "train oa: 95.2 train loss 0.11919429689369529\n",
      " val oa: 0.9026 val_loss: 0.3616144718456018\n",
      "epoch: 1707\n",
      "train oa: 94.85 train loss 0.12305770025559248\n",
      " val oa: 0.9034 val_loss: 0.3687472459458617\n",
      "epoch: 1708\n",
      "train oa: 94.85 train loss 0.12890213522421773\n",
      " val oa: 0.901 val_loss: 0.36294626566498783\n",
      "epoch: 1709\n",
      "train oa: 94.45 train loss 0.13020480456915362\n",
      " val oa: 0.8978 val_loss: 0.3769191628278441\n",
      "epoch: 1710\n",
      "train oa: 93.8 train loss 0.13160086083006478\n",
      " val oa: 0.8996 val_loss: 0.3466906625844634\n",
      "epoch: 1711\n",
      "train oa: 93.65 train loss 0.14012802407574368\n",
      " val oa: 0.902 val_loss: 0.3598672159858212\n",
      "epoch: 1712\n",
      "global steps 54800 running loss: 0.009\n",
      "train oa: 94.5 train loss 0.11451675095317632\n",
      " val oa: 0.8986 val_loss: 0.3835449346171928\n",
      "epoch: 1713\n",
      "train oa: 94.75 train loss 0.11759294730976294\n",
      " val oa: 0.9024 val_loss: 0.35202111252374396\n",
      "epoch: 1714\n",
      "train oa: 94.3 train loss 0.14122135982643036\n",
      " val oa: 0.8922 val_loss: 0.41386077285785433\n",
      "epoch: 1715\n",
      "train oa: 94.15 train loss 0.13695486586947025\n",
      " val oa: 0.901 val_loss: 0.3695988751692974\n",
      "epoch: 1716\n",
      "train oa: 94.05 train loss 0.14466455257635197\n",
      " val oa: 0.9072 val_loss: 0.3292532356983184\n",
      "epoch: 1717\n",
      "train oa: 95.25 train loss 0.12628378305449794\n",
      " val oa: 0.8962 val_loss: 0.33674699476559233\n",
      "epoch: 1718\n",
      "global steps 55000 running loss: 0.015\n",
      "train oa: 94.65 train loss 0.12503210877693627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val oa: 0.899 val_loss: 0.34497704329101136\n",
      "epoch: 1719\n",
      "train oa: 95.4 train loss 0.10602706586645382\n",
      " val oa: 0.8982 val_loss: 0.36204200464523495\n",
      "epoch: 1720\n",
      "train oa: 94.5 train loss 0.12578434037072528\n",
      " val oa: 0.9004 val_loss: 0.38816557322234585\n",
      "epoch: 1721\n",
      "train oa: 94.1 train loss 0.13140440561147185\n",
      " val oa: 0.881 val_loss: 0.38115907444164643\n",
      "epoch: 1722\n",
      "train oa: 94.45 train loss 0.1401893118172538\n",
      " val oa: 0.8978 val_loss: 0.36693493658178106\n",
      "epoch: 1723\n",
      "train oa: 94.75 train loss 0.13238762376452695\n",
      " val oa: 0.8998 val_loss: 0.33042730850003343\n",
      "epoch: 1724\n",
      "global steps 55200 running loss: 0.022\n",
      "train oa: 93.8 train loss 0.14046968670465523\n",
      " val oa: 0.8832 val_loss: 0.40147972309144786\n",
      "epoch: 1725\n",
      "train oa: 93.7 train loss 0.14535348920266017\n",
      " val oa: 0.8918 val_loss: 0.34837551748394946\n",
      "epoch: 1726\n",
      "train oa: 93.55 train loss 0.1461865133094246\n",
      " val oa: 0.907 val_loss: 0.324002945189257\n",
      "epoch: 1727\n",
      "train oa: 94.7 train loss 0.12639769488336292\n",
      " val oa: 0.9024 val_loss: 0.33464127000222244\n",
      "epoch: 1728\n",
      "train oa: 95.1 train loss 0.11043425114107264\n",
      " val oa: 0.9054 val_loss: 0.3448400458730734\n",
      "epoch: 1729\n",
      "train oa: 94.3 train loss 0.12847113253754855\n",
      " val oa: 0.8956 val_loss: 0.3495060937397198\n",
      "epoch: 1730\n",
      "train oa: 95.35 train loss 0.11520998911946356\n",
      " val oa: 0.8962 val_loss: 0.35911200752506905\n",
      "epoch: 1731\n",
      "global steps 55400 running loss: 0.005\n",
      "train oa: 93.95 train loss 0.14360447252507985\n",
      " val oa: 0.8908 val_loss: 0.3506022242997666\n",
      "epoch: 1732\n",
      "train oa: 94.4 train loss 0.11880794999541408\n",
      " val oa: 0.9006 val_loss: 0.36053875246088307\n",
      "epoch: 1733\n",
      "train oa: 94.85 train loss 0.12796836634758674\n",
      " val oa: 0.8976 val_loss: 0.3591492895718104\n",
      "epoch: 1734\n",
      "train oa: 93.5 train loss 0.13282126582117712\n",
      " val oa: 0.9036 val_loss: 0.3241974459650049\n",
      "epoch: 1735\n",
      "train oa: 94.55 train loss 0.1267245495449454\n",
      " val oa: 0.8862 val_loss: 0.3654814913671981\n",
      "epoch: 1736\n",
      "train oa: 94.6 train loss 0.13167663433352272\n",
      " val oa: 0.9012 val_loss: 0.35704363429899966\n",
      "epoch: 1737\n",
      "global steps 55600 running loss: 0.008\n",
      "train oa: 95.0 train loss 0.11217991295791133\n",
      " val oa: 0.8876 val_loss: 0.3792712044292382\n",
      "epoch: 1738\n",
      "train oa: 95.0 train loss 0.11893639194580324\n",
      " val oa: 0.892 val_loss: 0.35318112840887494\n",
      "epoch: 1739\n",
      "train oa: 94.3 train loss 0.12834636997722154\n",
      " val oa: 0.9012 val_loss: 0.3426743724678449\n",
      "epoch: 1740\n",
      "train oa: 94.6 train loss 0.1160324821692449\n",
      " val oa: 0.8968 val_loss: 0.36207911655946845\n",
      "epoch: 1741\n",
      "train oa: 94.25 train loss 0.1302288048191347\n",
      " val oa: 0.8978 val_loss: 0.34253595705028145\n",
      "epoch: 1742\n",
      "train oa: 94.3 train loss 0.1388349256190091\n",
      " val oa: 0.8994 val_loss: 0.35193172226249547\n",
      "epoch: 1743\n",
      "global steps 55800 running loss: 0.015\n",
      "train oa: 94.35 train loss 0.1269160405941133\n",
      " val oa: 0.8888 val_loss: 0.405332203686294\n",
      "epoch: 1744\n",
      "train oa: 94.15 train loss 0.14869273601573746\n",
      " val oa: 0.8948 val_loss: 0.3641777492713768\n",
      "epoch: 1745\n",
      "train oa: 93.3 train loss 0.1477708708607096\n",
      " val oa: 0.8878 val_loss: 0.3837745440564138\n",
      "epoch: 1746\n",
      "train oa: 94.05 train loss 0.13444711295782583\n",
      " val oa: 0.8976 val_loss: 0.3398643944809205\n",
      "epoch: 1747\n",
      "train oa: 94.2 train loss 0.12169906334334067\n",
      " val oa: 0.9006 val_loss: 0.3549084880157246\n",
      "epoch: 1748\n",
      "train oa: 95.6 train loss 0.10884927310558519\n",
      " val oa: 0.9108 val_loss: 0.3330082464644998\n",
      "epoch: 1748 best val accuracy: 0.9108\n",
      "epoch: 1749\n",
      "global steps 56000 running loss: 0.021\n",
      "train oa: 94.7 train loss 0.1298692979448316\n",
      " val oa: 0.8966 val_loss: 0.37762070980884926\n",
      "epoch: 1750\n",
      "train oa: 93.95 train loss 0.13340249550184172\n",
      " val oa: 0.893 val_loss: 0.3358578147339915\n",
      "epoch: 1751\n",
      "train oa: 94.35 train loss 0.12542967826311738\n",
      " val oa: 0.9016 val_loss: 0.3468585853022729\n",
      "epoch: 1752\n",
      "train oa: 94.95 train loss 0.12305973871695144\n",
      " val oa: 0.8942 val_loss: 0.3366084928531018\n",
      "epoch: 1753\n",
      "train oa: 94.4 train loss 0.13484296661056155\n",
      " val oa: 0.8994 val_loss: 0.3514235059118586\n",
      "epoch: 1754\n",
      "train oa: 94.25 train loss 0.14338235820716666\n",
      " val oa: 0.8926 val_loss: 0.36794431988560333\n",
      "epoch: 1755\n",
      "train oa: 94.5 train loss 0.12215764668291683\n",
      " val oa: 0.8916 val_loss: 0.3925959394002071\n",
      "epoch: 1756\n",
      "global steps 56200 running loss: 0.005\n",
      "train oa: 94.55 train loss 0.12254184760335159\n",
      " val oa: 0.8996 val_loss: 0.3564648449845788\n",
      "epoch: 1757\n",
      "train oa: 94.45 train loss 0.1250253188473582\n",
      " val oa: 0.8934 val_loss: 0.39567909801393014\n",
      "epoch: 1758\n",
      "train oa: 93.7 train loss 0.14365214876127627\n",
      " val oa: 0.8972 val_loss: 0.33602217544810997\n",
      "epoch: 1759\n",
      "train oa: 94.1 train loss 0.127752809984812\n",
      " val oa: 0.8938 val_loss: 0.3760151442207402\n",
      "epoch: 1760\n",
      "train oa: 94.85 train loss 0.12735833002038427\n",
      " val oa: 0.8972 val_loss: 0.36185161180334235\n",
      "epoch: 1761\n",
      "train oa: 94.55 train loss 0.12626829300374873\n",
      " val oa: 0.9028 val_loss: 0.344345472204623\n",
      "epoch: 1762\n",
      "global steps 56400 running loss: 0.008\n",
      "train oa: 94.85 train loss 0.11692573487672206\n",
      " val oa: 0.8936 val_loss: 0.38893477068864846\n",
      "epoch: 1763\n",
      "train oa: 94.5 train loss 0.11780391494889474\n",
      " val oa: 0.8982 val_loss: 0.3704186007672685\n",
      "epoch: 1764\n",
      "train oa: 94.7 train loss 0.11497614171980758\n",
      " val oa: 0.8898 val_loss: 0.37010286497010514\n",
      "epoch: 1765\n",
      "train oa: 95.25 train loss 0.12014985671773627\n",
      " val oa: 0.8978 val_loss: 0.3876816566922134\n",
      "epoch: 1766\n",
      "train oa: 94.1 train loss 0.13734794120314398\n",
      " val oa: 0.8926 val_loss: 0.3633815773358943\n",
      "epoch: 1767\n",
      "train oa: 94.8 train loss 0.12824957961142378\n",
      " val oa: 0.9022 val_loss: 0.3515337114887409\n",
      "epoch: 1768\n",
      "global steps 56600 running loss: 0.017\n",
      "train oa: 94.3 train loss 0.13011703971279953\n",
      " val oa: 0.894 val_loss: 0.3690255181079321\n",
      "epoch: 1769\n",
      "train oa: 94.95 train loss 0.11499005598507611\n",
      " val oa: 0.9012 val_loss: 0.3776866806024954\n",
      "epoch: 1770\n",
      "train oa: 94.3 train loss 0.12980200315095283\n",
      " val oa: 0.9036 val_loss: 0.34847482005720254\n",
      "epoch: 1771\n",
      "train oa: 94.95 train loss 0.11426001477170379\n",
      " val oa: 0.8866 val_loss: 0.41035125539559775\n",
      "epoch: 1772\n",
      "train oa: 94.0 train loss 0.12989905926459985\n",
      " val oa: 0.8982 val_loss: 0.3512630816585953\n",
      "epoch: 1773\n",
      "train oa: 94.65 train loss 0.1290671284687558\n",
      " val oa: 0.8968 val_loss: 0.36165110902409137\n",
      "epoch: 1774\n",
      "global steps 56800 running loss: 0.026\n",
      "train oa: 93.4 train loss 0.16104988348377858\n",
      " val oa: 0.8928 val_loss: 0.36648972987358364\n",
      "epoch: 1775\n",
      "train oa: 94.05 train loss 0.14673087239925145\n",
      " val oa: 0.8974 val_loss: 0.3380479635261756\n",
      "epoch: 1776\n",
      "train oa: 94.45 train loss 0.12400669573678483\n",
      " val oa: 0.8956 val_loss: 0.3448224933980777\n",
      "epoch: 1777\n",
      "train oa: 94.6 train loss 0.11405106653204498\n",
      " val oa: 0.9022 val_loss: 0.33352875022997985\n",
      "epoch: 1778\n",
      "train oa: 95.15 train loss 0.1127819405096051\n",
      " val oa: 0.8988 val_loss: 0.36998625345886926\n",
      "epoch: 1779\n",
      "train oa: 94.75 train loss 0.12597998030789986\n",
      " val oa: 0.8956 val_loss: 0.370348591477892\n",
      "epoch: 1780\n",
      "train oa: 94.45 train loss 0.12331832871943263\n",
      " val oa: 0.8968 val_loss: 0.35917480303229554\n",
      "epoch: 1781\n",
      "global steps 57000 running loss: 0.004\n",
      "train oa: 94.9 train loss 0.1137491848180564\n",
      " val oa: 0.8968 val_loss: 0.377852520487044\n",
      "epoch: 1782\n",
      "train oa: 95.05 train loss 0.1154906246417907\n",
      " val oa: 0.8962 val_loss: 0.37046513446863044\n",
      "epoch: 1783\n",
      "train oa: 94.85 train loss 0.1129383360338595\n",
      " val oa: 0.8932 val_loss: 0.37738811012938506\n",
      "epoch: 1784\n",
      "train oa: 94.95 train loss 0.11180339084630074\n",
      " val oa: 0.8956 val_loss: 0.39483285294866316\n",
      "epoch: 1785\n",
      "train oa: 95.55 train loss 0.11144135695253013\n",
      " val oa: 0.8964 val_loss: 0.40605565921446324\n",
      "epoch: 1786\n",
      "train oa: 94.9 train loss 0.13433251249292344\n",
      " val oa: 0.898 val_loss: 0.3698620809670995\n",
      "epoch: 1787\n",
      "global steps 57200 running loss: 0.009\n",
      "train oa: 95.9 train loss 0.10268308806567504\n",
      " val oa: 0.8876 val_loss: 0.4214551817691964\n",
      "epoch: 1788\n",
      "train oa: 94.7 train loss 0.12661870359333585\n",
      " val oa: 0.8932 val_loss: 0.40829755525974026\n",
      "epoch: 1789\n",
      "train oa: 94.65 train loss 0.11930822237374865\n",
      " val oa: 0.8898 val_loss: 0.4114038774612452\n",
      "epoch: 1790\n",
      "train oa: 94.85 train loss 0.11531205995327995\n",
      " val oa: 0.897 val_loss: 0.35527312073992606\n",
      "epoch: 1791\n",
      "train oa: 94.55 train loss 0.1280611715402073\n",
      " val oa: 0.8902 val_loss: 0.39583498663008715\n",
      "epoch: 1792\n",
      "train oa: 94.6 train loss 0.1251952841813571\n",
      " val oa: 0.9 val_loss: 0.3632938869592527\n",
      "epoch: 1793\n",
      "global steps 57400 running loss: 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 95.2 train loss 0.1152300014207473\n",
      " val oa: 0.9008 val_loss: 0.3544366037463461\n",
      "epoch: 1794\n",
      "train oa: 95.5 train loss 0.12386741347500795\n",
      " val oa: 0.8944 val_loss: 0.35900645035151674\n",
      "epoch: 1795\n",
      "train oa: 95.05 train loss 0.11666358880542892\n",
      " val oa: 0.9012 val_loss: 0.3483637480793469\n",
      "epoch: 1796\n",
      "train oa: 94.4 train loss 0.12856999580579667\n",
      " val oa: 0.9 val_loss: 0.3697187463034445\n",
      "epoch: 1797\n",
      "train oa: 94.55 train loss 0.13229160916444782\n",
      " val oa: 0.9022 val_loss: 0.3491947441389488\n",
      "epoch: 1798\n",
      "train oa: 94.85 train loss 0.12204801771043101\n",
      " val oa: 0.8938 val_loss: 0.36781300000682005\n",
      "epoch: 1799\n",
      "global steps 57600 running loss: 0.018\n",
      "train oa: 95.2 train loss 0.11233415264262814\n",
      " val oa: 0.9014 val_loss: 0.36803827026778807\n",
      "epoch: 1800\n",
      "train oa: 94.7 train loss 0.12119622726747444\n",
      " val oa: 0.8918 val_loss: 0.3747498491025961\n",
      "epoch: 1801\n",
      "train oa: 94.55 train loss 0.12574584587308885\n",
      " val oa: 0.8958 val_loss: 0.3735823206376534\n",
      "epoch: 1802\n",
      "train oa: 94.75 train loss 0.11521908951379338\n",
      " val oa: 0.9048 val_loss: 0.34411600003073917\n",
      "epoch: 1803\n",
      "train oa: 95.0 train loss 0.1124295509720896\n",
      " val oa: 0.9018 val_loss: 0.3631682649493062\n",
      "epoch: 1804\n",
      "train oa: 94.2 train loss 0.136127353653035\n",
      " val oa: 0.8844 val_loss: 0.4182668876668165\n",
      "epoch: 1805\n",
      "train oa: 94.65 train loss 0.1251545874533598\n",
      " val oa: 0.8972 val_loss: 0.34869678062709664\n",
      "epoch: 1806\n",
      "global steps 57800 running loss: 0.004\n",
      "train oa: 95.4 train loss 0.11051769848799571\n",
      " val oa: 0.8988 val_loss: 0.35556672540039413\n",
      "epoch: 1807\n",
      "train oa: 95.0 train loss 0.11921087951006258\n",
      " val oa: 0.897 val_loss: 0.40598544711159823\n",
      "epoch: 1808\n",
      "train oa: 94.35 train loss 0.14067304796521904\n",
      " val oa: 0.894 val_loss: 0.3550219660077796\n",
      "epoch: 1809\n",
      "train oa: 94.85 train loss 0.12250405487563612\n",
      " val oa: 0.897 val_loss: 0.36332838342570767\n",
      "epoch: 1810\n",
      "train oa: 94.75 train loss 0.11784474251873711\n",
      " val oa: 0.9014 val_loss: 0.36769353918818437\n",
      "epoch: 1811\n",
      "train oa: 94.8 train loss 0.13145733557916192\n",
      " val oa: 0.9006 val_loss: 0.3727964050589197\n",
      "epoch: 1812\n",
      "global steps 58000 running loss: 0.009\n",
      "train oa: 94.9 train loss 0.11556918711151845\n",
      " val oa: 0.8998 val_loss: 0.3536863196663035\n",
      "epoch: 1813\n",
      "train oa: 94.7 train loss 0.11898736883319252\n",
      " val oa: 0.895 val_loss: 0.37740459915292845\n",
      "epoch: 1814\n",
      "train oa: 94.4 train loss 0.12076085826636374\n",
      " val oa: 0.899 val_loss: 0.36799088041378203\n",
      "epoch: 1815\n",
      "train oa: 94.95 train loss 0.12051660186787617\n",
      " val oa: 0.8946 val_loss: 0.36859955303545233\n",
      "epoch: 1816\n",
      "train oa: 95.4 train loss 0.11566974109421851\n",
      " val oa: 0.885 val_loss: 0.4032704399760043\n",
      "epoch: 1817\n",
      "train oa: 94.95 train loss 0.12400074205025871\n",
      " val oa: 0.899 val_loss: 0.36307562562206297\n",
      "epoch: 1818\n",
      "global steps 58200 running loss: 0.012\n",
      "train oa: 95.8 train loss 0.10355013735898384\n",
      " val oa: 0.8994 val_loss: 0.3959102624044828\n",
      "epoch: 1819\n",
      "train oa: 95.0 train loss 0.11553778363409987\n",
      " val oa: 0.8922 val_loss: 0.38852616015586466\n",
      "epoch: 1820\n",
      "train oa: 94.85 train loss 0.1171191390948486\n",
      " val oa: 0.8934 val_loss: 0.359697340873121\n",
      "epoch: 1821\n",
      "train oa: 94.75 train loss 0.12225211722595526\n",
      " val oa: 0.896 val_loss: 0.38303528517669494\n",
      "epoch: 1822\n",
      "train oa: 94.9 train loss 0.1092485926109487\n",
      " val oa: 0.8984 val_loss: 0.38360885152484403\n",
      "epoch: 1823\n",
      "train oa: 95.05 train loss 0.1227044942352403\n",
      " val oa: 0.8934 val_loss: 0.39551403657070155\n",
      "epoch: 1824\n",
      "global steps 58400 running loss: 0.019\n",
      "train oa: 94.75 train loss 0.11965982947455557\n",
      " val oa: 0.8974 val_loss: 0.3622427366706661\n",
      "epoch: 1825\n",
      "train oa: 95.3 train loss 0.11171784774645155\n",
      " val oa: 0.899 val_loss: 0.38169064878239484\n",
      "epoch: 1826\n",
      "train oa: 95.15 train loss 0.10949857369142169\n",
      " val oa: 0.8968 val_loss: 0.386765562564953\n",
      "epoch: 1827\n",
      "train oa: 94.7 train loss 0.12358003114621055\n",
      " val oa: 0.8892 val_loss: 0.41254611273990843\n",
      "epoch: 1828\n",
      "train oa: 94.7 train loss 0.13373114398668903\n",
      " val oa: 0.8934 val_loss: 0.3645418798368285\n",
      "epoch: 1829\n",
      "train oa: 95.05 train loss 0.11117220122702545\n",
      " val oa: 0.8982 val_loss: 0.3807698201172896\n",
      "epoch: 1830\n",
      "train oa: 95.05 train loss 0.11304212834915162\n",
      " val oa: 0.897 val_loss: 0.40774377897044684\n",
      "epoch: 1831\n",
      "global steps 58600 running loss: 0.004\n",
      "train oa: 94.5 train loss 0.1292217681708522\n",
      " val oa: 0.8994 val_loss: 0.3672915897999606\n",
      "epoch: 1832\n",
      "train oa: 94.55 train loss 0.13302093318005206\n",
      " val oa: 0.8944 val_loss: 0.36249425140235586\n",
      "epoch: 1833\n",
      "train oa: 95.3 train loss 0.10944329010269982\n",
      " val oa: 0.886 val_loss: 0.3974188261446111\n",
      "epoch: 1834\n",
      "train oa: 94.5 train loss 0.1303481790151846\n",
      " val oa: 0.8974 val_loss: 0.34201621656514264\n",
      "epoch: 1835\n",
      "train oa: 94.3 train loss 0.13164693197128013\n",
      " val oa: 0.8974 val_loss: 0.38341187937695204\n",
      "epoch: 1836\n",
      "train oa: 95.45 train loss 0.10798347848027484\n",
      " val oa: 0.8982 val_loss: 0.35847283586325046\n",
      "epoch: 1837\n",
      "global steps 58800 running loss: 0.011\n",
      "train oa: 93.45 train loss 0.14014372471638203\n",
      " val oa: 0.8954 val_loss: 0.3666662639305698\n",
      "epoch: 1838\n",
      "train oa: 94.35 train loss 0.13364180577089335\n",
      " val oa: 0.8966 val_loss: 0.3667159526038509\n",
      "epoch: 1839\n",
      "train oa: 94.8 train loss 0.12443406125978954\n",
      " val oa: 0.8928 val_loss: 0.363618666714178\n",
      "epoch: 1840\n",
      "train oa: 94.8 train loss 0.11823460223215955\n",
      " val oa: 0.8978 val_loss: 0.36589416697066846\n",
      "epoch: 1841\n",
      "train oa: 95.15 train loss 0.1117524145754175\n",
      " val oa: 0.8988 val_loss: 0.3715553193781491\n",
      "epoch: 1842\n",
      "train oa: 94.55 train loss 0.11635763868429641\n",
      " val oa: 0.8968 val_loss: 0.39041984152406095\n",
      "epoch: 1843\n",
      "global steps 59000 running loss: 0.015\n",
      "train oa: 94.55 train loss 0.12409127677752789\n",
      " val oa: 0.8918 val_loss: 0.37703358159704686\n",
      "epoch: 1844\n",
      "train oa: 95.4 train loss 0.11551558334140627\n",
      " val oa: 0.8976 val_loss: 0.3588420349070642\n",
      "epoch: 1845\n",
      "train oa: 94.35 train loss 0.13334917087424103\n",
      " val oa: 0.8984 val_loss: 0.3752102694091533\n",
      "epoch: 1846\n",
      "train oa: 95.25 train loss 0.11735019936460765\n",
      " val oa: 0.9016 val_loss: 0.34692133916441675\n",
      "epoch: 1847\n",
      "train oa: 95.2 train loss 0.10830961432065572\n",
      " val oa: 0.9024 val_loss: 0.3725041989905383\n",
      "epoch: 1848\n",
      "train oa: 94.55 train loss 0.12269385842193078\n",
      " val oa: 0.9006 val_loss: 0.34863548399285604\n",
      "epoch: 1849\n",
      "global steps 59200 running loss: 0.018\n",
      "train oa: 95.2 train loss 0.11044205927326736\n",
      " val oa: 0.901 val_loss: 0.35111865510895385\n",
      "epoch: 1850\n",
      "train oa: 94.5 train loss 0.12832041702643435\n",
      " val oa: 0.8992 val_loss: 0.37271553468205476\n",
      "epoch: 1851\n",
      "train oa: 95.6 train loss 0.10475390035519243\n",
      " val oa: 0.901 val_loss: 0.3364847009138231\n",
      "epoch: 1852\n",
      "train oa: 95.6 train loss 0.11400706666732696\n",
      " val oa: 0.8894 val_loss: 0.38519539202511766\n",
      "epoch: 1853\n",
      "train oa: 93.3 train loss 0.14915923659823224\n",
      " val oa: 0.8906 val_loss: 0.3780695276709997\n",
      "epoch: 1854\n",
      "train oa: 93.9 train loss 0.1291155801910436\n",
      " val oa: 0.8964 val_loss: 0.35690248080956666\n",
      "epoch: 1855\n",
      "train oa: 95.1 train loss 0.10785489894918504\n",
      " val oa: 0.9002 val_loss: 0.37162515425595066\n",
      "epoch: 1856\n",
      "global steps 59400 running loss: 0.003\n",
      "train oa: 95.3 train loss 0.10929280751017953\n",
      " val oa: 0.8948 val_loss: 0.385449963843027\n",
      "epoch: 1857\n",
      "train oa: 95.6 train loss 0.10371192359183297\n",
      " val oa: 0.9004 val_loss: 0.3542315943043105\n",
      "epoch: 1858\n",
      "train oa: 94.7 train loss 0.11664265960482491\n",
      " val oa: 0.8944 val_loss: 0.38059431393370113\n",
      "epoch: 1859\n",
      "train oa: 95.6 train loss 0.1066732828785888\n",
      " val oa: 0.894 val_loss: 0.3674393434807224\n",
      "epoch: 1860\n",
      "train oa: 95.2 train loss 0.12240563051078783\n",
      " val oa: 0.8974 val_loss: 0.36496356172719013\n",
      "epoch: 1861\n",
      "train oa: 95.15 train loss 0.10858723321936688\n",
      " val oa: 0.9074 val_loss: 0.3559016641933898\n",
      "epoch: 1862\n",
      "global steps 59600 running loss: 0.009\n",
      "train oa: 95.4 train loss 0.10883337082520937\n",
      " val oa: 0.8968 val_loss: 0.3702633926303071\n",
      "epoch: 1863\n",
      "train oa: 94.7 train loss 0.13365367378801818\n",
      " val oa: 0.8916 val_loss: 0.428034040940757\n",
      "epoch: 1864\n",
      "train oa: 94.8 train loss 0.12176284914133154\n",
      " val oa: 0.901 val_loss: 0.3735644993286859\n",
      "epoch: 1865\n",
      "train oa: 95.25 train loss 0.11719076232364438\n",
      " val oa: 0.8926 val_loss: 0.36848035134892704\n",
      "epoch: 1866\n",
      "train oa: 94.85 train loss 0.11809469847228013\n",
      " val oa: 0.8994 val_loss: 0.38294058523745006\n",
      "epoch: 1867\n",
      "train oa: 94.9 train loss 0.12694544380744507\n",
      " val oa: 0.8972 val_loss: 0.3594645361962493\n",
      "epoch: 1868\n",
      "global steps 59800 running loss: 0.015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 95.0 train loss 0.11389622594803989\n",
      " val oa: 0.9024 val_loss: 0.3388646758667264\n",
      "epoch: 1869\n",
      "train oa: 95.05 train loss 0.12184343761778585\n",
      " val oa: 0.8936 val_loss: 0.3681813528724456\n",
      "epoch: 1870\n",
      "train oa: 93.95 train loss 0.13716658191125436\n",
      " val oa: 0.8972 val_loss: 0.36642341315531435\n",
      "epoch: 1871\n",
      "train oa: 95.35 train loss 0.10090719812495226\n",
      " val oa: 0.9014 val_loss: 0.3523862588759958\n",
      "epoch: 1872\n",
      "train oa: 95.15 train loss 0.11328813567286744\n",
      " val oa: 0.9052 val_loss: 0.35674794620146233\n",
      "epoch: 1873\n",
      "train oa: 94.95 train loss 0.12790238599073236\n",
      " val oa: 0.8888 val_loss: 0.40414504569638876\n",
      "epoch: 1874\n",
      "global steps 60000 running loss: 0.021\n",
      "train oa: 94.8 train loss 0.131543813359452\n",
      " val oa: 0.8836 val_loss: 0.3806395104497818\n",
      "epoch: 1875\n",
      "train oa: 94.15 train loss 0.12601461139042852\n",
      " val oa: 0.8962 val_loss: 0.36559428767297997\n",
      "epoch: 1876\n",
      "train oa: 95.35 train loss 0.11945653842223394\n",
      " val oa: 0.9066 val_loss: 0.3296254800781573\n",
      "epoch: 1877\n",
      "train oa: 94.8 train loss 0.12084790778105134\n",
      " val oa: 0.8942 val_loss: 0.38463975073084417\n",
      "epoch: 1878\n",
      "train oa: 94.6 train loss 0.12291073448086969\n",
      " val oa: 0.9006 val_loss: 0.3564505427200031\n",
      "epoch: 1879\n",
      "train oa: 94.4 train loss 0.12151160025634172\n",
      " val oa: 0.8956 val_loss: 0.3778357008048669\n",
      "epoch: 1880\n",
      "train oa: 94.9 train loss 0.1146324856276121\n",
      " val oa: 0.8988 val_loss: 0.3718915047667002\n",
      "epoch: 1881\n",
      "global steps 60200 running loss: 0.005\n",
      "train oa: 94.65 train loss 0.11844277803761906\n",
      " val oa: 0.8954 val_loss: 0.3663020186564297\n",
      "epoch: 1882\n",
      "train oa: 95.15 train loss 0.11463667434529998\n",
      " val oa: 0.906 val_loss: 0.3458665096077353\n",
      "epoch: 1883\n",
      "train oa: 95.45 train loss 0.10294020977429573\n",
      " val oa: 0.8948 val_loss: 0.3809795515710704\n",
      "epoch: 1884\n",
      "train oa: 95.5 train loss 0.10690574595941069\n",
      " val oa: 0.8974 val_loss: 0.3815564446579371\n",
      "epoch: 1885\n",
      "train oa: 95.1 train loss 0.1010744741606936\n",
      " val oa: 0.899 val_loss: 0.36144375992632466\n",
      "epoch: 1886\n",
      "train oa: 93.95 train loss 0.13501599225386002\n",
      " val oa: 0.8996 val_loss: 0.3699279182593399\n",
      "epoch: 1887\n",
      "global steps 60400 running loss: 0.009\n",
      "train oa: 95.75 train loss 0.10209564482255608\n",
      " val oa: 0.9008 val_loss: 0.38307706962437454\n",
      "epoch: 1888\n",
      "train oa: 95.65 train loss 0.10494541029968715\n",
      " val oa: 0.897 val_loss: 0.40828537883843824\n",
      "epoch: 1889\n",
      "train oa: 94.95 train loss 0.10840951884622596\n",
      " val oa: 0.898 val_loss: 0.4030457588831045\n",
      "epoch: 1890\n",
      "train oa: 94.65 train loss 0.1278967158636153\n",
      " val oa: 0.8986 val_loss: 0.3706219801868266\n",
      "epoch: 1891\n",
      "train oa: 94.45 train loss 0.13757645726295312\n",
      " val oa: 0.8946 val_loss: 0.38842774034970484\n",
      "epoch: 1892\n",
      "train oa: 94.7 train loss 0.11963715130319016\n",
      " val oa: 0.8988 val_loss: 0.37371168012360073\n",
      "epoch: 1893\n",
      "global steps 60600 running loss: 0.013\n",
      "train oa: 95.25 train loss 0.10667132394689247\n",
      " val oa: 0.8964 val_loss: 0.3581895798638018\n",
      "epoch: 1894\n",
      "train oa: 94.9 train loss 0.11678137729468231\n",
      " val oa: 0.8996 val_loss: 0.3651812855226291\n",
      "epoch: 1895\n",
      "train oa: 94.7 train loss 0.11959474268168242\n",
      " val oa: 0.8894 val_loss: 0.4135091573949607\n",
      "epoch: 1896\n",
      "train oa: 94.85 train loss 0.12272056772467593\n",
      " val oa: 0.8978 val_loss: 0.357274237917491\n",
      "epoch: 1897\n",
      "train oa: 95.65 train loss 0.10448566594644237\n",
      " val oa: 0.8898 val_loss: 0.41046647462586044\n",
      "epoch: 1898\n",
      "train oa: 95.5 train loss 0.10017869081019186\n",
      " val oa: 0.8968 val_loss: 0.39546930486408277\n",
      "epoch: 1899\n",
      "global steps 60800 running loss: 0.020\n",
      "train oa: 94.5 train loss 0.12624669507421138\n",
      " val oa: 0.8904 val_loss: 0.3906327178266176\n",
      "epoch: 1900\n",
      "train oa: 94.75 train loss 0.1137422661439642\n",
      " val oa: 0.8884 val_loss: 0.40911811993566927\n",
      "epoch: 1901\n",
      "train oa: 95.9 train loss 0.1038016927992363\n",
      " val oa: 0.898 val_loss: 0.38731601366791296\n",
      "epoch: 1902\n",
      "train oa: 94.65 train loss 0.12116145607264324\n",
      " val oa: 0.8898 val_loss: 0.42964237333322997\n",
      "epoch: 1903\n",
      "train oa: 94.55 train loss 0.12939135874214358\n",
      " val oa: 0.897 val_loss: 0.4017976893891082\n",
      "epoch: 1904\n",
      "train oa: 93.85 train loss 0.13399508622241504\n",
      " val oa: 0.8992 val_loss: 0.35438011684224047\n",
      "epoch: 1905\n",
      "train oa: 95.0 train loss 0.11864952179631673\n",
      " val oa: 0.8916 val_loss: 0.3774588689161302\n",
      "epoch: 1906\n",
      "global steps 61000 running loss: 0.005\n",
      "train oa: 94.6 train loss 0.11308101294801258\n",
      " val oa: 0.8998 val_loss: 0.3500703459246642\n",
      "epoch: 1907\n",
      "train oa: 94.85 train loss 0.12318904931913469\n",
      " val oa: 0.9 val_loss: 0.3605740752450143\n",
      "epoch: 1908\n",
      "train oa: 94.9 train loss 0.1251565864242244\n",
      " val oa: 0.9012 val_loss: 0.34596712698624377\n",
      "epoch: 1909\n",
      "train oa: 95.5 train loss 0.11530957066711932\n",
      " val oa: 0.8992 val_loss: 0.3654959968559544\n",
      "epoch: 1910\n",
      "train oa: 95.55 train loss 0.11208773908787217\n",
      " val oa: 0.8996 val_loss: 0.3896539706345671\n",
      "epoch: 1911\n",
      "train oa: 94.85 train loss 0.12158725171854892\n",
      " val oa: 0.901 val_loss: 0.35291128578946185\n",
      "epoch: 1912\n",
      "global steps 61200 running loss: 0.007\n",
      "train oa: 95.4 train loss 0.0967997753191372\n",
      " val oa: 0.898 val_loss: 0.3905527317826287\n",
      "epoch: 1913\n",
      "train oa: 95.25 train loss 0.1165500884626248\n",
      " val oa: 0.8874 val_loss: 0.4387150723786698\n",
      "epoch: 1914\n",
      "train oa: 94.15 train loss 0.13271863443853263\n",
      " val oa: 0.898 val_loss: 0.3806141783504788\n",
      "epoch: 1915\n",
      "train oa: 94.4 train loss 0.1295107929794685\n",
      " val oa: 0.8942 val_loss: 0.36069965358791084\n",
      "epoch: 1916\n",
      "train oa: 94.25 train loss 0.13732866164346952\n",
      " val oa: 0.8976 val_loss: 0.37196062893680437\n",
      "epoch: 1917\n",
      "train oa: 94.9 train loss 0.11451975091977437\n",
      " val oa: 0.899 val_loss: 0.37245925038784633\n",
      "epoch: 1918\n",
      "global steps 61400 running loss: 0.012\n",
      "train oa: 95.85 train loss 0.09760492672696798\n",
      " val oa: 0.8996 val_loss: 0.3782518745605276\n",
      "epoch: 1919\n",
      "train oa: 95.4 train loss 0.1083804244691134\n",
      " val oa: 0.8964 val_loss: 0.40281861921524015\n",
      "epoch: 1920\n",
      "train oa: 95.5 train loss 0.10547030407430061\n",
      " val oa: 0.8992 val_loss: 0.36359491613214096\n",
      "epoch: 1921\n",
      "train oa: 95.2 train loss 0.11843927959246289\n",
      " val oa: 0.9024 val_loss: 0.38219108668496116\n",
      "epoch: 1922\n",
      "train oa: 94.65 train loss 0.1155692935641379\n",
      " val oa: 0.9054 val_loss: 0.33801059541435186\n",
      "epoch: 1923\n",
      "train oa: 95.95 train loss 0.10711094532972631\n",
      " val oa: 0.9058 val_loss: 0.3579233042319681\n",
      "epoch: 1924\n",
      "global steps 61600 running loss: 0.016\n",
      "train oa: 95.9 train loss 0.09797951330343371\n",
      " val oa: 0.9024 val_loss: 0.3779794728595834\n",
      "epoch: 1925\n",
      "train oa: 94.4 train loss 0.14398614873938972\n",
      " val oa: 0.9004 val_loss: 0.3469487829802779\n",
      "epoch: 1926\n",
      "train oa: 95.05 train loss 0.1118766920780693\n",
      " val oa: 0.903 val_loss: 0.3592445157320383\n",
      "epoch: 1927\n",
      "train oa: 95.4 train loss 0.1082437216337093\n",
      " val oa: 0.9086 val_loss: 0.3368797880510016\n",
      "epoch: 1928\n",
      "train oa: 95.05 train loss 0.11537817406463927\n",
      " val oa: 0.9016 val_loss: 0.35489194499047977\n",
      "epoch: 1929\n",
      "train oa: 95.65 train loss 0.10645357231329124\n",
      " val oa: 0.902 val_loss: 0.3678278817075417\n",
      "epoch: 1930\n",
      "train oa: 95.9 train loss 0.11133861628564334\n",
      " val oa: 0.8916 val_loss: 0.3983004025895888\n",
      "epoch: 1931\n",
      "global steps 61800 running loss: 0.005\n",
      "train oa: 94.1 train loss 0.13968863842563223\n",
      " val oa: 0.891 val_loss: 0.37697324577204744\n",
      "epoch: 1932\n",
      "train oa: 94.1 train loss 0.12443864280769945\n",
      " val oa: 0.8998 val_loss: 0.3599090877577247\n",
      "epoch: 1933\n",
      "train oa: 95.8 train loss 0.09982080832558926\n",
      " val oa: 0.9006 val_loss: 0.39654679704099904\n",
      "epoch: 1934\n",
      "train oa: 95.5 train loss 0.10681684823000961\n",
      " val oa: 0.8972 val_loss: 0.34499438440915997\n",
      "epoch: 1935\n",
      "train oa: 95.4 train loss 0.11320873191253206\n",
      " val oa: 0.9006 val_loss: 0.36131843964560595\n",
      "epoch: 1936\n",
      "train oa: 95.0 train loss 0.10420626571959382\n",
      " val oa: 0.8928 val_loss: 0.3873579189748561\n",
      "epoch: 1937\n",
      "global steps 62000 running loss: 0.009\n",
      "train oa: 94.8 train loss 0.11841166627462525\n",
      " val oa: 0.8974 val_loss: 0.35562578477775\n",
      "epoch: 1938\n",
      "train oa: 94.6 train loss 0.1271997528255311\n",
      " val oa: 0.896 val_loss: 0.3556972480284029\n",
      "epoch: 1939\n",
      "train oa: 96.1 train loss 0.10548509682942644\n",
      " val oa: 0.8992 val_loss: 0.37331317101067535\n",
      "epoch: 1940\n",
      "train oa: 95.55 train loss 0.10444932590103204\n",
      " val oa: 0.8928 val_loss: 0.3775607488435495\n",
      "epoch: 1941\n",
      "train oa: 95.55 train loss 0.10576259244445042\n",
      " val oa: 0.8952 val_loss: 0.40002865895930567\n",
      "epoch: 1942\n",
      "train oa: 95.5 train loss 0.10361678371641975\n",
      " val oa: 0.8986 val_loss: 0.37575234047119094\n",
      "epoch: 1943\n",
      "global steps 62200 running loss: 0.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train oa: 95.15 train loss 0.10173033639624214\n",
      " val oa: 0.8954 val_loss: 0.3828200562228981\n",
      "epoch: 1944\n",
      "train oa: 95.9 train loss 0.10176874687268501\n",
      " val oa: 0.9002 val_loss: 0.3710275131898534\n",
      "epoch: 1945\n",
      "train oa: 94.85 train loss 0.11164517681220701\n",
      " val oa: 0.8938 val_loss: 0.39850497969925797\n",
      "epoch: 1946\n",
      "train oa: 94.85 train loss 0.10357623537530919\n",
      " val oa: 0.8968 val_loss: 0.40358844721391346\n",
      "epoch: 1947\n",
      "train oa: 95.5 train loss 0.1092771183089616\n",
      " val oa: 0.8944 val_loss: 0.4089328926547962\n",
      "epoch: 1948\n",
      "train oa: 95.15 train loss 0.11578529030316671\n",
      " val oa: 0.8882 val_loss: 0.43694411128957866\n",
      "epoch: 1949\n",
      "global steps 62400 running loss: 0.019\n",
      "train oa: 95.1 train loss 0.1182779082947679\n",
      " val oa: 0.8984 val_loss: 0.3681627382229102\n",
      "epoch: 1950\n",
      "train oa: 95.8 train loss 0.09875902003172476\n",
      " val oa: 0.8912 val_loss: 0.41943648319859245\n",
      "epoch: 1951\n",
      "train oa: 96.0 train loss 0.10147474169260746\n",
      " val oa: 0.885 val_loss: 0.4123640891570701\n",
      "epoch: 1952\n",
      "train oa: 95.55 train loss 0.11143914662770522\n",
      " val oa: 0.8866 val_loss: 0.4306266031189307\n",
      "epoch: 1953\n",
      "train oa: 95.1 train loss 0.11858624775432354\n",
      " val oa: 0.8922 val_loss: 0.36771957814440326\n",
      "epoch: 1954\n",
      "train oa: 94.85 train loss 0.11759883410055072\n",
      " val oa: 0.8966 val_loss: 0.37456858130585563\n",
      "epoch: 1955\n",
      "train oa: 95.55 train loss 0.10707522691262503\n",
      " val oa: 0.898 val_loss: 0.41186775159643435\n",
      "epoch: 1956\n",
      "global steps 62600 running loss: 0.005\n",
      "train oa: 95.1 train loss 0.13035502281217123\n",
      " val oa: 0.897 val_loss: 0.3825157083322287\n",
      "epoch: 1957\n",
      "train oa: 95.1 train loss 0.12094102653167212\n",
      " val oa: 0.8994 val_loss: 0.37752614918982547\n",
      "epoch: 1958\n",
      "train oa: 95.15 train loss 0.12194545665314452\n",
      " val oa: 0.8946 val_loss: 0.374017167258917\n",
      "epoch: 1959\n",
      "train oa: 95.6 train loss 0.10401997768986292\n",
      " val oa: 0.9036 val_loss: 0.347989011234715\n",
      "epoch: 1960\n",
      "train oa: 95.45 train loss 0.09932802757789917\n",
      " val oa: 0.893 val_loss: 0.3983693986155215\n",
      "epoch: 1961\n",
      "train oa: 96.4 train loss 0.10223216110092538\n",
      " val oa: 0.8988 val_loss: 0.3956425665586986\n",
      "epoch: 1962\n",
      "global steps 62800 running loss: 0.010\n",
      "train oa: 94.7 train loss 0.1265673905626925\n",
      " val oa: 0.8972 val_loss: 0.380928306119081\n",
      "epoch: 1963\n",
      "train oa: 95.2 train loss 0.11506350537436917\n",
      " val oa: 0.9 val_loss: 0.35388363470319795\n",
      "epoch: 1964\n",
      "train oa: 95.5 train loss 0.10155376685042035\n",
      " val oa: 0.9064 val_loss: 0.3396125097810479\n",
      "epoch: 1965\n",
      "train oa: 95.95 train loss 0.09737108450884548\n",
      " val oa: 0.898 val_loss: 0.38309225622420834\n",
      "epoch: 1966\n",
      "train oa: 96.55 train loss 0.09414000615202728\n",
      " val oa: 0.9016 val_loss: 0.40373609738269256\n",
      "epoch: 1967\n",
      "train oa: 96.0 train loss 0.09202715946008791\n",
      " val oa: 0.9018 val_loss: 0.40685061520836185\n",
      "epoch: 1968\n",
      "global steps 63000 running loss: 0.014\n",
      "train oa: 94.9 train loss 0.11496704516889646\n",
      " val oa: 0.8964 val_loss: 0.39737643476630957\n",
      "epoch: 1969\n",
      "train oa: 95.1 train loss 0.1106349631614758\n",
      " val oa: 0.8998 val_loss: 0.36986374491143786\n",
      "epoch: 1970\n",
      "train oa: 94.65 train loss 0.12262029178636448\n",
      " val oa: 0.8948 val_loss: 0.36934642437691445\n",
      "epoch: 1971\n",
      "train oa: 95.35 train loss 0.1026184908687759\n",
      " val oa: 0.8996 val_loss: 0.411886053653494\n",
      "epoch: 1972\n",
      "train oa: 96.0 train loss 0.09228159562589061\n",
      " val oa: 0.8994 val_loss: 0.41089158657060726\n",
      "epoch: 1973\n",
      "train oa: 95.4 train loss 0.12396856697151648\n",
      " val oa: 0.9028 val_loss: 0.36445317245585135\n",
      "epoch: 1974\n",
      "global steps 63200 running loss: 0.021\n",
      "train oa: 94.55 train loss 0.12912112461381287\n",
      " val oa: 0.8942 val_loss: 0.3894200649841899\n",
      "epoch: 1975\n",
      "train oa: 94.7 train loss 0.120806169537632\n",
      " val oa: 0.9002 val_loss: 0.3819349219434354\n",
      "epoch: 1976\n",
      "train oa: 95.65 train loss 0.10200630516603534\n",
      " val oa: 0.8984 val_loss: 0.3637616412913141\n",
      "epoch: 1977\n",
      "train oa: 96.35 train loss 0.08909589919859763\n",
      " val oa: 0.904 val_loss: 0.3757104845439098\n",
      "epoch: 1978\n",
      "train oa: 96.9 train loss 0.08425617958127489\n",
      " val oa: 0.899 val_loss: 0.39365203806112864\n",
      "epoch: 1979\n",
      "train oa: 96.3 train loss 0.09076907442062519\n",
      " val oa: 0.8948 val_loss: 0.4241647289117199\n",
      "epoch: 1980\n",
      "train oa: 95.8 train loss 0.10334270419003591\n",
      " val oa: 0.9036 val_loss: 0.3901537002212917\n",
      "epoch: 1981\n",
      "global steps 63400 running loss: 0.005\n",
      "train oa: 94.4 train loss 0.12568274628267626\n",
      " val oa: 0.8996 val_loss: 0.3639759812540396\n",
      "epoch: 1982\n",
      "train oa: 94.15 train loss 0.1443458258121294\n",
      " val oa: 0.8928 val_loss: 0.38932805355089534\n",
      "epoch: 1983\n",
      "train oa: 95.45 train loss 0.10442348940358637\n",
      " val oa: 0.901 val_loss: 0.346562028547329\n",
      "epoch: 1984\n",
      "train oa: 95.85 train loss 0.10167196744858133\n",
      " val oa: 0.8924 val_loss: 0.413910538489575\n",
      "epoch: 1985\n",
      "train oa: 95.9 train loss 0.0978936248894513\n",
      " val oa: 0.8916 val_loss: 0.439998972122701\n",
      "epoch: 1986\n",
      "train oa: 94.75 train loss 0.1111164196976902\n",
      " val oa: 0.9028 val_loss: 0.3525670575825715\n",
      "epoch: 1987\n",
      "global steps 63600 running loss: 0.010\n",
      "train oa: 94.5 train loss 0.1194919706861561\n",
      " val oa: 0.8876 val_loss: 0.4097433751932122\n",
      "epoch: 1988\n",
      "train oa: 95.1 train loss 0.11500726252017054\n",
      " val oa: 0.8984 val_loss: 0.38541435333165874\n",
      "epoch: 1989\n",
      "train oa: 96.0 train loss 0.09609977693182138\n",
      " val oa: 0.8956 val_loss: 0.3780631656184434\n",
      "epoch: 1990\n",
      "train oa: 96.1 train loss 0.10492375208234882\n",
      " val oa: 0.8988 val_loss: 0.4077620377685361\n",
      "epoch: 1991\n",
      "train oa: 95.45 train loss 0.10140191647980384\n",
      " val oa: 0.8988 val_loss: 0.36963055276589146\n",
      "epoch: 1992\n",
      "train oa: 95.8 train loss 0.09817784170437208\n",
      " val oa: 0.897 val_loss: 0.3742656264199431\n",
      "epoch: 1993\n",
      "global steps 63800 running loss: 0.014\n",
      "train oa: 95.45 train loss 0.11164580757006101\n",
      " val oa: 0.8982 val_loss: 0.34984472330317196\n",
      "epoch: 1994\n",
      "train oa: 95.5 train loss 0.11192443407795723\n",
      " val oa: 0.9002 val_loss: 0.34912663526290344\n",
      "epoch: 1995\n",
      "train oa: 95.65 train loss 0.10690679814166963\n",
      " val oa: 0.8926 val_loss: 0.38187718427113515\n",
      "epoch: 1996\n",
      "train oa: 95.65 train loss 0.09823633972585247\n",
      " val oa: 0.889 val_loss: 0.4081825385342221\n",
      "epoch: 1997\n",
      "train oa: 95.1 train loss 0.11196579502911894\n",
      " val oa: 0.9006 val_loss: 0.39210503137154396\n",
      "epoch: 1998\n",
      "train oa: 95.15 train loss 0.11727409088427147\n",
      " val oa: 0.906 val_loss: 0.38305247239459556\n",
      "epoch: 1999\n",
      "global steps 64000 running loss: 0.017\n",
      "train oa: 95.4 train loss 0.10394693804266636\n",
      " val oa: 0.8976 val_loss: 0.3934061858391544\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "filename=os.path.join(setting.train_result_dir,'traintime_'+time_str+'.txt')\n",
    "print(filename)\n",
    "\n",
    "result=open(filename,'a')\n",
    "start=time.time()\n",
    "for epoch in range(setting.EPOCH):  # loop over the dataset multiple times\n",
    "    print('epoch:',epoch)\n",
    "    result.write('epoch:'+str(epoch)+'\\n')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    train_loss= 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        b_inputs=Variable(inputs).to(device)\n",
    "        b_labels=Variable(labels).to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "#         print(inputs.shape)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(b_inputs)\n",
    "#         print(labels.unique)\n",
    "#         print(outputs.unique)\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        global_steps+=1\n",
    "#         loss_list.append(loss)\n",
    "            \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        train_loss += loss.item()\n",
    "        if (global_steps) % 200 == 0:    # print every 2000 mini-batches\n",
    "            print('global steps %5d running loss: %.3f' %\n",
    "                  (global_steps, running_loss / 200))\n",
    "#             state = {\n",
    "#                 'epoch': epoch,\n",
    "#                 'step': i,\n",
    "#                 'net': net.state_dict(),\n",
    "#                 'optimizer':optimizer.state_dict(),\n",
    "#                 'loss':running_loss / 200\n",
    "#             }\n",
    "#             checkpoint_name=os.path.join(setting.checkpoint_dir,'model_time_'+time_str+'_epoch_'+str(epoch)+'_step_'+str(i)+'.pth')\n",
    "#             torch.save(state, checkpoint_name)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        #save loss log file\n",
    "#         if global_steps%10 ==0:\n",
    "#             writer.add_scalar('train_loss',loss,global_steps)\n",
    "            \n",
    "        \n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print( 'train oa:', 100 * correct / total,'train loss',train_loss/len(trainloader))\n",
    "    result.write('train accuracy:'+str(100 * correct / total) +'\\t'+'train loss:'+str(train_loss/len(trainloader)))\n",
    "    #save training accuracy log file\n",
    "    writer.add_scalar('train_accuracy',accuracy,epoch)\n",
    "    writer.add_scalar('train_loss',train_loss/len(trainloader),epoch)\n",
    "    \n",
    "    \n",
    "    val_correct = 0.0\n",
    "    val_total = 0.0\n",
    "    val_loss = 0.0\n",
    "    net.eval()\n",
    "    for valdata in valloader:\n",
    "        val_inputs, val_labels = valdata\n",
    "        val_inputs = val_inputs.to(device)\n",
    "        val_labels =  val_labels.to(device)\n",
    "        val_outputs = net(val_inputs)\n",
    "        valloss = criterion(val_outputs, val_labels)\n",
    "        val_loss += valloss.item()\n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "        val_total += val_labels.size(0)\n",
    "        val_correct += (val_predicted == val_labels).sum().item()\n",
    "    val_oa=val_correct/val_total\n",
    "    print(' val oa:',val_oa, 'val_loss:',val_loss/len(valloader))\n",
    "    result.write('\\t val oa:'+str(val_oa)+'\\t'+ 'val_loss:'+str(val_loss/len(valloader)))\n",
    "    writer.add_scalar('val_Accu',val_oa , epoch)\n",
    "    writer.add_scalar('val_loss',val_loss/len(valloader) , epoch)\n",
    "    \n",
    "    if val_oa > best_acc:\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'accuracy': val_oa,\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer':optimizer.state_dict(),\n",
    "            'loss':loss\n",
    "        }\n",
    "        best_model_name=os.path.join(setting.best_model_dir,'model_time_'+time_str+'.pth')\n",
    "        torch.save(state, best_model_name)\n",
    "        best_acc=val_oa\n",
    "        best_epoch=epoch\n",
    "        print('epoch:',epoch,'best val accuracy:',val_oa)\n",
    "    \n",
    "    #pridict  testing data and save accuracy log file\n",
    "#     if (epoch) % 10 == 0:\n",
    "#         test_correct = 0.0\n",
    "#         test_total = 0.0\n",
    "#         test_loss = 0.0\n",
    "#         net.eval()\n",
    "#         for testdata in testloader:\n",
    "#             test_inputs, test_labels = testdata\n",
    "#             test_inputs = test_inputs.to(device)\n",
    "#             test_labels =  test_labels.to(device)\n",
    "#             test_outputs = net(test_inputs)\n",
    "#             tsloss = criterion(test_outputs, test_labels)\n",
    "#             test_loss += tsloss.item()\n",
    "#             _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "#             test_total += test_labels.size(0)\n",
    "#             test_correct += (test_predicted == test_labels).sum().item()\n",
    "#         oa=test_correct/test_total\n",
    "#         print(' test oa:',oa, 'loss:',test_loss/len(testloader))\n",
    "#         result.write('\\t test oa:'+str(oa)+'\\t'+ 'loss:'+str(test_loss/len(testloader)))\n",
    "#         writer.add_scalar('Test_Accu',oa , epoch)\n",
    "#         writer.add_scalar('Test_loss',test_loss/len(testloader) , epoch)\n",
    "    \n",
    "#         if oa > best_acc:\n",
    "#             state = {\n",
    "#                 'epoch': epoch,\n",
    "#                 'accuracy': oa,\n",
    "#                 'net': net.state_dict(),\n",
    "#                 'optimizer':optimizer.state_dict(),\n",
    "#                 'loss':loss\n",
    "#             }\n",
    "#             best_model_name=os.path.join(setting.best_model_dir,'model_time_'+time_str+'.pth')\n",
    "#             torch.save(state, best_model_name)\n",
    "#             best_acc=oa\n",
    "#             print('epoch:',epoch,'best test accuracy:',oa)\n",
    "        \n",
    "\n",
    "    finish_state = {\n",
    "        'epoch': epoch,\n",
    "        'net': net.state_dict(),\n",
    "        'optimizer':optimizer.state_dict(),\n",
    "        'loss':loss,\n",
    "        'train_oa':accuracy,\n",
    "#         'test_oa':oa\n",
    "        }\n",
    "end=time.time()\n",
    "model_name=os.path.join(setting.model_dir,'model_time_'+time_str+'.pth')\n",
    "torch.save(finish_state, model_name)\n",
    "#plt.ioff()\n",
    "#plt.show()\n",
    "print('Finished Training')\n",
    "writer.close()\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_acc: 0.9108\n",
      "best_epoch: 1748\n",
      "train time: 14005.140038013458\n",
      "./model/model_best/model_time_2022_10_31_17_16.pth\n",
      "./model/model_time_2022_10_31_17_16.pth\n"
     ]
    }
   ],
   "source": [
    "print('best_acc:',best_acc)\n",
    "print('best_epoch:',best_epoch)\n",
    "print('train time:', end-start)\n",
    "print(best_model_name)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_name='./model/salinas/model_best/model_time_2021_03_18_12_17.pth'\n",
    "# model_name='.\\model\\salinas\\model_time_2020_03_31_11_43.pth'\n",
    "# model = torch.load(model_name)\n",
    "model = torch.load(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(model['net'])\n",
    "# optimizer.load_state_dict(model['optimizer'])\n",
    "# start_epoch = model['epoch'] + 1\n",
    "# loss=model['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/lfs-env/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict training set finished\n",
      "predict train time: 1.3504996299743652\n",
      "OA_train: 0.966 \n",
      "AA_train: 0.966 \n",
      "kappa_train: 0.9319999999999999 \n",
      "acc_train: [0.954 0.978]\n"
     ]
    }
   ],
   "source": [
    "predict_trainlabels=[]\n",
    "trainlabels=[]\n",
    "t1=time.time()\n",
    "with torch.no_grad():\n",
    "    for trdata in trainloader:\n",
    "        trinputs, trlabels = trdata\n",
    "        trinputs = trinputs.to(device)\n",
    "        trlabels = trlabels.to(device)\n",
    "        troutputs = net(trinputs)\n",
    "        _, trpredicted = torch.max(F.softmax(troutputs), 1)\n",
    "        predict_trainlabels.extend(trpredicted)\n",
    "        trainlabels.extend(trlabels)\n",
    "    print('predict training set finished')\n",
    "#print(len(trainlabels))\n",
    "#print(len(predict_trainlabels))\n",
    "t2=time.time()\n",
    "print('predict train time:',t2-t1)\n",
    "oa_train, aa_train, kappa_train, acc_train=cf.eval_results_own(predict_trainlabels,trainlabels,2)\n",
    "print('OA_train:',oa_train, '\\nAA_train:', aa_train, '\\nkappa_train:', kappa_train, '\\nacc_train:', acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/lfs-env/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict val set finished\n",
      "predict val time: 3.4086055755615234\n",
      "OA_val: 0.9108 \n",
      "AA_val: 0.9108 \n",
      "kappa_val: 0.8216000000000001 \n",
      "acc_val: [0.8892 0.9324]\n"
     ]
    }
   ],
   "source": [
    "predict_vallabels=[]\n",
    "val_labels=[]\n",
    "t1=time.time()\n",
    "with torch.no_grad():\n",
    "    for valdata in valloader:\n",
    "        vinputs, vlabels = valdata\n",
    "        vinputs = vinputs.to(device)\n",
    "        vlabels = vlabels.to(device)\n",
    "        voutputs = net(vinputs)\n",
    "        _, vpredicted = torch.max(F.softmax(voutputs), 1)\n",
    "        predict_vallabels.extend(vpredicted)\n",
    "        val_labels.extend(vlabels)\n",
    "    print('predict val set finished')\n",
    "#print(len(trainlabels))\n",
    "#print(len(predict_trainlabels))\n",
    "t2=time.time()\n",
    "print('predict val time:',t2-t1)\n",
    "oa_val, aa_val, kappa_val, acc_val=cf.eval_results_own(predict_vallabels,val_labels,2)\n",
    "print('OA_val:',oa_val, '\\nAA_val:', aa_val, '\\nkappa_val:', kappa_val, '\\nacc_val:', acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
